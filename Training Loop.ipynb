{"cells":[{"cell_type":"markdown","metadata":{"id":"m7FQaWQbojAb"},"source":["### Import and Setup"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5750,"status":"ok","timestamp":1639077840857,"user":{"displayName":"Chenwei Wu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04482025801710927377"},"user_tz":300},"id":"Wyz5JwHDopli","outputId":"9a36456a-627e-4f62-a61a-f8a694cee2d9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting unidecode\n","  Downloading Unidecode-1.3.2-py3-none-any.whl (235 kB)\n","\u001b[K     |████████████████████████████████| 235 kB 2.1 MB/s \n","\u001b[?25hInstalling collected packages: unidecode\n","Successfully installed unidecode-1.3.2\n"]}],"source":["!pip install unidecode"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"T8q-CET4os1p"},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","Mounted at /content/drive\n"]}],"source":["import json\n","import re\n","from unidecode import unidecode\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","import nltk\n","nltk.download('punkt')\n","from collections import Counter\n","import pickle\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","from torch.utils import data\n","import operator\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","assert device == \"cuda\"  \n","\n","seed = 42\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"S3i2hvtYoo30"},"source":["### Data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"OGzVRiwZowGv"},"outputs":[],"source":["# # These IDs are reserved.\n","MAX_SENT_LENGTH = 15\n","MAX_SENT_LENGTH_PLUS_SOS_EOS = MAX_SENT_LENGTH + 2\n","PAD_INDEX = 0\n","UNK_INDEX = 1\n","SOS_INDEX = 2\n","EOS_INDEX = 3\n","RARE_WORD_TRESHOLD = 0\n","\n","class TSTDataset(data.Dataset):\n","    def __init__(self, taylor_sentences, drake_sentences, vocab, vocab_counts, sampling=1.):\n","        self.taylor_sentences = taylor_sentences[:int(len(taylor_sentences) * sampling)]\n","        self.drake_sentences = drake_sentences[:int(len(drake_sentences) * sampling)]\n","\n","        self.max_seq_length = MAX_SENT_LENGTH_PLUS_SOS_EOS\n","        self.vocab = vocab\n","        self.vocab_counts = vocab_counts\n","\n","        self.v2id = {v : i for i, v in enumerate(self.vocab)}\n","        self.id2v = {val : key for key, val in self.v2id.items()}\n","    \n","    def __len__(self):\n","        return min(len(self.taylor_sentences), len(self.drake_sentences))\n","    \n","    def __getitem__(self, index):\n","        taylor_sent = self.taylor_sentences[index]\n","        taylor_len = len(taylor_sent) + 2   # add \u003cs\u003e and \u003c/s\u003e to each sentence\n","        taylor_id = []\n","        for w in taylor_sent:\n","            if w not in self.vocab:\n","                w = '\u003cunk\u003e'\n","            if vocab_counts[w] \u003c= RARE_WORD_TRESHOLD:\n","                w = '\u003cunk\u003e'\n","            taylor_id.append(self.v2id[w])\n","\n","        taylor_id = ([SOS_INDEX] + taylor_id + [EOS_INDEX] + [PAD_INDEX] *\n","                  (self.max_seq_length - taylor_len))\n","\n","        drake_sent = self.drake_sentences[index]\n","        drake_len = len(drake_sent) + 2   # add \u003cs\u003e and \u003c/s\u003e to each sentence\n","        drake_id = []\n","        for w in drake_sent:\n","            if w not in self.vocab:\n","                w = '\u003cunk\u003e'\n","            if vocab_counts[w] \u003c= RARE_WORD_TRESHOLD:\n","                w = '\u003cunk\u003e'\n","            drake_id.append(self.v2id[w])\n","\n","        drake_id = ([SOS_INDEX] + drake_id + [EOS_INDEX] + [PAD_INDEX] *\n","                  (self.max_seq_length - drake_len))\n","\n","        return torch.tensor(taylor_id), taylor_len, torch.tensor(drake_id), drake_len\n","\n","train_dataset = torch.load('/content/drive/Shareddrives/MIT NLP 8.864/Data/train.pt')\n","valid_dataset = torch.load('/content/drive/Shareddrives/MIT NLP 8.864/Data/valid.pt')\n","test_dataset = torch.load('/content/drive/Shareddrives/MIT NLP 8.864/Data/test.pt')\n","vocab_file = open('/content/drive/Shareddrives/MIT NLP 8.864/Data/vocab.pkl', \"rb\")\n","vocab = pickle.load(vocab_file)\n","vocab_file.close()\n","\n","vocab_counts = Counter(vocab)\n","vocab_counts['\u003cpad\u003e'] = RARE_WORD_TRESHOLD + 1\n","vocab_counts['\u003cunk\u003e'] = RARE_WORD_TRESHOLD + 1\n","vocab_counts['\u003cs\u003e'] = RARE_WORD_TRESHOLD + 1\n","vocab_counts['\u003c/s\u003e'] = RARE_WORD_TRESHOLD + 1\n","\n","# f = open('/content/drive/Shareddrives/MIT NLP 8.864/Data/drake.json')\n","# drake = json.load(f)\n","# f.close()\n","\n","# f = open('/content/drive/Shareddrives/MIT NLP 8.864/Data/tswift.json')\n","# taylor = json.load(f)\n","# f.close()\n","\n","# drake = [drake['songs'][i]['lyrics'] for i in range(len(drake['songs']))]\n","# taylor = [taylor['songs'][i]['lyrics'] for i in range(len(taylor['songs']))]\n","\n","# taylor_lyrics = [re.sub('\\u2005', ' ', re.sub(r'[\\(\\[].*?[\\)\\]]', '', taylor[i])).split('\\n') for i in range(len(taylor))]\n","# taylor_lyrics = [[unidecode(i) for i in taylor_lyrics[j]] for j in range(len(taylor_lyrics))]\n","# taylor_lyrics = [[re.sub('\\d+EmbedShare URLCopyEmbedCopy', '', i) for i in taylor_lyrics[j]] for j in range(len(taylor_lyrics))]\n","# taylor_lyrics = [[re.sub('\\d+.EmbedShare URLCopyEmbedCopy', '', i) for i in taylor_lyrics[j]] for j in range(len(taylor_lyrics))]\n","# taylor_lyrics = [[re.sub('EmbedShare URLCopyEmbedCopy', '', i) for i in taylor_lyrics[j]] for j in range(len(taylor_lyrics))]\n","# taylor_lyrics = [[i for i in taylor_lyrics[j] if i != ''] for j in range(len(taylor_lyrics))]\n","\n","# drake_lyrics = [re.sub('\\u2005', ' ', re.sub(r'[\\(\\[].*?[\\)\\]]', '', drake[i])).split('\\n') for i in range(len(drake))]\n","# drake_lyrics = [[unidecode(i) for i in drake_lyrics[j]] for j in range(len(drake_lyrics))]\n","# drake_lyrics = [[re.sub('\\d+EmbedShare URLCopyEmbedCopy', '', i) for i in drake_lyrics[j]] for j in range(len(drake_lyrics))]\n","# drake_lyrics = [[re.sub('\\d+.EmbedShare URLCopyEmbedCopy', '', i) for i in drake_lyrics[j]] for j in range(len(drake_lyrics))]\n","# drake_lyrics = [[re.sub('EmbedShare URLCopyEmbedCopy', '', i) for i in drake_lyrics[j]] for j in range(len(drake_lyrics))]\n","# drake_lyrics = [[i for i in drake_lyrics[j] if i != ''] for j in range(len(drake_lyrics))]\n","\n","# # taylor_lyrics = [[line1 + ', ' + line2 for line1,line2 in zip(song[0::2], song[1::2])] for song in taylor_lyrics]\n","# # drake_lyrics = [[line1 + ', ' + line2 for line1,line2 in zip(song[0::2], song[1::2])] for song in drake_lyrics]\n","\n","# drake_tokenized = [[word_tokenize(drake_lyrics[i][j]) for j in range(len(drake_lyrics[i]))] for i in range(len(drake_lyrics))]\n","# taylor_tokenized = [[word_tokenize(taylor_lyrics[i][j]) for j in range(len(taylor_lyrics[i]))] for i in range(len(taylor_lyrics))]\n","\n","# drake_tokenized = [[[word.lower() for word in line] for line in song] for song in drake_tokenized]\n","# taylor_tokenized = [[[word.lower() for word in line] for line in song] for song in taylor_tokenized]\n","\n","# drake_length = sum([[len(sent) for sent in song] for song in drake_tokenized], [])\n","# taylor_length = sum([[len(sent) for sent in song] for song in taylor_tokenized], [])\n","\n","# # drake_lyrics = sum([[sent for sent in song if (len(sent) \u003e= 10 and len (sent) \u003c= 30)] for song in drake_tokenized], [])\n","# # taylor_lyrics = sum([[sent for sent in song if (len(sent) \u003e= 10 and len (sent) \u003c= 30)] for song in taylor_tokenized], [])\n","\n","# drake_lyrics = sum([[sent for sent in song if (len(sent) \u003e= 5 and len (sent) \u003c= 15)] for song in drake_tokenized], [])\n","# taylor_lyrics = sum([[sent for sent in song if (len(sent) \u003e= 5 and len (sent) \u003c= 15)] for song in taylor_tokenized], [])\n","\n","# taylor_vocab = sum(taylor_lyrics,[])\n","# drake_vocab = sum(drake_lyrics,[])\n","\n","# def unique(list1):\n","     \n","#     # insert the list to the set\n","#     list_set = set(list1)\n","#     # convert the set to the list\n","#     unique_list = (list(list_set))\n","#     return unique_list\n","\n","# vocab = taylor_vocab + drake_vocab\n","# vocab_counts = Counter(vocab)\n","# vocab = unique(vocab)\n","# vocab = ['\u003cpad\u003e','\u003cunk\u003e','\u003cs\u003e', '\u003c/s\u003e'] + vocab\n","\n","# vocab_counts['\u003cpad\u003e'] = RARE_WORD_TRESHOLD + 1\n","# vocab_counts['\u003cunk\u003e'] = RARE_WORD_TRESHOLD + 1\n","# vocab_counts['\u003cs\u003e'] = RARE_WORD_TRESHOLD + 1\n","# vocab_counts['\u003c/s\u003e'] = RARE_WORD_TRESHOLD + 1\n","\n","# class TSTDataset(data.Dataset):\n","#     def __init__(self, taylor_sentences, drake_sentences, vocab, vocab_counts, sampling=1.):\n","#         self.taylor_sentences = taylor_sentences[:int(len(taylor_sentences) * sampling)]\n","#         self.drake_sentences = drake_sentences[:int(len(drake_sentences) * sampling)]\n","\n","#         self.max_seq_length = MAX_SENT_LENGTH_PLUS_SOS_EOS\n","#         self.vocab = vocab\n","#         self.vocab_counts = vocab_counts\n","\n","#         self.v2id = {v : i for i, v in enumerate(self.vocab)}\n","#         self.id2v = {val : key for key, val in self.v2id.items()}\n","    \n","#     def __len__(self):\n","#         return min(len(self.taylor_sentences), len(self.drake_sentences))\n","    \n","#     def __getitem__(self, index):\n","#         taylor_sent = self.taylor_sentences[index]\n","#         taylor_len = len(taylor_sent) + 2   # add \u003cs\u003e and \u003c/s\u003e to each sentence\n","#         taylor_id = []\n","#         for w in taylor_sent:\n","#             if w not in self.vocab:\n","#                 w = '\u003cunk\u003e'\n","#             if vocab_counts[w] \u003c= RARE_WORD_TRESHOLD:\n","#                 w = '\u003cunk\u003e'\n","#             taylor_id.append(self.v2id[w])\n","\n","#         taylor_id = ([SOS_INDEX] + taylor_id + [EOS_INDEX] + [PAD_INDEX] *\n","#                   (self.max_seq_length - taylor_len))\n","\n","#         drake_sent = self.drake_sentences[index]\n","#         drake_len = len(drake_sent) + 2   # add \u003cs\u003e and \u003c/s\u003e to each sentence\n","#         drake_id = []\n","#         for w in drake_sent:\n","#             if w not in self.vocab:\n","#                 w = '\u003cunk\u003e'\n","#             if vocab_counts[w] \u003c= RARE_WORD_TRESHOLD:\n","#                 w = '\u003cunk\u003e'\n","#             drake_id.append(self.v2id[w])\n","\n","#         drake_id = ([SOS_INDEX] + drake_id + [EOS_INDEX] + [PAD_INDEX] *\n","#                   (self.max_seq_length - drake_len))\n","\n","#         return torch.tensor(taylor_id), taylor_len, torch.tensor(drake_id), drake_len\n","\n","# dataset = TSTDataset(taylor_lyrics, drake_lyrics, vocab, vocab_counts)\n","\n","# test_pct = 0.2\n","# valid_pct = 0.1\n","\n","# train_dataset, test_dataset = torch.utils.data.random_split(dataset, [int(len(dataset)*(1-test_pct)),len(dataset)-int(len(dataset)*(1-test_pct))])\n","# valid_dataset, train_dataset = torch.utils.data.random_split(train_dataset, [int(len(dataset)*valid_pct),len(train_dataset)-int(len(dataset)*valid_pct)])\n","\n","# train_dataset = torch.save(train_dataset, '/content/drive/Shareddrives/MIT NLP 8.864/Data/train.pt')\n","# valid_dataset = torch.save(valid_dataset, '/content/drive/Shareddrives/MIT NLP 8.864/Data/valid.pt')\n","# test_dataset = torch.save(test_dataset, '/content/drive/Shareddrives/MIT NLP 8.864/Data/test.pt')\n","\n","# vocab_file = open('/content/drive/Shareddrives/MIT NLP 8.864/Data/vocab.pkl', \"wb\")\n","# pickle.dump(vocab, vocab_file)\n","# vocab_file.close()\n","\n","# vocab_file = open('/content/drive/Shareddrives/MIT NLP 8.864/Data/vocab.pkl', \"wb\")\n","# pickle.dump(vocab, vocab_file)\n","# vocab_file.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":170,"status":"ok","timestamp":1638981583815,"user":{"displayName":"Hammaad Adam","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03287553409115045279"},"user_tz":300},"id":"XHAN0LMK6Yew","outputId":"1c7ac727-9a9b-44eb-9b61-cb754ca70236"},"outputs":[{"data":{"text/plain":["17997"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["len(train_dataset) + len(valid_dataset) + len(test_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":198,"status":"ok","timestamp":1638981580128,"user":{"displayName":"Hammaad Adam","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03287553409115045279"},"user_tz":300},"id":"I3lfwY0k6Eoq","outputId":"673c60d0-213c-452e-d5ba-01c1142e80b1"},"outputs":[{"data":{"text/plain":["12598"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["len(train_dataset)"]},{"cell_type":"markdown","metadata":{"id":"ynBTHa5grUr7"},"source":["### Encoder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FieXza5erYUH"},"outputs":[],"source":["from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","\n","class Encoder(nn.Module):\n","  def __init__(self, input_size, hidden_size, dropout=0.):\n","    \"\"\"\n","    Inputs: \n","      - `input_size`: an int representing the RNN input size.\n","      - `hidden_size`: an int representing the RNN hidden size.\n","      - `dropout`: a float representing the dropout rate during training. Note\n","          that for 1-layer RNN this has no effect since dropout only applies to\n","          outputs of intermediate layers.\n","    \"\"\"\n","    super(Encoder, self).__init__()\n","    self.rnn = nn.GRU(input_size, hidden_size, num_layers=1, batch_first=True,\n","                      dropout=dropout, bidirectional=False)\n","\n","  def forward(self, inputs, lengths, init_state=None):\n","    \"\"\"\n","    Inputs:\n","      - `inputs`: a 3d-tensor of shape (batch_size, max_seq_length, embed_size)\n","          representing a batch of padded embedded word vectors of source\n","          sentences.\n","      - `lengths`: a 1d-tensor of shape (batch_size,) representing the sequence\n","          lengths of `inputs`.\n","\n","    Returns:\n","      - `outputs`: a 3d-tensor of shape\n","        (batch_size, max_seq_length, hidden_size).\n","      - `finals`: a 3d-tensor of shape (num_layers, batch_size, hidden_size).\n","      Hint: `outputs` and `finals` are both standard GRU outputs. Check:\n","      https://pytorch.org/docs/stable/nn.html#gru\n","    \"\"\"\n","    # Our variable-length inputs are padded to the same length for batching\n","    # Here we \"pack\" them for computational efficiency (see note below)\n","    packed = pack_padded_sequence(inputs, lengths.cpu(), batch_first=True,\n","                                  enforce_sorted=False)\n","    outputs, finals = self.rnn(packed, init_state)\n","    outputs, _ = pad_packed_sequence(outputs, batch_first=True,\n","                                     total_length=MAX_SENT_LENGTH_PLUS_SOS_EOS)\n","    return outputs, finals"]},{"cell_type":"markdown","metadata":{"id":"rKiJAkvZGMWl"},"source":["### Decoder"]},{"cell_type":"markdown","metadata":{"id":"q96M0-eW9OBl"},"source":["#### Generator"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E1Tdf6_283WY"},"outputs":[],"source":["class GeneratorTransferredSampled(nn.Module):\n","  \"\"\"Define standard linear + softmax generation step.\"\"\"\n","  def __init__(self, hidden_size, vocab_size, src_embed, gamma=0.001):\n","    \"\"\"\n","    Inputs:\n","      - `src_embed`: a 2d-tensor of shape (vocab_size, embed_size )\n","    \"\"\"\n","    super(GeneratorTransferredSampled, self).__init__()\n","    self.proj = nn.Linear(hidden_size, vocab_size, bias=True)\n","    self.gamma = gamma\n","    self.logsoftmax = nn.LogSoftmax(dim = 2)\n","    self.softmax = nn.Softmax(dim = 2)\n","    self.src_embed = src_embed\n","\n","  def embedding(self,x):\n","    return torch.matmul(x,self.src_embed.weight)\n","    \n","  def gumbel_softmax(self,logits, eps=1e-20):\n","    U = torch.rand(logits.shape).to(device)\n","    G = -torch.log(-torch.log(U + eps) + eps).to(device)\n","    return self.logsoftmax((logits + G) / self.gamma)\n","\n","  def gumbel(self,logits, eps=1e-20):\n","    U = torch.rand(logits.shape).to(device)\n","    G = -torch.log(-torch.log(U + eps) + eps).to(device)\n","    return (logits + G) / self.gamma\n","\n","  def forward(self, x):\n","    logits = self.proj(x)\n","    logprob = self.logsoftmax(logits)\n","    prob = self.softmax(logits)\n","    output = self.embedding(prob)\n","    word  = logits.argmax(dim = 2, keepdim = False)\n","\n","    return output, logprob, word\n","\n","  def forward_gumbel(self, x):\n","    logits = self.proj(x)\n","    prob = self.softmax(self.gumbel(logits))\n","    logprob = self.logsoftmax(self.gumbel(logits))\n","    output = self.embedding(prob)\n","    word  = logits.argmax(dim = 2, keepdim = False)\n","\n","    return output, logprob, word"]},{"cell_type":"markdown","metadata":{"id":"HeRBfORu9P49"},"source":["#### Basic Decoder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"plJpAyPu86yw"},"outputs":[],"source":["class Decoder(nn.Module):\n","  \"\"\"An RNN decoder + generator with GRU\"\"\"\n","\n","  def __init__(self, input_size, hidden_size, max_len,generator, num_layers = 1, dropout=0.):\n","    \"\"\"\n","      Inputs:\n","        - `input_size` , `hidden_size`, and `dropout` the same as in Encoder.\n","    \"\"\"\n","    super(Decoder, self).__init__()\n","    self.input_size = input_size\n","    self.hidden_size = hidden_size\n","    self.rnn = nn.GRU(input_size, hidden_size, num_layers=num_layers, batch_first=True,\n","                      dropout=dropout, bidirectional=False)\n","    self.generator = generator\n","    self.max_len = max_len\n","    self.dropout_layer = nn.Dropout(p=dropout)\n","    self.rnn_to_pre = nn.Linear(input_size + hidden_size,\n","                                hidden_size, bias=False)\n","\n","  def forward_step(self, prev_embed, hidden):\n","    \"\"\"Helper function for forward below:\n","       Perform a single decoder step (1 word).\n","\n","       Inputs:\n","      - `prev_embed`: a 3d-tensor of shape (batch_size, 1, embed_size = vocab_size)\n","          representing the padded embedded word vectors at this step in training\n","      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size) representing\n","          the current hidden state.\n","\n","      Returns:\n","      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size)\n","          representing the current decoder hidden state.\n","      - `output`: a 3d-tensor of shape (batch_size, max_len, vocab_size)\n","          representing the total generated outputs.\n","    \"\"\"\n","    pre_output, hidden = self.rnn(prev_embed, hidden)\n","    pre_output = torch.cat([prev_embed, pre_output], dim=2)\n","    pre_output = self.dropout_layer(pre_output)\n","    pre_output = self.rnn_to_pre(pre_output)\n","    pre_output = torch.tanh(pre_output)\n","\n","    return hidden, pre_output\n","\n","    ### Your code here!\n","    pre_output, hidden = self.rnn(prev_embed, hidden)\n","    pre_output = torch.cat([prev_embed, pre_output], dim=2)\n","    pre_output = self.dropout_layer(pre_output)\n","    pre_output = self.rnn_to_pre(pre_output)\n","    pre_output = self.pre_activation(pre_output)\n","    \n","  def forward_step_beam(self, prev_embed, encoder_hidden, \n","                   src_mask, proj_key, hidden):\n","    \"\"\"Beam Search only: Unroll the decoder one step at a time.\n","    Inputs:\n","      - `input`: a 3d-tensor of shape (batch_size, 1, embed_size)\n","          representing the padded embedded word vectors at this step in training\n","      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size) representing\n","          the current hidden state.\n","    Returns:\n","      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size)\n","          representing the current decoder hidden state.\n","      - `output`: a 3d-tensor of shape (batch_size, 1, vocab_size)\n","          representing the total generated outputs.    \n","      - `gumbel_logits`: a 3d-tensor of shape\n","          (batch_size, 1, trg_vocab_size) representing the mapped decoder\n","          outputs from gumbel softmax.\n","      - `output_word`: a 2d-tensor of shape\n","          (batch_size, 1) representing output sentence and\n","          the corresponding word index (can be used for embedding)    \n","      - `logits`: a 2d-tensor of shape\n","          (batch_size, 1, trg_vocab_size) representing the mapped decoder\n","          outputs from log softmax \n","          \"\"\"\n","    temp_hidden, pre_output = self.forward_step(prev_embed,encoder_hidden, \n","                   src_mask, proj_key, hidden)\n","    output, logits, output_word = self.generator.forward_gumbel(pre_output)\n","    return  temp_hidden, output, logits, output_word\n","\n","  def forward(self, input, encoder_finals,max_len, hidden=None):\n","    \"\"\"Unroll the decoder one step at a time.\n","\n","    Inputs:\n","      - `inputs`: a 3d-tensor of shape (batch_size, 1, embed_size)\n","          representing a batch of padded embedded word vectors of SOS . \n","          If size is (batch_size,max_len, embed_size), then it is teacher forcing.\n","      - `encoder_finals`: a 3d-tensor of shape\n","          (num_enc_layers, batch_size, hidden_size) representing the final\n","          encoder hidden states used to initialize the initial decoder hidden\n","          states.\n","      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size) representing\n","          the value to be used to initialize the initial decoder hidden states.\n","          If None, then use `encoder_finals`.\n","      - `max_len`: an int representing the maximum decoding length.\n","      - `style`: TAYLOR_STYLE or DRAKE_STYLE\n","\n","    Returns:\n","      - `hidden`: a 3d-tensor of shape\n","          (num_layers, batch_size, hidden_size) representing the final hidden\n","          state for each element in the batch.\n","      - `outputs`: a 3d-tensor of shape\n","          (batch_size, max_len, hidden_size) representing the raw decoder\n","          outputs (before mapping to a `trg_vocab_size`-dim vector).\n","      - `logits_vectors`: a 3d-tensor of shape\n","          (batch_size, max_len, trg_vocab_size) representing the mapped decoder\n","          outputs.\n","      - `words`: a 3d-tensor of shape\n","          (batch_size, max_len, 1) representing output sentence and\n","          the corresponding word index (can be used for embedding)  \n","    \"\"\"\n","\n","    # Initialize decoder hidden state.\n","    if hidden is None:\n","      hidden = self.init_hidden(encoder_finals)\n","    output_vectors = []\n","    logits_vectors = []\n","    words = []\n","    hidden_states = []\n","    hidden_states.append(hidden[-1][:,None,:])\n","    for i in range(max_len-1) :\n","      hidden, prev_output = self.forward_step(input,hidden)\n","      input, logits, output_word = self.generator.forward_gumbel(prev_output)\n","      # input, logits, output_word = self.generator(prev_output)\n","\n","      # input = torch.concat([input,torch.full(input.shape,style)], axis = -1)\n","      logits_vectors.append(logits)\n","      output_vectors.append(input)\n","      words.append(output_word)\n","      hidden_states.append(prev_output)\n","\n","    outputs = torch.cat(output_vectors, dim =1)\n","    logits_vectors = torch.cat(logits_vectors,dim = 1)\n","    words = torch.cat(words, axis = -1)\n","    hidden_states = torch.cat(hidden_states, axis = 1)\n","    return hidden, outputs , logits_vectors, words, hidden_states\n","\n","  def forward_teacher(self, input, encoder_finals, max_len=None, hidden=None):\n","    \"\"\"Unroll the decoder one step at a time.\n","\n","    Inputs:\n","      - `inputs`: a 3d-tensor of shape (batch_size,max_len, embed_size)\n","          representing a batch of padded embedded word vectors of original \n","          sentence and acts as  teacher forcing.\n","\n","    Returns:\n","      - `hidden`: a 3d-tensor of shape\n","          (num_layers, batch_size, hidden_size) representing the final hidden\n","          state for each element in the batch.\n","      - `outputs`: a 3d-tensor of shape\n","          (batch_size, max_len, hidden_size) representing the raw decoder\n","          outputs (before mapping to a `trg_vocab_size`-dim vector).\n","      - `logits_vectors`: a 3d-tensor of shape\n","          (batch_size, max_len, trg_vocab_size) representing the mapped decoder\n","          outputs each represents the probability? \n","      - `words`: a 3d-tensor of shape\n","          (batch_size, max_len, 1) representing output sentence and\n","          the corresponding word index (can be used for embedding)      \n","    \"\"\"\n","\n","    # Initialize decoder hidden state.\n","    if max_len is None:\n","      max_len = input.shape[1]\n","    if hidden is None:\n","      hidden = self.init_hidden(encoder_finals)\n","    output_vectors = []\n","    logits_vectors = []\n","    words = []\n","    hidden_states = []\n","    hidden_states.append(hidden[-1][:,None,:])\n","    for i in range(max_len):\n","      hidden, prev_output = self.forward_step(input[:,i:i+1,:],hidden)\n","      # output, logits, output_word = self.generator(prev_output)\n","      output, logits, output_word = self.generator.forward_gumbel(prev_output)\n","      \n","      logits_vectors.append(logits)\n","      output_vectors.append(output)\n","      words.append(output_word)\n","      hidden_states.append(prev_output)\n","\n","    outputs = torch.cat(output_vectors, dim =1)\n","    logits_vectors = torch.cat(logits_vectors,dim = 1)\n","    words = torch.cat(words, axis = -1)\n","    hidden_states = torch.cat(hidden_states, axis = 1)\n","    return hidden, outputs , logits_vectors, words, hidden_states\n","\n","  def init_hidden(self, encoder_finals):\n","    \"\"\"Use encoder final hidden state to initialize decoder's first hidden\n","       state.\n","\n","       Input: `encoder_finals` is same as in forward()\n","\n","       Returns: \n","         - `decoder_init_hiddens`: a 3d-tensor of shape \n","              (num_layers, batch_size, hidden_size) representing the initial\n","              hidden state of the decoder for each element in the batch \n","    \"\"\"\n","    decoder_init_hiddens = torch.tanh(encoder_finals)\n","    return decoder_init_hiddens"]},{"cell_type":"markdown","metadata":{"id":"iaAnYmyv9gbu"},"source":["#### Beam Search"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OA_J4dhd9ibD"},"outputs":[],"source":["from queue import PriorityQueue\n","class BeamSearchNode:\n","  def __init__(self, hiddenstate, previousNode, cur_embed, wordId, \n","               logProb,  length ):\n","    self.h = hiddenstate\n","    self.prevNode = previousNode\n","    self.cur_embed = cur_embed\n","    self.wordid = wordId\n","    self.logp = logProb\n","    self.leng = length\n","    \n","  def __lt__(self,other):\n","    return self.logp \u003c other.logp\n","\n","  def eval(self, alpha=1.0):\n","    return self.logp \n","    # Add here a function for shaping a reward\n","    # reward = 0\n","    # return self.logp / float(self.leng - 1 + 1e-6) + alpha * reward'\n","\n","class BeamSearch:\n","  def __init__(self,decoder, beam_width, topk, line_embed,max_len,\n","               max_iter=2000):\n","    \"\"\"Use Beam Search to generate a full sentence with the given decoder model\n","  Inputs:\n","      - `decoder`: decoder module with forward_step_beam function\n","      - `beam_width` : the length of the beam \n","      - `max_len`: an int representing the maximum decoding length.\n","      - `max_iter`: The maximum decoding iteration\n","    \"\"\"\n","    self.decoder = decoder\n","    self.beam_width = beam_width\n","    self.topk = topk\n","    self.line_embed = line_embed\n","    self.max_len = max_len\n","    self.max_iter = max_iter\n","\n","  def beam_decode(self, inputs, encoder_hidden, encoder_finals, src_mask, max_len,\n","                  hidden = None):\n","                  # inputs, encoder_finals,src_mask, proj_key, hidden):\n","    \"\"\"Use Beam Search to generate a full sentence with the given decoder model\n","    Inputs:\n","        - `inputs`: a 3d-tensor of shape (batch_size, 1, embed_size)\n","            representing a batch of padded embedded word vectors of SOS . \n","        - `encoder_finals`: a 3d-tensor of shape\n","            (num_enc_layers, batch_size, hidden_size) representing the final\n","            encoder hidden states used to initialize the initial decoder hidden\n","            states.\n","\n","    Returns:\n","        - `final_logp_batch`: a 2d-tensor of shape\n","            (batch_size, sentences_num) representing the probability of generating \n","            the sentence.\n","        - `final_hidden_batch`: a 4d-tensor of shape\n","            (sentences_num, num_layers, batch_size, hidden_size) representing \n","            the final hidden layer\n","        - `decoded_batch`: a 3d-tensor of shape\n","            (batch_size,sentences_num,  max_len) representing output sentence and\n","            the corresponding word index (can be used for embedding)  \n","    \n","    \"\"\"\n","    \n","    decoded_batch = []\n","    final_hidden_batch, final_logp_batch = [],[]\n","    # print(\"shape of encoder_finals:\",encoder_finals.shape)\n","    for i in range(inputs.shape[0]):\n","      if hidden is None:\n","        hidden = self.decoder.init_hidden(encoder_finals[:,i:i+1,:])\n","      decoder_input = inputs[i:i+1,:,:]\n","      # Number of sentence to generate\n","      endnodes = []\n","      number_required = self.topk\n","      proj_key = self.decoder.attention.key_layer(encoder_hidden[i:i+1,:,:])\n","\n","      # starting node -  hidden vector, previous node, cur_embed, word id , logp, length\n","      node = BeamSearchNode(self.decoder.init_hidden(encoder_finals[:,i:i+1,:]), \n","                            None, decoder_input, [[SOS_INDEX]], 0, 1)\n","      nodes = PriorityQueue()\n","\n","      nodes.put((-node.eval(), node))\n","      qsize = 1\n","\n","      while qsize\u003c=self.max_iter:\n","        tocheck = min(nodes.qsize(), self.beam_width)\n","        new_nodes = PriorityQueue()\n","        while tocheck\u003e0:\n","          score, n = nodes.get()\n","          decoder_input = n.cur_embed\n","          decoder_hidden = n.h\n","          if n.leng \u003e self.max_len:\n","            endnodes.append((score, n))\n","            # if we reached maximum # of sentences required\n","            if len(endnodes) \u003e= number_required:\n","                break\n","\n","          # decode for one step using decoder\n","          hidden, _, logsoftmax_logits, wordId = decoder.forward_step_beam(decoder_input, \n","                                                                           encoder_hidden[i:i+1,:,:],\n","                                                                          src_mask[i:i+1,:,:], \n","                                                                          proj_key, \n","                                                                          decoder_hidden)\n","          tocheck -= 1\n","          # PUT HERE REAL BEAM SEARCH OF TOP\n","          log_prob, indexes = torch.topk(logsoftmax_logits, self.beam_width)\n","          for new_k in range(self.beam_width):\n","            \n","            decoded_t = indexes[0][0][new_k].view(1, -1)\n","            log_p = log_prob[0][0][new_k]\n","            prev_embed = self.line_embed(decoded_t)\n","\n","            node = BeamSearchNode(decoder_hidden, n,prev_embed, decoded_t, \n","                                  n.logp + log_p,n.leng + 1)\n","            score = -node.eval()\n","            new_nodes.put((score, node))\n","          qsize += self.beam_width - 1\n","        nodes = new_nodes\n","\n","        if len(endnodes) \u003e= number_required:\n","            break\n","        \n","\n","      # choose nbest paths, back trace them\n","      if len(endnodes) == 0:\n","          endnodes = [nodes.get() for _ in range(self.topk)]\n","\n","      utterances = []\n","      final_logps = []\n","      final_hiddens = []\n","      # final_gumbel_logits = []\n","      for score, n in sorted(endnodes, key=operator.itemgetter(0)):\n","          end_node = n\n","          utterance = []\n","          # gumbel_logits = []\n","          utterance.append(n.wordid[0][0])\n","          # gumbel_logits =gumbel_logits + [n.gumbel_logits]\n","          # back trace\n","          while n.prevNode != None:\n","              n = n.prevNode\n","              utterance.append(n.wordid[0][0])\n","              # if n.gumbel_logits is not None:\n","              #   gumbel_logits =gumbel_logits + [n.gumbel_logits]\n","\n","          utterance = torch.unsqueeze(torch.tensor(utterance[::-1][:self.max_len]), axis = 0)\n","          utterances.append(utterance)\n","          # gumbel_logits = torch.cat(gumbel_logits, dim = 1)\n","          final_logp = end_node.logp \n","          final_hidden = end_node.h\n","          final_logps.append(final_logp)\n","          final_hiddens.append(torch.unsqueeze(torch.tensor(final_hidden), axis = 0))\n","          # final_gumbel_logits.append(gumbel_logits)\n","        \n","      utterances = torch.cat(utterances, axis = 0 )\n","      decoded_batch.append(torch.unsqueeze(utterances, axis = 0))\n","      final_logp_batch.append(torch.unsqueeze(torch.tensor(final_logps), axis = 0))\n","      final_hiddens = torch.cat(final_hiddens, axis = 0)\n","      final_hidden_batch.append(final_hiddens)\n","      # final_gumbel_logits = torch.cat(final_gumbel_logits, axis = 0)\n","      # final_gumbel_logits_batch.append(torch.unsqueeze(final_gumbel_logits, axis = 0))\n","\n","    # decoded_batch size = (batch, topk, sentence_len, 1)\n","    final_logp_batch = torch.cat(final_logp_batch, axis = 0)\n","    final_hidden_batch = torch.cat(final_hidden_batch, axis = 2)\n","    # final_gumbel_logits_batch = torch.cat(final_gumbel_logits_batch, axis = 0)\n","    decoded_batch = torch.cat(decoded_batch, axis = 0)\n","    print(\"final_logp_batch.shape:\",final_logp_batch.shape)\n","    print(\"final_hidden_batch.shape:\",final_hidden_batch.shape)\n","    # print(\"final_gumbel_logits_batch.shape:\",final_gumbel_logits_batch.shape)\n","    print(\"decoded_batch.shape:\",decoded_batch.shape)\n","    print(\"max_len:\", self.max_len)\n","    print(\"topk:\", self.topk)\n","    print(\"batch size:\", inputs.shape[0])\n","    return final_hidden_batch, final_logp_batch,  decoded_batch\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"NX25tl4G5vWa"},"source":["### Attention Decoder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VrWt0I5N6ijP"},"outputs":[],"source":["class BahdanauAttention(nn.Module):\n","    \"\"\"Implements Bahdanau (MLP) attention\"\"\"\n","    \n","    def __init__(self, hidden_size, key_size=None, query_size=None):\n","        super(BahdanauAttention, self).__init__()\n","        \n","        # We assume a bi-directional encoder so key_size is 2*hidden_size\n","        key_size = 2 * hidden_size if key_size is None else key_size\n","        query_size = hidden_size if query_size is None else query_size\n","\n","        self.key_layer = nn.Linear(key_size, hidden_size, bias=False)\n","        self.query_layer = nn.Linear(query_size, hidden_size, bias=False)\n","        self.energy_layer = nn.Linear(hidden_size, 1, bias=False)\n","        \n","        # to store attention scores\n","        self.alphas = None\n","        \n","    def forward(self, query=None, proj_key=None, value=None, mask=None):\n","        assert mask is not None, \"mask is required\"\n","\n","        # We first project the query (the decoder state).\n","        # The projected keys (the encoder states) were already pre-computated.\n","        query = self.query_layer(query)\n","        \n","        # Calculate scores.\n","        scores = self.energy_layer(torch.tanh(query + proj_key))\n","        scores = scores.squeeze(2).unsqueeze(1)\n","        \n","        # Mask out invalid positions.\n","        # The mask marks valid positions so we invert it using `mask \u0026 0`.\n","        scores.data.masked_fill_(mask == 0, -float('inf'))\n","        \n","        # Turn scores to probabilities.\n","        alphas = F.softmax(scores, dim=-1)\n","        self.alphas = alphas        \n","        \n","        # The context vector is the weighted sum of the values.\n","        context = torch.bmm(alphas, value)\n","        \n","        # context shape: [B, 1, 2D], alphas shape: [B, 1, M]\n","        return context, alphas"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KKIgeUK75xSm"},"outputs":[],"source":["class AttentionDecoder(nn.Module):\n","  \"\"\"An RNN decoder + generator with GRU\"\"\"\n","\n","  def __init__(self, input_size, hidden_size, attention, max_len,generator, num_layers = 1, dropout=0.):\n","    \"\"\"\n","      Inputs:\n","        - `input_size` , `hidden_size`, and `dropout` the same as in Encoder.\n","    \"\"\"\n","    super(AttentionDecoder, self).__init__()\n","    self.input_size = input_size\n","    self.hidden_size = hidden_size\n","    # self.rnn = nn.GRU(input_size, hidden_size, num_layers=num_layers, batch_first=True,\n","    #                   dropout=dropout, bidirectional=False)\n","    self.rnn = nn.GRU(input_size + hidden_size, hidden_size, num_layers,\n","                          batch_first=True, dropout=dropout)\n","    \n","    self.generator = generator\n","    self.max_len = max_len\n","    self.dropout_layer = nn.Dropout(p=dropout)\n","    # self.rnn_to_pre = nn.Linear(input_size + hidden_size,\n","    #                             hidden_size, bias=False)\n","    self.rnn_to_pre = nn.Linear(hidden_size + hidden_size + input_size,\n","                                hidden_size, bias=False)\n","    self.attention = attention\n","\n","  def forward_step(self, prev_embed, encoder_hidden, src_mask, proj_key, hidden):\n","    \"\"\"Helper function for forward below:\n","       Perform a single decoder step (1 word).\n","\n","       Inputs:\n","      - `prev_embed`: a 3d-tensor of shape (batch_size, 1, embed_size = vocab_size)\n","          representing the padded embedded word vectors at this step in training\n","      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size) representing\n","          the current hidden state.\n","\n","      Returns:\n","      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size)\n","          representing the current decoder hidden state.\n","      - `output`: a 3d-tensor of shape (batch_size, max_len, vocab_size)\n","          representing the total generated outputs.\n","    \"\"\"\n","\n","    # compute context vector using attention mechanism\n","    query = hidden[-1].unsqueeze(1)  # [#layers, B, D] -\u003e [B, 1, D]\n","    context, attn_probs = self.attention(\n","        query=query, proj_key=proj_key,\n","        value=encoder_hidden, mask=src_mask)\n","    \n","    # RNN\n","    rnn_input = torch.cat([prev_embed, context], dim=2)\n","\n","    output, hidden = self.rnn(rnn_input, hidden)\n","    \n","    pre_output = torch.cat([prev_embed, output, context], dim=2)\n","    pre_output = self.dropout_layer(pre_output)\n","    pre_output = self.rnn_to_pre(pre_output)\n","    \n","    return hidden, pre_output\n","\n","  def forward_step_beam(self, prev_embed, encoder_hidden, \n","                   src_mask, proj_key, hidden):\n","    \"\"\"Beam Search only: Unroll the decoder one step at a time.\"\"\"\n","    temp_hidden, pre_output = self.forward_step(prev_embed,encoder_hidden, \n","                   src_mask, proj_key, hidden)\n","    output, logits, output_word = self.generator.forward_gumbel(pre_output)\n","    return  temp_hidden, output, logits, output_word\n","\n","  def forward(self, input, encoder_hidden, encoder_finals, src_mask, max_len, hidden=None):\n","    \"\"\"Unroll the decoder one step at a time.\n","\n","    Inputs:\n","      - `inputs`: a 3d-tensor of shape (batch_size, 1, embed_size)\n","          representing a batch of padded embedded word vectors of SOS . \n","          If size is (batch_size,max_len, embed_size), then it is teacher forcing.\n","      - `encoder_finals`: a 3d-tensor of shape\n","          (num_enc_layers, batch_size, hidden_size) representing the final\n","          encoder hidden states used to initialize the initial decoder hidden\n","          states.\n","      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size) representing\n","          the value to be used to initialize the initial decoder hidden states.\n","          If None, then use `encoder_finals`.\n","      - `max_len`: an int representing the maximum decoding length.\n","      - `style`: TAYLOR_STYLE or DRAKE_STYLE\n","\n","    Returns:\n","      - `hidden`: a 3d-tensor of shape\n","          (num_layers, batch_size, hidden_size) representing the final hidden\n","          state for each element in the batch.\n","      - `outputs`: a 3d-tensor of shape\n","          (batch_size, max_len, hidden_size) representing the raw decoder\n","          outputs (before mapping to a `trg_vocab_size`-dim vector).\n","      - `logits_vectors`: a 3d-tensor of shape\n","          (batch_size, max_len, trg_vocab_size) representing the mapped decoder\n","          outputs.\n","      - `words`: a 3d-tensor of shape\n","          (batch_size, max_len, 1) representing output sentence and\n","          the corresponding word index (can be used for embedding)  \n","    \"\"\"\n","\n","    # Initialize decoder hidden state.\n","    if hidden is None:\n","      hidden = self.init_hidden(encoder_finals)\n","\n","    proj_key = self.attention.key_layer(encoder_hidden)\n","\n","    output_vectors = []\n","    logits_vectors = []\n","    words = []\n","    hidden_states = []\n","    hidden_states.append(hidden[-1][:,None,:])\n","    \n","    for i in range(max_len-1) :\n","      \n","      hidden, prev_output = self.forward_step(input,encoder_hidden, src_mask, proj_key, hidden)\n","      input, logits, output_word = self.generator.forward_gumbel(prev_output)\n","\n","      logits_vectors.append(logits)\n","      output_vectors.append(input)\n","      words.append(output_word)\n","      hidden_states.append(prev_output)\n","\n","    outputs = torch.cat(output_vectors, dim =1)\n","    logits_vectors = torch.cat(logits_vectors,dim = 1)\n","    words = torch.cat(words, axis = -1)\n","    hidden_states = torch.cat(hidden_states, axis = 1)\n","\n","    return hidden, outputs , logits_vectors, words, hidden_states\n","  \n","  def forward_teacher(self, input, encoder_hidden, encoder_finals, src_mask, max_len=None, hidden=None):\n","    \"\"\"Unroll the decoder one step at a time.\n","\n","    Inputs:\n","      - `inputs`: a 3d-tensor of shape (batch_size,max_len, embed_size)\n","          representing a batch of padded embedded word vectors of original \n","          sentence and acts as  teacher forcing.\n","\n","    Returns:\n","      - `hidden`: a 3d-tensor of shape\n","          (num_layers, batch_size, hidden_size) representing the final hidden\n","          state for each element in the batch.\n","      - `outputs`: a 3d-tensor of shape\n","          (batch_size, max_len, hidden_size) representing the raw decoder\n","          outputs (before mapping to a `trg_vocab_size`-dim vector).\n","      - `logits_vectors`: a 3d-tensor of shape\n","          (batch_size, max_len, trg_vocab_size) representing the mapped decoder\n","          outputs each represents the probability? \n","      - `words`: a 3d-tensor of shape\n","          (batch_size, max_len, 1) representing output sentence and\n","          the corresponding word index (can be used for embedding)      \n","    \"\"\"\n","\n","    # Initialize decoder hidden state.\n","    if max_len is None:\n","      max_len = input.shape[1]\n","    if hidden is None:\n","      hidden = self.init_hidden(encoder_finals)\n","    \n","    proj_key = self.attention.key_layer(encoder_hidden)\n","\n","    output_vectors = []\n","    logits_vectors = []\n","    words = []\n","    hidden_states = []\n","    hidden_states.append(hidden[-1][:,None,:])\n","\n","    for i in range(max_len):\n","      hidden, prev_output = self.forward_step(input[:,i:i+1,:], encoder_hidden, src_mask, proj_key, hidden)\n","      \n","      output, logits, output_word = self.generator.forward_gumbel(prev_output)\n","      logits_vectors.append(logits)\n","      output_vectors.append(output)\n","      words.append(output_word)\n","      hidden_states.append(prev_output)\n","\n","    outputs = torch.cat(output_vectors, dim =1)\n","    logits_vectors = torch.cat(logits_vectors,dim = 1)\n","    words = torch.cat(words, axis = -1)\n","    hidden_states = torch.cat(hidden_states, axis = 1)\n","    return hidden, outputs , logits_vectors, words, hidden_states\n","\n","  def init_hidden(self, encoder_finals):\n","    \"\"\"Use encoder final hidden state to initialize decoder's first hidden\n","       state.\n","\n","       Input: `encoder_finals` is same as in forward()\n","\n","       Returns: \n","         - `decoder_init_hiddens`: a 3d-tensor of shape \n","              (num_layers, batch_size, hidden_size) representing the initial\n","              hidden state of the decoder for each element in the batch \n","    \"\"\"\n","    decoder_init_hiddens = torch.tanh(encoder_finals)\n","    return decoder_init_hiddens"]},{"cell_type":"markdown","metadata":{"id":"kxvEOOu_9Zkt"},"source":["#### EncoderDecoderAttention (in progress)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":188253,"status":"ok","timestamp":1637773273265,"user":{"displayName":"Hammaad Adam","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03287553409115045279"},"user_tz":300},"id":"bJgy5Rqt7efj","outputId":"7e5c2b37-c7cd-4906-94cb-71ff411918a4"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n"]},{"name":"stdout","output_type":"stream","text":["Finished Training Epoch  0\n","Training PPL 186.3012933666054\n","Valid PPL 36.84848284237294\n","Finished Training Epoch  1\n","Training PPL 13.585992878438688\n","Valid PPL 7.053865752802458\n","Finished Training Epoch  2\n","Training PPL 4.822962307204506\n","Valid PPL 4.504301768389754\n"]}],"source":["### Work in progress\n","class EncoderDecoderAttention(nn.Module):\n","  def __init__(self, encoder, decoder, line_embed, generator):\n","    super(EncoderDecoderAttention, self).__init__()\n","\n","    self.encoder = encoder\n","    self.decoder = decoder\n","    self.line_embed = line_embed\n","    self.generator = generator\n","\n","  def forward(self, lines, line_lens):\n","    encoder_hidden, encoder_finals = self.encode(lines, line_lens)\n","    src_mask = (lines != PAD_INDEX).unsqueeze(-2)\n","    # return self.reconstruct(encoder_hidden, encoder_finals, lines[:, :-1], src_mask)\n","    return self.reconstruct(encoder_hidden, encoder_finals, lines[:, :-1], src_mask), self.decode(encoder_hidden, encoder_finals, src_mask)\n","\n","  def encode(self, lines, line_lens):\n","    return self.encoder(self.line_embed(lines), line_lens)\n","    \n","  def reconstruct(self, encoder_hidden, h0, lines, src_mask):\n","    original = self.line_embed(lines)\n","    return self.decoder.forward_teacher(original,encoder_hidden, h0, src_mask)\n","\n","  def decode(self, encoder_hidden, h0, src_mask):\n","    target = self.line_embed(torch.tensor([SOS_INDEX]).repeat(h0.size()[1],1).to(device))\n","    return self.decoder.forward(target, encoder_hidden, h0, src_mask, max_len)\n","\n","epochs = 3\n","lr = 1e-3\n","batch_size = 32\n","print_every = 100\n","max_len = dataset.max_seq_length\n","vocab_size = len(vocab)\n","embed_size = 256\n","hidden_size = 256\n","dropout = 0.2\n","gamma = 0.001\n","\n","train_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","valid_loader = data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n","\n","line_embed = nn.Embedding(vocab_size, embed_size)\n","encoder = Encoder(embed_size,hidden_size)\n","generator = GeneratorTransferredSampled(hidden_size,vocab_size, line_embed, gamma = gamma)\n","attention = BahdanauAttention(hidden_size, key_size=hidden_size)\n","decoder = AttentionDecoder(embed_size, hidden_size, attention=attention, max_len=vocab_size, generator = generator,dropout=dropout)\n","model = EncoderDecoderAttention(encoder, decoder, line_embed, generator).to(device)\n","optimizer_model = torch.optim.Adam(model.parameters(), lr=lr)\n","rec_loss = nn.NLLLoss(reduction=\"mean\",ignore_index = PAD_INDEX)\n","\n","for epoch in range(epochs):\n","  epoch_rec_loss = 0\n","  epoch_tokens = 0\n","  model.train()\n","  for i,(taylor_lines, taylor_len, drake_lines, drake_len) in enumerate(train_loader):\n","    lines = torch.cat((taylor_lines, drake_lines), 0).to(device)    \n","    line_lens = torch.cat((taylor_len, drake_len), 0).to(device)\n","\n","    # Train model\n","    rec_orig,dec_orig  = model(lines, line_lens)   \n","    # rec_orig  = model(lines, line_lens)    \n","    loss_rec = rec_loss(input=dec_orig[2].permute(0,2,1), target=lines[:, 1:])\n","\n","    optimizer_model.zero_grad()\n","    loss_rec.backward()\n","    optimizer_model.step()\n","    \n","    epoch_rec_loss += loss_rec.item() * line_lens.sum().item()\n","    epoch_tokens += line_lens.sum().item()\n","\n","  print(\"Finished Training Epoch \", epoch)\n","  print(\"Training PPL\", np.exp(epoch_rec_loss / float(epoch_tokens)))\n","  val_loss = 0\n","  val_tokens = 0\n","\n","  for i,(taylor_lines, taylor_len, drake_lines, drake_len) in enumerate(valid_loader):\n","    lines = torch.cat((taylor_lines, drake_lines), 0).to(device)    \n","    line_lens = torch.cat((taylor_len, drake_len), 0).to(device)\n","\n","    rec_orig,dec_orig  = model(lines, line_lens)  \n","    # rec_orig  = model(lines, line_lens)    \n","    loss_rec = rec_loss(input=dec_orig[2].permute(0,2,1), target=lines[:, 1:])\n","\n","    val_loss += loss_rec.item() * line_lens.sum().item()\n","    val_tokens += line_lens.sum().item()\n","  \n","  print(\"Valid PPL\", np.exp(val_loss / float(val_tokens)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n-8K-9QC4W25"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":135,"status":"ok","timestamp":1637773052647,"user":{"displayName":"Hammaad Adam","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03287553409115045279"},"user_tz":300},"id":"UO_EaE4pb-wm","outputId":"685e9ae9-29fa-4682-ce2c-351f72f3b770"},"outputs":[{"data":{"text/plain":["tensor([[ 4619,  9239,  9239,  ...,  9268, 12570,  9268],\n","        [ 6947,  2453,  6338,  ...,  7785, 10651, 10651],\n","        [ 7670, 12181,  6029,  ..., 12186,  3635, 12570],\n","        ...,\n","        [ 7372,  3998, 10130,  ...,  8616,  2051,  7983],\n","        [ 9077, 10213,  9280,  ...,  1119,  3461,  7149],\n","        [ 5248,  9280, 12234,  ..., 11901,  6566,  9320]], device='cuda:0')"]},"execution_count":43,"metadata":{},"output_type":"execute_result"}],"source":["dec_orig[3]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1179,"status":"ok","timestamp":1637774415076,"user":{"displayName":"Hammaad Adam","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03287553409115045279"},"user_tz":300},"id":"Dznyd7lnV-vr","outputId":"a0625e71-77da-4aae-c8cd-d34f77b4ee0f"},"outputs":[{"name":"stdout","output_type":"stream","text":["['\u003cs\u003e', 'i', 'just', ',', 'i', 'ca', \"n't\", ',', 'i', 'just', 'ca', \"n't\", 'be', 'lovin', \"'\", 'you', 'no', 'more', ',', 'i', 'love', 'you', 'more', 'than', 'i', 'love', 'myself', '\u003c/s\u003e', '\u003cpad\u003e', '\u003cpad\u003e', '\u003cpad\u003e', '\u003cpad\u003e']\n","['i', 'just', ',', 'i', 'ca', \"n't\", ',', 'i', 'just', 'ca', \"n't\", 'be', 'lovin', \"'\", 'no', 'no', 'more', ',', 'i', 'love', 'more', 'more', 'than', 'i', '\u003c/s\u003e', 'myself', '\u003c/s\u003e', '\u003c/s\u003e', '\u003c/s\u003e', '\u003c/s\u003e', '\u003c/s\u003e']\n","['i', 'just', ',', 'i', 'ca', \"n't\", ',', 'i', 'just', 'ca', \"n't\", 'be', 'lovin', \"'\", 'you', 'more', 'more', ',', 'i', 'love', 'more', 'more', 'than', 'i', 'myself', '\u003c/s\u003e', '\u003c/s\u003e', '\u003c/s\u003e', '\u003c/s\u003e', '\u003c/s\u003e', '\u003c/s\u003e']\n"]}],"source":["def lookup_words(x, vocab):\n","  return [vocab[i] for i in x]\n","\n","idx=17\n","print(lookup_words(lines[idx], vocab))\n","print(lookup_words(rec_orig[3][idx], vocab))\n","print(lookup_words(dec_orig[3][idx], vocab))"]},{"cell_type":"markdown","metadata":{"id":"t2mg2wYk30dv"},"source":["### Encoder-Decoder for Testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SlnB42ru9uBI"},"outputs":[],"source":["# ### Work in progress\n","# class EncoderDecoder(nn.Module):\n","#   def __init__(self, encoder, decoder, line_embed, generator):\n","#     super(EncoderDecoder, self).__init__()\n","\n","#     self.encoder = encoder\n","#     self.decoder = decoder\n","#     self.line_embed = line_embed\n","#     self.generator = generator\n","\n","#   def forward(self, lines, line_lens):\n","#     encoder_hiddens, encoder_finals = self.encode(lines, line_lens)\n","#     del encoder_hiddens\n","#     return self.reconstruct(encoder_finals, lines[:, :-1]), self.decode(encoder_finals)\n","\n","#   def encode(self, lines, line_lens):\n","#     return self.encoder(self.line_embed(lines), line_lens)\n","    \n","#   def reconstruct(self, h0, lines):\n","#     original = self.line_embed(lines)\n","#     return self.decoder.forward_teacher(original,h0)\n","\n","#   def decode(self, h0):\n","#     target = self.line_embed(torch.tensor([SOS_INDEX]).repeat(h0.size()[1],1).to(device))\n","#     return self.decoder.forward(target,h0,max_len)\n","\n","# epochs = 3\n","# lr = 1e-3\n","# batch_size = 32\n","# print_every = 100\n","# max_len = dataset.max_seq_length\n","# vocab_size = len(vocab)\n","# embed_size = 256\n","# hidden_size = 256\n","# dropout = 0.2\n","# gamma = 0.001\n","\n","# train_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","# valid_loader = data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n","# test_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n","\n","# line_embed = nn.Embedding(vocab_size, embed_size)\n","# encoder = Encoder(embed_size,hidden_size)\n","# generator = GeneratorTransferredSampled(hidden_size,vocab_size, line_embed, gamma = gamma)\n","# decoder = Decoder(embed_size, hidden_size, max_len=vocab_size, generator = generator,dropout=dropout)\n","# model = EncoderDecoder(encoder, decoder, line_embed, generator).to(device)\n","# optimizer_model = torch.optim.Adam(model.parameters(), lr=lr)\n","# rec_loss = nn.NLLLoss(reduction=\"mean\",ignore_index = PAD_INDEX)\n","\n","# for epoch in range(epochs):\n","#   epoch_rec_loss = 0\n","#   epoch_tokens = 0\n","#   model.train()\n","#   for i,(taylor_lines, taylor_len, drake_lines, drake_len) in enumerate(train_loader):\n","#     lines = torch.cat((taylor_lines, drake_lines), 0).to(device)    \n","#     line_lens = torch.cat((taylor_len, drake_len), 0).to(device)\n","\n","#     # Train model\n","#     rec_orig,dec_orig  = model(lines, line_lens)    \n","#     loss_rec = rec_loss(input=rec_orig[2].permute(0,2,1), target=lines[:, 1:])\n","\n","#     optimizer_model.zero_grad()\n","#     loss_rec.backward()\n","#     optimizer_model.step()\n","    \n","#     epoch_rec_loss += loss_rec.item() * line_lens.sum().item()\n","#     epoch_tokens += line_lens.sum().item()\n","\n","#   print(\"Finished Training Epoch \", epoch)\n","#   print(\"Training PPL\", np.exp(epoch_rec_loss / float(epoch_tokens)))\n","#   val_loss = 0\n","#   val_tokens = 0\n","\n","#   for i,(taylor_lines, taylor_len, drake_lines, drake_len) in enumerate(valid_loader):\n","#     lines = torch.cat((taylor_lines, drake_lines), 0).to(device)    \n","#     line_lens = torch.cat((taylor_len, drake_len), 0).to(device)\n","\n","#     rec_orig,dec_orig  = model(lines, line_lens)    \n","#     loss_rec = rec_loss(input=rec_orig[2].permute(0,2,1), target=lines[:, 1:])\n","\n","#     val_loss += loss_rec.item() * line_lens.sum().item()\n","#     val_tokens += line_lens.sum().item()\n","  \n","#   print(\"Valid PPL\", np.exp(val_loss / float(val_tokens)))"]},{"cell_type":"markdown","metadata":{"id":"wg_FbKl-T9xd"},"source":["### Classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SeDjRElB65UC"},"outputs":[],"source":["class LSTMDiscriminator(nn.Module):\n","  def __init__(self, input_size, hidden_size, LSTMlayers=1, dropout = 0.5):\n","    super(LSTMDiscriminator, self).__init__()\n","\n","    self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=LSTMlayers, \n","                        batch_first=True, bidirectional=True)\n","    self.drop = nn.Dropout(p=dropout)\n","    self.fc = nn.Linear(2*hidden_size, 1)\n","    self.hidden_size = hidden_size\n","\n","  def forward(self, text_emb, text_len):\n","    text_len[text_len==0] += 1\n","\n","    packed_input = pack_padded_sequence(text_emb, text_len.cpu(), batch_first=True, enforce_sorted=False)\n","    packed_output, _ = self.lstm(packed_input)\n","    output, _ = pad_packed_sequence(packed_output, batch_first=True)\n","\n","    out_forward = output[range(len(output)), text_len - 1, :self.hidden_size]\n","    out_reverse = output[:, 0, self.hidden_size:]\n","    out_reduced = torch.cat((out_forward, out_reverse), 1)\n","    text_fea = self.drop(out_reduced)\n","\n","    text_fea = self.fc(text_fea)\n","    text_fea = torch.squeeze(text_fea, 1)\n","    text_out = torch.sigmoid(text_fea)\n","\n","    return text_out\n","    \n","class LSTMClassifier(nn.Module):\n","\n","    def __init__(self, dimension=128):\n","        super(LSTMClassifier, self).__init__()\n","\n","        self.embedding = nn.Linear(len(vocab), 300)\n","        self.dimension = dimension\n","        self.lstm = nn.LSTM(input_size=300,\n","                            hidden_size=dimension,\n","                            num_layers=1,\n","                            batch_first=True,\n","                            bidirectional=True)\n","        self.drop = nn.Dropout(p=0.5)\n","\n","        self.fc = nn.Linear(2*dimension, 1)\n","\n","    def forward(self, text, text_len):\n","\n","        text_emb = self.embedding(text)\n","        text_len[text_len==0] += 1\n","\n","        packed_input = pack_padded_sequence(text_emb, text_len.cpu(), batch_first=True, enforce_sorted=False)\n","        packed_output, _ = self.lstm(packed_input)\n","        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n","\n","        out_forward = output[range(len(output)), text_len - 1, :self.dimension]\n","        out_reverse = output[:, 0, self.dimension:]\n","        out_reduced = torch.cat((out_forward, out_reverse), 1)\n","        text_fea = self.drop(out_reduced)\n","\n","        text_fea = self.fc(text_fea)\n","        text_fea = torch.squeeze(text_fea, 1)\n","        text_out = torch.sigmoid(text_fea)\n","\n","        return text_out"]},{"cell_type":"markdown","metadata":{"id":"_xuubDG0F5i3"},"source":["### Training"]},{"cell_type":"markdown","metadata":{"id":"9XEGiaaa93DR"},"source":["#### TSTModel"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yjs1mzrQEyef"},"outputs":[],"source":["class TSTModel(nn.Module):\n","  def __init__(self, max_len, vocab_size, embed_size, hidden_size_z, hidden_size_y, line_embed, encoder, generator, decoder, classifier):\n","    super(TSTModel, self).__init__()\n","\n","    self.hidden_size = hidden_size_y + hidden_size_z\n","\n","    self.encoder = encoder\n","    self.generator = generator\n","    self.decoder = decoder\n","    self.classifier = classifier\n","\n","    self.line_embed = line_embed\n","    self.y_embed_enc = nn.Embedding(2,hidden_size_y)\n","    self.y_embed_gen = nn.Embedding(2,hidden_size_y)\n","\n","    self.max_len = max_len\n","    self.vocab_size = vocab_size\n","    self.embed_size = embed_size\n","    self.hidden_size_z = hidden_size_z\n","    self.hidden_size_y = hidden_size_y\n","\n","  def forward(self, lines, line_lens, labels):\n","\n","    encoder_hidden, encoded_lines = self.encode(lines, line_lens, labels)\n","    z = encoded_lines[-1][:,self.hidden_size_y:]\n","\n","    h0_orig = torch.cat((self.y_embed_gen(labels),z), 1)[None,:]\n","    h0_tsf = torch.cat((self.y_embed_gen(1-labels),z), 1)[None,:]\n","\n","    # Decode back into original form for reconstruction\n","    rec_orig = self.reconstruct(h0_orig, lines[:, :-1])\n","\n","    # Decode into original and transferred forms for classification\n"," \n","    decode_orig = self.decode(h0_orig)\n","    decode_tsf = self.decode(h0_tsf)\n","    \n","    half = int(lines.size(0) / 2)\n","\n","    discrim1_input = torch.cat((rec_orig[4][:half], decode_tsf[4][half:]))\n","    discrim0_input = torch.cat((rec_orig[4][half:], decode_tsf[4][:half]))\n","\n","    classifier_lines = torch.cat((torch.exp(decode_orig[2]), torch.exp(decode_tsf[2]), F.one_hot(lines[:,1:], self.vocab_size).to(torch.float)), 0)\n","    \n","    rec_orig_len = first_eos(rec_orig[3]) + 1\n","    decode_orig_len = first_eos(decode_orig[3]) + 1\n","    decode_tsf_len = first_eos(decode_tsf[3]) + 1\n","\n","    classifier_line_lens = torch.cat((decode_orig_len, decode_tsf_len, line_lens),0)\n","    # classifier_line_lens = torch.cat((line_lens, line_lens, line_lens),0)\n","    discrim0_lens = torch.cat((rec_orig_len[half:], decode_tsf_len[:half]))\n","    discrim1_lens = torch.cat((rec_orig_len[:half], decode_tsf_len[half:]))\n","\n","    pred_class = self.classifier(classifier_lines, classifier_line_lens-1)\n","\n","    # return rec_orig, decode_orig\n","    return rec_orig, pred_class, decode_orig, decode_tsf, (discrim0_input, discrim0_lens), (discrim1_input, discrim1_lens)\n","\n","  def encode(self, lines, line_lens, labels):\n","    init_state = torch.cat((self.y_embed_enc(labels), torch.zeros((len(lines),self.hidden_size_z), device=device)), 1)[None,:].to(device)\n","    return self.encoder(self.line_embed(lines), line_lens, init_state)\n","\n","  def reconstruct(self, h0, lines):\n","    original = self.line_embed(lines)\n","    return self.decoder.forward_teacher(original,h0)\n","\n","  def decode(self, h0):\n","    target = self.line_embed(torch.tensor([SOS_INDEX]).repeat(h0.size()[1],1).to(device))\n","    return self.decoder.forward(target,h0,self.max_len)\n","\n","def first_eos(x):\n","  eos_pos = (x == EOS_INDEX)\n","  found, indices = ((eos_pos.cumsum(1) == 1) \u0026 eos_pos).max(1)\n","  indices = indices + (~found*x.size(1))\n","  return indices"]},{"cell_type":"markdown","metadata":{"id":"C1-4ipH195Oe"},"source":["#### TSTModelAttention"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5dN3kpsaZpFX"},"outputs":[],"source":["class TSTModelAttention(nn.Module):\n","  def __init__(self, max_len, vocab_size, embed_size, hidden_size_z, \n","               hidden_size_y, line_embed, encoder, generator, decoder, \n","               classifier,beamSeasrch):\n","    super(TSTModelAttention, self).__init__()\n","\n","    self.hidden_size = hidden_size_y + hidden_size_z\n","\n","    self.encoder = encoder\n","    self.generator = generator\n","    self.decoder = decoder\n","    self.classifier = classifier\n","\n","    self.beamSeasrch = beamSeasrch\n","\n","    self.line_embed = line_embed\n","    self.y_embed_enc = nn.Embedding(2,hidden_size_y)\n","    self.y_embed_gen = nn.Embedding(2,hidden_size_y)\n","\n","    self.max_len = max_len\n","    self.vocab_size = vocab_size\n","    self.embed_size = embed_size\n","    self.hidden_size_z = hidden_size_z\n","    self.hidden_size_y = hidden_size_y\n","\n","  def forward(self, lines, line_lens, labels):\n","\n","    src_mask = (lines != PAD_INDEX).unsqueeze(-2)\n","    encoder_hidden, encoder_finals = self.encode(lines, line_lens, labels)\n","\n","    z = encoder_finals[-1][:,self.hidden_size_y:]\n","\n","    h0_orig = torch.cat((self.y_embed_gen(labels),z), 1)[None,:]\n","    h0_tsf = torch.cat((self.y_embed_gen(1-labels),z), 1)[None,:]\n","\n","    # Decode back into original form for reconstruction\n","    rec_orig = self.reconstruct(encoder_hidden, h0_orig, lines[:, :-1], src_mask)\n","\n","    # Decode into original and transferred forms for classification\n","    decode_orig = self.decode(encoder_hidden, h0_orig, src_mask)\n","    decode_tsf = self.decode(encoder_hidden, h0_tsf, src_mask)\n","    \n","    half = int(lines.size(0) / 2)\n","\n","    discrim1_input = torch.cat((rec_orig[4][:half], decode_tsf[4][half:]))\n","    discrim0_input = torch.cat((rec_orig[4][half:], decode_tsf[4][:half]))\n","\n","    classifier_lines = torch.cat((torch.exp(decode_orig[2]), torch.exp(decode_tsf[2]), F.one_hot(lines[:,1:], self.vocab_size).to(torch.float)), 0)\n","\n","    rec_orig_len = first_eos(rec_orig[3]) + 1\n","    decode_orig_len = first_eos(decode_orig[3]) + 1\n","    decode_tsf_len = first_eos(decode_tsf[3]) + 1\n","\n","    classifier_line_lens = torch.cat((decode_orig_len, decode_tsf_len, line_lens),0)\n","    # classifier_line_lens = torch.cat((line_lens, line_lens, line_lens),0)\n","    discrim0_lens = torch.cat((rec_orig_len[half:], decode_tsf_len[:half]))\n","    discrim1_lens = torch.cat((rec_orig_len[:half], decode_tsf_len[half:]))\n","\n","    pred_class = self.classifier(classifier_lines, classifier_line_lens-1)\n","    \n","    # return rec_orig, pred_class, decode_orig, decode_tsf, (discrim0_input, line_lens), (discrim1_input, line_lens)\n","    return rec_orig, pred_class, decode_orig, decode_tsf, (discrim0_input, discrim0_lens), (discrim1_input, discrim1_lens)\n","\n","  def forward_beam(self,lines, line_lens, labels):\n","    src_mask = (lines != PAD_INDEX).unsqueeze(-2)\n","    encoder_hidden, encoder_finals = self.encode(lines, line_lens, labels)\n","    z = encoder_finals[-1][:,self.hidden_size_y:]\n","\n","    h0_orig = torch.cat((self.y_embed_gen(labels),z), 1)[None,:]\n","    h0_tsf = torch.cat((self.y_embed_gen(1-labels),z), 1)[None,:]\n","\n","    decode_orig = self.decode_beam(encoder_hidden, h0_orig, src_mask)\n","    decode_tsf = self.decode_beam(encoder_hidden, h0_tsf, src_mask)\n","\n","    return decode_orig, decode_tsf #rec_orig, ,pred_class,  (discrim0_input, line_lens), (discrim1_input, line_lens)\n","\n","  \n","  def decode_beam(self,encoder_hidden,h0,src_mask):\n","    target = self.line_embed(torch.tensor([SOS_INDEX]).repeat(h0.size()[1],1).to(device))\n","    return self.beamSeasrch.beam_decode(target, encoder_hidden,h0,src_mask, self.max_len)\n","\n","  def encode(self, lines, line_lens, labels):\n","    init_state = torch.cat((self.y_embed_enc(labels), torch.zeros((len(lines),self.hidden_size_z), device=device)), 1)[None,:].to(device)\n","    return self.encoder(self.line_embed(lines), line_lens, init_state)\n","\n","  def reconstruct(self, encoder_hidden, h0, lines, src_mask):\n","    original = self.line_embed(lines)\n","    return self.decoder.forward_teacher(original,encoder_hidden, h0, src_mask)\n","\n","  def decode(self, encoder_hidden, h0, src_mask):\n","    target = self.line_embed(torch.tensor([SOS_INDEX]).repeat(h0.size()[1],1).to(device))\n","    return self.decoder.forward(target, encoder_hidden, h0, src_mask, self.max_len)"]},{"cell_type":"markdown","metadata":{"id":"BoCYL-kD-Q4o"},"source":["#### Training"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":223},"executionInfo":{"elapsed":9781,"status":"error","timestamp":1638758085712,"user":{"displayName":"Hammaad Adam","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03287553409115045279"},"user_tz":300},"id":"O8jaGcIiNQUi","outputId":"94e24996-6010-40de-acd4-7c6e112589be"},"outputs":[{"ename":"NameError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-18-cb9064264de4\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mdiscriminator1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTMDiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 39\u001b[0;31m \u001b[0mstop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpre_train_classifier\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"]}],"source":["# Changing learning rate can affect things\n","# Focusing on reconstruction first, then later building in discriminator or classifier seems to help\n","# Without attention, autoencoder needs at least 7-8 epochs to get reasonable reconstructions\n","\n","attention = True\n","classify = False\n","discriminate = True\n","\n","pre_train_classifier = False\n","\n","epochs = 10\n","class_epochs = 2\n","lr = 1e-3\n","batch_size = 32\n","print_every = 100\n","\n","max_len = MAX_SENT_LENGTH_PLUS_SOS_EOS\n","vocab_size = len(vocab)\n","embed_size = 100\n","hidden_size_z = 500\n","hidden_size_y = 200\n","hidden_size = hidden_size_z + hidden_size_y\n","dropout = 0.2\n","gamma = 0.1\n","\n","TAYLOR_STYLE=1 # for information only, don't change\n","DRAKE_STYLE=0  # for information only, don't change\n","train_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","valid_loader = data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n","\n","line_embed = nn.Embedding(vocab_size, embed_size)\n","encoder = Encoder(embed_size,hidden_size)\n","generator = GeneratorTransferredSampled(hidden_size,vocab_size, line_embed, gamma = gamma)\n","classifier = LSTMClassifier()\n","discriminator0 = LSTMDiscriminator(hidden_size, hidden_size).to(device)\n","discriminator1 = LSTMDiscriminator(hidden_size, hidden_size).to(device)\n","\n","stop\n","\n","if pre_train_classifier:\n","  classifier = classifier.to(device)\n","  optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-3) \n","  class_loss = nn.BCELoss()\n","\n","  for epoch in range(class_epochs):\n","    correct = 0\n","    classifier.train()\n","    for i,(taylor_lines, taylor_len, drake_lines, drake_len) in enumerate(train_loader):\n","      lines = torch.cat((taylor_lines, drake_lines), 0).to(device)  \n","      classifier_lines = F.one_hot(lines[:,1:], len(vocab)).to(torch.float).to(device)\n","\n","      line_lens = torch.cat((taylor_len, drake_len), 0).to(device)\n","      labels = torch.cat((torch.ones(size=(len(taylor_lines),), dtype=torch.int32),torch.zeros(size=(len(drake_lines),),dtype=torch.int32))).to(device)\n","\n","      pred_class = classifier(classifier_lines, line_lens-1)\n","      loss_class = class_loss(input=pred_class, target=labels.to(torch.float))\n","\n","      optimizer.zero_grad()\n","      loss_class.backward()\n","      optimizer.step()\n","\n","      correct += torch.sum((pred_class \u003e= 0.5) == labels)\n","    print(\"Pre-Training Accuracy: \", correct / float(2*len(train_dataset)))\n","    classifier.eval()\n","    correct = 0\n","    for i,(taylor_lines, taylor_len, drake_lines, drake_len) in enumerate(valid_loader):\n","      lines = torch.cat((taylor_lines, drake_lines), 0).to(device)  \n","      classifier_lines = F.one_hot(lines[:,1:], len(vocab)).to(torch.float).to(device)\n","\n","      line_lens = torch.cat((taylor_len, drake_len), 0).to(device)\n","      labels = torch.cat((torch.ones(size=(len(taylor_lines),), dtype=torch.int32),torch.zeros(size=(len(drake_lines),),dtype=torch.int32))).to(device)\n","\n","      pred_class = classifier(classifier_lines, line_lens-1)\n","      correct += torch.sum((pred_class \u003e= 0.5) == labels)\n","    print(\"Pre-Valid Accuracy: \", correct / float(2*len(valid_dataset)))\n","\n","if attention:\n","  attention_mech = BahdanauAttention(hidden_size, key_size=hidden_size)\n","  decoder = AttentionDecoder(embed_size, hidden_size, attention=attention_mech, max_len=vocab_size, generator = generator,dropout=dropout)\n","  # BEAM SEARCH\n","  beamSeasrch = BeamSearch(decoder, 3,3,line_embed,max_len)\n","  model = TSTModelAttention(max_len, vocab_size, embed_size, hidden_size_z, hidden_size_y, line_embed, encoder, generator, decoder, classifier,beamSeasrch).to(device)\n","else:\n","  decoder = Decoder(embed_size, hidden_size, max_len=vocab_size, generator = generator, dropout=dropout)\n","  # BEAM SEARCH\n","  beamSeasrch = BeamSearch(decoder, 3,3,line_embed,max_len)\n","  model = TSTModel(max_len, vocab_size, embed_size, hidden_size_z, hidden_size_y, line_embed, encoder, generator, decoder, classifier).to(device)\n","\n","\n","optimizer_model = torch.optim.Adam(model.parameters(), lr=lr) \n","optimizer_discr = torch.optim.Adam(list(discriminator0.parameters()) + list(discriminator1.parameters()), lr=lr) \n","\n","rec_loss = nn.NLLLoss(reduction=\"mean\",ignore_index = PAD_INDEX)\n","class_loss = nn.BCELoss()\n","discr_loss = nn.BCELoss()\n","\n","epoch_losses = []\n","for epoch in range(epochs):\n","  epoch_loss = 0\n","  epoch_class_loss = 0\n","  epoch_rec_loss = 0\n","  epoch_adv_loss = 0\n","  epoch_loss_d = 0\n","  epoch_tokens = 0\n","  model.train()\n","  for i,(taylor_lines, taylor_len, drake_lines, drake_len) in enumerate(train_loader):\n","    lines = torch.cat((taylor_lines, drake_lines), 0).to(device)    \n","    line_lens = torch.cat((taylor_len, drake_len), 0).to(device)\n","    labels = torch.cat((torch.ones(size=(len(taylor_lines),), dtype=torch.int32),torch.zeros(size=(len(drake_lines),),dtype=torch.int32))).to(device)\n","    classifier_labels = torch.cat((labels,1-labels, labels))\n","    \n","    fake_labels = torch.cat((torch.zeros(size=(len(taylor_lines),), dtype=torch.int32),torch.ones(size=(len(taylor_lines),),dtype=torch.int32))).to(device)\n","    fake_labels = fake_labels\n","\n","    # Train discriminator\n","\n","    if discriminate:\n","      rec_orig, pred_class, decode_orig, decode_tsf, pred_fake0, pred_fake1 = model(lines, line_lens, labels)\n","      \n","      pred_fake0 = discriminator0(pred_fake0[0], pred_fake0[1])\n","      pred_fake1 = discriminator1(pred_fake1[0], pred_fake1[1])\n","\n","      loss_d0 = discr_loss(pred_fake0, fake_labels.to(torch.float))\n","      loss_d1 = discr_loss(pred_fake1, fake_labels.to(torch.float))\n","      loss_d = loss_d0 + loss_d1\n","\n","      optimizer_discr.zero_grad()\n","      loss_d.backward()\n","      optimizer_discr.step()\n","\n","    # Train model\n","\n","    rec_orig, pred_class, decode_orig, decode_tsf, pred_fake0, pred_fake1 = model(lines, line_lens, labels)\n","\n","    loss_rec = rec_loss(input=rec_orig[2].permute(0,2,1), target=lines[:, 1:])\n","\n","    loss = loss_rec\n","\n","    if attention:\n","      rec_treshold  = 10\n","    else:\n","      rec_treshold = 10\n","    \n","    if discriminate:\n","      pred_fake0 = discriminator0(pred_fake0[0], pred_fake0[1])\n","      pred_fake1 = discriminator1(pred_fake1[0], pred_fake1[1])\n","      loss_adv0 = class_loss(pred_fake0[len(drake_lines):], fake_labels[len(drake_lines):].to(torch.float))\n","      loss_adv1 = class_loss(pred_fake1[len(taylor_lines):], fake_labels[len(taylor_lines):].to(torch.float))\n","\n","      if loss_adv0 \u003c 1.2 and loss_adv1 \u003c 1.2 and loss_rec \u003c rec_treshold:\n","      # Don't use adversarial training unless discriminator and reconstruction are both good\n","        loss -= (loss_adv0 + loss_adv1)\n","    \n","    if classify:\n","      loss_class = class_loss(pred_class, classifier_labels.to(torch.float))\n","      loss_class_generated = class_loss(pred_class[:-len(labels)], classifier_labels[:-len(labels)].to(torch.float))\n","\n","      # if loss_class_generated \u003e 0.5 and loss_rec \u003c rec_treshold:\n","      # # If generated examples are too similar and reconstruction is good, only focus on achieving better style (i.e. classifier)\n","      # # Else, focus on both\n","      # # Note: play around with these, it probably affects performance\n","      #   loss = loss_class\n","      # else:\n","      #   loss += loss_class\n","      if loss_rec \u003c rec_treshold:\n","        loss += loss_class\n","\n","    optimizer_model.zero_grad()\n","    loss.backward()\n","    optimizer_model.step()\n","    \n","    epoch_loss += loss.item()\n","    epoch_rec_loss += loss_rec.item() * line_lens.sum().item()\n","    epoch_tokens += line_lens.sum().item()\n","\n","    if discriminate:\n","      epoch_loss_d += loss_d.item()\n","      epoch_adv_loss += (loss_adv0.item() + loss_adv1.item())\n","      \n","    if classify:\n","      epoch_class_loss += loss_class.item()\n","\n","    if model.training and i % print_every == 0:\n","      print(\"Epoch Step: %d Loss: %f\" % (i, loss.item()))\n","      if classify:\n","        print(\"Epoch Step: %d Class Loss: %f\" % (i, loss_class.item()))\n","  \n","  epoch_losses.append(epoch_loss)\n","  print(\"Finished Training Epoch \", epoch)\n","  print(\"Training PPL\", np.exp(epoch_rec_loss / float(epoch_tokens)))\n","\n","  if discriminate:\n","    print(\"Adversarial Loss\", epoch_adv_loss)\n","    print(\"Discriminator Loss\", epoch_loss_d)\n","  \n","  if classify:\n","    print(\"Classification Loss\", epoch_class_loss)\n","\n","  val_loss = 0\n","  val_tokens = 0\n","  val_class_loss = 0\n","  correct_pred = 0\n","  correct_pred_all = 0\n","  correct_pred_drake = 0\n","  correct_pred_tay = 0\n","\n","  for i,(taylor_lines, taylor_len, drake_lines, drake_len) in enumerate(valid_loader):\n","    lines = torch.cat((taylor_lines, drake_lines), 0).to(device)    \n","    line_lens = torch.cat((taylor_len, drake_len), 0).to(device)\n","    labels = torch.cat((torch.ones(size=(len(taylor_lines),), dtype=torch.int32),torch.zeros(size=(len(drake_lines),),dtype=torch.int32))).to(device)\n","    classifier_labels = torch.cat((labels,1-labels, labels))\n","    \n","    fake_labels = torch.cat((torch.zeros(size=(len(taylor_lines),), dtype=torch.int32),torch.ones(size=(len(taylor_lines),),dtype=torch.int32))).to(device)\n","\n","    rec_orig, pred_class, decode_orig, decode_tsf, pred_fake0, pred_fake1 = model(lines, line_lens, labels)\n","    loss_rec = rec_loss(input=rec_orig[2].permute(0,2,1), target=lines[:, 1:])\n","    # loss_class = class_loss(pred_class, classifier_labels.to(torch.float))\n","\n","    val_loss += loss_rec.item() * line_lens.sum().item()\n","    val_tokens += line_lens.sum().item()\n","    # val_class_loss += loss_class.item()*classifier_labels.size(0)\n","\n","    if classify:\n","      correct_pred += torch.sum((pred_class[-len(lines):] \u003e= 0.5) == classifier_labels[-len(lines):])\n","      correct_pred_all += torch.sum((pred_class \u003e= 0.5) == classifier_labels)\n","\n","    if discriminate:\n","      pred_fake0 = discriminator0(pred_fake0[0], pred_fake0[1])\n","      pred_fake1 = discriminator1(pred_fake1[0], pred_fake1[1])\n","\n","      correct_pred_drake += torch.sum((pred_fake0 \u003e= 0.5) == fake_labels) \n","      correct_pred_tay += torch.sum((pred_fake1 \u003e= 0.5) == fake_labels)\n","  \n","  print(\"Valid PPL\", np.exp(val_loss / float(val_tokens)))\n","  if classify:\n","    print(\"Valid Classification Accuracy on True\", correct_pred / (2.*len(valid_dataset)))\n","    print(\"Valid Classification Accuracy on All\", correct_pred_all / (3.*2.*len(valid_dataset)))\n","\n","  if discriminate:\n","    print(\"Valid Classification Accuracy on Drake\", correct_pred_drake / (2.*len(valid_dataset)))\n","    print(\"Valid Classification Accuracy on Taylor\", correct_pred_tay / (2.*len(valid_dataset)))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1638750168918,"user":{"displayName":"Hammaad Adam","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03287553409115045279"},"user_tz":300},"id":"Y6lRAikKW0EZ","outputId":"02f6babb-f67b-42f8-89ff-13d4b33a884a"},"outputs":[{"name":"stdout","output_type":"stream","text":["['\u003cs\u003e', 'you', \"'re\", 'being', 'too', 'loud', '\u003c/s\u003e', '\u003cpad\u003e', '\u003cpad\u003e', '\u003cpad\u003e', '\u003cpad\u003e', '\u003cpad\u003e', '\u003cpad\u003e', '\u003cpad\u003e', '\u003cpad\u003e', '\u003cpad\u003e', '\u003cpad\u003e']\n","['you', \"'re\", 'being', 'too', 'loud', '\u003c/s\u003e', '\u003c/s\u003e', 'too', 'loud', '\u003c/s\u003e', '\u003c/s\u003e', 'too', 'loud', '\u003c/s\u003e', '\u003c/s\u003e', 'too']\n","['you', \"'re\", 'being', 'too', 'loud', '\u003c/s\u003e', '\u003c/s\u003e', 'too', 'loud', '\u003c/s\u003e', '\u003c/s\u003e', 'too', 'loud', '\u003c/s\u003e', '\u003c/s\u003e', 'too']\n","['you', \"'re\", 'being', 'too', 'loud', '\u003c/s\u003e', 'being', 'too', 'loud', '\u003c/s\u003e', '\u003c/s\u003e', 'too', 'loud', '\u003c/s\u003e', '\u003c/s\u003e', 'too']\n","['\u003cs\u003e', 'love', 'certain', 'ones', 'but', 'never', 'get', 'attached', 'to', \"'em\", '\u003c/s\u003e', '\u003cpad\u003e', '\u003cpad\u003e', '\u003cpad\u003e', '\u003cpad\u003e', '\u003cpad\u003e', '\u003cpad\u003e']\n","['love', 'certain', 'ones', 'but', 'never', 'get', 'attached', 'to', \"'em\", '\u003c/s\u003e', 'tales', \"'em\", '\u003c/s\u003e', '\u003c/s\u003e', \"'em\", '\u003c/s\u003e']\n","['love', 'certain', 'ones', 'but', 'never', 'get', 'attached', 'to', \"'em\", '\u003c/s\u003e', 'closed', \"'em\", '\u003c/s\u003e', '\u003c/s\u003e', \"'em\", '\u003c/s\u003e']\n","['love', 'certain', 'ones', 'but', 'never', 'get', 'attached', 'to', \"'em\", '\u003c/s\u003e', 'to', \"'em\", '\u003c/s\u003e', '\u003c/s\u003e', \"'em\", '\u003c/s\u003e']\n"]}],"source":["# Quick assessment\n","def lookup_words(x, vocab):\n","  return [vocab[i] for i in x]\n","\n","idx=0\n","print(lookup_words(lines[idx], vocab))\n","print(lookup_words(rec_orig[3][idx], vocab))\n","print(lookup_words(decode_orig[3][idx], vocab))\n","print(lookup_words(decode_tsf[3][idx], vocab))\n","\n","idx=9\n","print(lookup_words(lines[idx], vocab))\n","print(lookup_words(rec_orig[3][idx], vocab))\n","print(lookup_words(decode_orig[3][idx], vocab))\n","print(lookup_words(decode_tsf[3][idx], vocab))"]},{"cell_type":"markdown","metadata":{"id":"iFrALY7-CBVq"},"source":["#### Save Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4cUviHkqGngZ"},"outputs":[],"source":["from datetime import datetime\n","import pytz\n","import os\n","now = datetime.now()\n","now.astimezone(pytz.timezone('America/New_York'))\n","\n","model_dir = '/content/drive/Shareddrives/MIT NLP 8.864/model'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cb9sgSLsGTOQ"},"outputs":[],"source":["def current_time():\n","  now = datetime.now()\n","  now = now.astimezone(pytz.timezone('America/New_York'))\n","  return now.strftime(\"%Y%m%d_%H%M%S\")\n","def lookup_words(x, vocab):\n","  return [vocab[i] for i in x]"]},{"cell_type":"markdown","metadata":{"id":"tWIVaV2PFxB0"},"source":["##### Pre-trained Classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rX3V-MDS8yrn"},"outputs":[],"source":["if pre_train_classifier:\n","  classifier_path = os.path.join(model_dir,\n","                                 f'classifier/classifier_{current_time()}.pt')\n","  torch.save(classifier.state_dict(), classifier_path)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MhQ7JvVDIURI"},"outputs":[],"source":["# # Reload model Example\n","# model = LSTMClassifier()\n","# model.load_state_dict(torch.load(classifier_path))\n","# model = model.to(device)\n","# model.eval()"]},{"cell_type":"markdown","metadata":{"id":"CSiie6Y1F02-"},"source":["##### Style-Transfer Model"]},{"cell_type":"markdown","metadata":{"id":"-nyY5MVdIM62"},"source":["###### Set model path"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pDH74vWjEi-N"},"outputs":[],"source":["model_name = 'model_'\n","if not attention and not classify and not discriminate:\n","  # Dana\n","  model_name += '1'\n","elif attention and not classify and not discriminate:\n","  # Dana\n","  model_name += '2'\n","elif not attention and classify and not discriminate:\n","  # Sirui\n","  model_name += '3'\n","elif attention and classify and not discriminate:\n","  # Sirui\n","  model_name += '4'\n","elif not attention and classify and discriminate:\n","  # Chenwei\n","  model_name += '5'\n","elif attention and classify and discriminate:\n","  # Chenwei\n","  model_name += '6'\n","elif not attention and not classify and discriminate:\n","  # Hammaad\n","  model_name += '7'\n","elif attention and not classify and discriminate:\n","  # Hammaad\n","  model_name += '8'\n"]},{"cell_type":"markdown","metadata":{"id":"FV4U3sAbKY-D"},"source":["###### Save model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U6Pdc4LiL95Z"},"outputs":[],"source":["model_init_args = {\"Embedding_num_embeddings\":vocab_size,\n","           'Embedding_embedding_dim':embed_size,\n","           'Encoder_input_size':embed_size, \n","           'Encoder_hidden_size':hidden_size,\n","           'GeneratorTransferredSampled_hidden_size':hidden_size, \n","           'GeneratorTransferredSampled_vocab_size':vocab_size, \n","           'GeneratorTransferredSampled_gamma':gamma,\n","           'LSTMClassifier_dimension':classifier.dimension,\n","           'LSTMDiscriminator_input_size':hidden_size, \n","           'LSTMDiscriminator_hidden_size':hidden_size}\n","if attention: \n","  model_init_args.update({\n","           'BahdanauAttention_hidden_size':hidden_size,\n","           'BahdanauAttention_key_size':hidden_size,\n","           'AttentionDecoder_input_size':embed_size,\n","            'AttentionDecoder_hidden_size':hidden_size, \n","            'AttentionDecoder_max_len':vocab_size,  \n","            'AttentionDecoder_dropout':dropout,\n","            'TSTModelAttention_max_len': max_len, \n","            'TSTModelAttention_vocab_size':vocab_size, \n","            'TSTModelAttention_embed_size':embed_size, \n","            'TSTModelAttention_hidden_size_z':hidden_size_z, \n","            'TSTModelAttention_hidden_size_y':hidden_size_y})\n","else:\n","  model_init_args.update({\n","            'Decoder_':embed_size, \n","            'Decoder_':hidden_size, \n","            'Decoder_max_len':vocab_size, \n","            'Decoder_dropout':dropout,\n","            'TSTModel_max_len': max_len, \n","            'TSTModel_vocab_size':vocab_size, \n","            'TSTModel_embed_size':embed_size, \n","            'TSTModel_hidden_size_z':hidden_size_z, \n","            'TSTModel_hidden_size_y':hidden_size_y})\n","           \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e1Y4aRftUxB3"},"outputs":[],"source":["model_dir_current = os.path.join(model_dir,model_name)\n","model_dir_current = os.path.join(model_dir_current,\n","                                 f'{model_name}_{current_time()}')\n","if not os.path.isdir(model_dir_current):\n","  os.mkdir(model_dir_current)\n","model_path = os.path.join(model_dir_current,\n","                          f'{model_name}_{current_time()}.pt')\n","torch.save(model.state_dict(), model_path)\n","model_args_filename = model_path[:-3]+\"_params.txt\"\n","import json\n","with open(model_args_filename,'w') as f:\n","  json.dump(model_init_args, f)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":165,"status":"ok","timestamp":1638758457857,"user":{"displayName":"Hammaad Adam","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03287553409115045279"},"user_tz":300},"id":"wF46zZCxcVxR","outputId":"fbba3258-ea8a-4c80-90ea-cece8269689b"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/Shareddrives/MIT NLP 8.864/model/model_8/model_8_20211205_213744/model_8_20211205_213744.pt'"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["model_path"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":328,"status":"ok","timestamp":1638758154659,"user":{"displayName":"Hammaad Adam","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03287553409115045279"},"user_tz":300},"id":"CbGOTvG-Lfin","outputId":"08b4bc67-bf4f-4ff9-e252-3ce988ff6f9e"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n"]}],"source":["line_embed = nn.Embedding(vocab_size, embed_size)\n","encoder = Encoder(embed_size,hidden_size)\n","generator = GeneratorTransferredSampled(hidden_size,vocab_size, \n","                                        line_embed, gamma = gamma)\n","classifier = LSTMClassifier()\n","discriminator0 = LSTMDiscriminator(hidden_size, hidden_size).to(device)\n","discriminator1 = LSTMDiscriminator(hidden_size, hidden_size).to(device)\n","if attention:\n","  attention_mech = BahdanauAttention(hidden_size, key_size=hidden_size)\n","  decoder = AttentionDecoder(embed_size, hidden_size, attention=attention_mech, max_len=vocab_size, generator = generator,dropout=dropout)\n","  beamSeasrch = BeamSearch(decoder, 3,3,line_embed,max_len)\n","  model = TSTModelAttention(max_len, vocab_size, embed_size, hidden_size_z, hidden_size_y, line_embed, encoder, generator, decoder, classifier, beamSeasrch).to(device)\n","else:\n","  decoder = Decoder(embed_size, hidden_size, max_len=vocab_size, generator = generator, dropout=dropout)\n","  model = TSTModel(max_len, vocab_size, embed_size, hidden_size_z, hidden_size_y, line_embed, encoder, generator, decoder, classifier).to(device)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2153,"status":"ok","timestamp":1638758219765,"user":{"displayName":"Hammaad Adam","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03287553409115045279"},"user_tz":300},"id":"lRgC2nXSB3EO","outputId":"50752316-e319-4807-eece-e6dadb3a45ba"},"outputs":[{"data":{"text/plain":["TSTModelAttention(\n","  (encoder): Encoder(\n","    (rnn): GRU(100, 700, batch_first=True)\n","  )\n","  (generator): GeneratorTransferredSampled(\n","    (proj): Linear(in_features=700, out_features=12458, bias=True)\n","    (logsoftmax): LogSoftmax(dim=2)\n","    (softmax): Softmax(dim=2)\n","    (src_embed): Embedding(12458, 100)\n","  )\n","  (decoder): AttentionDecoder(\n","    (rnn): GRU(800, 700, batch_first=True, dropout=0.2)\n","    (generator): GeneratorTransferredSampled(\n","      (proj): Linear(in_features=700, out_features=12458, bias=True)\n","      (logsoftmax): LogSoftmax(dim=2)\n","      (softmax): Softmax(dim=2)\n","      (src_embed): Embedding(12458, 100)\n","    )\n","    (dropout_layer): Dropout(p=0.2, inplace=False)\n","    (rnn_to_pre): Linear(in_features=1500, out_features=700, bias=False)\n","    (attention): BahdanauAttention(\n","      (key_layer): Linear(in_features=700, out_features=700, bias=False)\n","      (query_layer): Linear(in_features=700, out_features=700, bias=False)\n","      (energy_layer): Linear(in_features=700, out_features=1, bias=False)\n","    )\n","  )\n","  (classifier): LSTMClassifier(\n","    (embedding): Linear(in_features=12458, out_features=300, bias=True)\n","    (lstm): LSTM(300, 128, batch_first=True, bidirectional=True)\n","    (drop): Dropout(p=0.5, inplace=False)\n","    (fc): Linear(in_features=256, out_features=1, bias=True)\n","  )\n","  (line_embed): Embedding(12458, 100)\n","  (y_embed_enc): Embedding(2, 200)\n","  (y_embed_gen): Embedding(2, 200)\n",")"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["model_path_load = '/content/drive/Shareddrives/MIT NLP 8.864/model/model_8/model_8_20211205_192248/model_8_20211205_192248.pt'\n","model.load_state_dict(torch.load(model_path_load))\n","model = model.to(device)\n","model.eval()"]},{"cell_type":"markdown","metadata":{"id":"pJaumBnKnA7G"},"source":["#### Save Test Result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_4DZXwvxeTEX"},"outputs":[],"source":["if os.path.exists(os.path.join(model_dir_current,\"raw.txt\")):\n","  os.remove(os.path.join(model_dir_current,\"raw.txt\"))\n","if os.path.exists(os.path.join(model_dir_current,\"orig.txt\")):\n","  os.remove(os.path.join(model_dir_current,\"orig.txt\"))\n","if os.path.exists(os.path.join(model_dir_current,\"tsf.txt\")):\n","  os.remove(os.path.join(model_dir_current,\"tsf.txt\"))\n","for i,(taylor_lines, taylor_len, drake_lines, drake_len) in enumerate(test_loader):\n","    lines = torch.cat((taylor_lines, drake_lines), 0).to(device)    \n","    line_lens = torch.cat((taylor_len, drake_len), 0).to(device)\n","    labels = torch.cat((torch.ones(size=(len(taylor_lines),), dtype=torch.int32),torch.zeros(size=(len(drake_lines),),dtype=torch.int32))).to(device)\n","    classifier_labels = torch.cat((labels,1-labels, labels))\n","    \n","    fake_labels = torch.cat((torch.zeros(size=(len(taylor_lines),), dtype=torch.int32),torch.ones(size=(len(taylor_lines),),dtype=torch.int32))).to(device)\n","\n","    rec_orig, pred_class, decode_orig, decode_tsf, _, _ = model(lines, line_lens, labels)\n","    for i in range(lines.shape[0]):\n","      with open(os.path.join(model_dir_current,\"raw.txt\"),'a') as f:\n","        f.write(str(lookup_words(lines[i],vocab))+\"\\n\")\n","      with open(os.path.join(model_dir_current,\"orig.txt\"),'a') as f:\n","        f.write(str(lookup_words(decode_orig[3][i],vocab))+\"\\n\")\n","      with open(os.path.join(model_dir_current,\"tsf.txt\"),'a') as f:\n","        f.write(str(lookup_words(decode_tsf[3][i],vocab))+\"\\n\")\n","      \n","    "]},{"cell_type":"markdown","metadata":{"id":"vfpA5Nvd-oHa"},"source":["#### BeamSearch Validation (only for the first validation sample)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":337},"executionInfo":{"elapsed":387,"status":"error","timestamp":1638648461352,"user":{"displayName":"Hammaad Adam","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03287553409115045279"},"user_tz":300},"id":"qLroJgAe-qvi","outputId":"638f977f-8d64-48de-a09e-eac550baec7a"},"outputs":[{"ename":"AttributeError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-74-9396eaff6817\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mline_lens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline_lens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 11\u001b[0;31m   \u001b[0mdecode_orig_beam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_tsf_beam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_beam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"3 Original Sentences decoded by beam search:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m-\u003e 1178\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u003e\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'TSTModel' object has no attribute 'forward_beam'"]}],"source":["for i,(taylor_lines, taylor_len, drake_lines, drake_len) in enumerate(valid_loader):\n","  lines = torch.cat((taylor_lines, drake_lines), 0).to(device)    \n","  line_lens = torch.cat((taylor_len, drake_len), 0).to(device)\n","  \n","  labels = torch.cat((torch.ones(size=(len(taylor_lines),), dtype=torch.int32),torch.zeros(size=(len(drake_lines),),dtype=torch.int32))).to(device)\n","  classifier_labels = torch.cat((labels,1-labels, labels))\n","\n","  lines = lines[0:1,:]\n","  line_lens = line_lens[0:1]\n","  labels = labels[0:1]\n","  decode_orig_beam, decode_tsf_beam = model.forward_beam(lines, line_lens, labels)\n","  \n","  print(\"3 Original Sentences decoded by beam search:\")\n","  print(lookup_words(decode_orig[-1][0], vocab))\n","  print(lookup_words(decode_orig[-1][1], vocab))\n","  print(lookup_words(decode_orig[-1][2], vocab))\n","  print(\"3 Transferred Sentences decoded by beam search:\")\n","  print(lookup_words(decode_tsf[-1][0], vocab))\n","  print(lookup_words(decode_tsf[-1][1], vocab))\n","  print(lookup_words(decode_tsf[-1][2], vocab))\n","  break"]},{"cell_type":"markdown","metadata":{"id":"tyF-GsOa-yxH"},"source":["### Pre-Trained Classifier for Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":57176,"status":"ok","timestamp":1638729419377,"user":{"displayName":"Hammaad Adam","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03287553409115045279"},"user_tz":300},"id":"BLb2RStM-1n3","outputId":"a62fecb8-15c1-467a-f058-fdf817092aca"},"outputs":[{"name":"stdout","output_type":"stream","text":["Pre-Training Accuracy:  tensor(0.7757, device='cuda:0')\n","Pre-Valid Accuracy:  tensor(0.8396, device='cuda:0')\n"]}],"source":["# from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n","\n","# # Train classifier used in eval\n","# class_epochs = 1\n","# lr_class = 1e-3\n","# batch_size = 32\n","# vocab_size = len(vocab)\n","\n","# train_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","# valid_loader = data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n","# test_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n","\n","# classifier_for_eval = LSTMClassifier().to(device)\n","# optimizer = torch.optim.Adam(classifier_for_eval.parameters(), lr=lr_class) \n","# class_loss = nn.BCELoss()\n","\n","# for epoch in range(class_epochs):\n","#   correct = 0\n","#   classifier_for_eval.train()\n","#   for i,(taylor_lines, taylor_len, drake_lines, drake_len) in enumerate(train_loader):\n","#     lines = torch.cat((taylor_lines, drake_lines), 0).to(device)  \n","#     classifier_lines = F.one_hot(lines[:,1:], len(vocab)).to(torch.float).to(device)\n","\n","#     line_lens = torch.cat((taylor_len, drake_len), 0).to(device)\n","#     labels = torch.cat((torch.ones(size=(len(taylor_lines),), dtype=torch.int32),torch.zeros(size=(len(drake_lines),),dtype=torch.int32))).to(device)\n","\n","#     pred_class = classifier_for_eval(classifier_lines, line_lens-1)\n","#     loss_class = class_loss(input=pred_class, target=labels.to(torch.float))\n","\n","#     optimizer.zero_grad()\n","#     loss_class.backward()\n","#     optimizer.step()\n","\n","#     correct += torch.sum((pred_class \u003e= 0.5) == labels)\n","#   print(\"Pre-Training Accuracy: \", correct / float(2*len(train_dataset)))\n","#   classifier_for_eval.eval()\n","#   correct = 0\n","#   for i,(taylor_lines, taylor_len, drake_lines, drake_len) in enumerate(valid_loader):\n","#     lines = torch.cat((taylor_lines, drake_lines), 0).to(device)  \n","#     classifier_lines = F.one_hot(lines[:,1:], len(vocab)).to(torch.float).to(device)\n","\n","#     line_lens = torch.cat((taylor_len, drake_len), 0).to(device)\n","#     labels = torch.cat((torch.ones(size=(len(taylor_lines),), dtype=torch.int32),torch.zeros(size=(len(drake_lines),),dtype=torch.int32))).to(device)\n","\n","#     pred_class = classifier_for_eval(classifier_lines, line_lens-1)\n","#     correct += torch.sum((pred_class \u003e= 0.5) == labels)\n","#   print(\"Pre-Valid Accuracy: \", correct / float(2*len(valid_dataset)))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":179,"status":"ok","timestamp":1638760204104,"user":{"displayName":"Hammaad Adam","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03287553409115045279"},"user_tz":300},"id":"6vVl-2XYrkdo","outputId":"10805aa3-c516-41ca-b773-d9ffd36091d6"},"outputs":[{"data":{"text/plain":["LSTMClassifier(\n","  (embedding): Linear(in_features=12458, out_features=300, bias=True)\n","  (lstm): LSTM(300, 128, batch_first=True, bidirectional=True)\n","  (drop): Dropout(p=0.5, inplace=False)\n","  (fc): Linear(in_features=256, out_features=1, bias=True)\n",")"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["classifier_for_eval = LSTMClassifier().to(device)\n","classifier_path = '/content/drive/Shareddrives/MIT NLP 8.864/model/classifier/classifier_for_eval_20211205_130209.pt'\n","classifier_for_eval.load_state_dict(torch.load(classifier_path))\n","classifier_for_eval = classifier_for_eval.to(device)\n","classifier_for_eval.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":77162,"status":"ok","timestamp":1638760282567,"user":{"displayName":"Hammaad Adam","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03287553409115045279"},"user_tz":300},"id":"KhD3ZKPkAd2a","outputId":"9dcf66cb-0fc9-4309-db8d-7776f17c9a3b"},"outputs":[{"name":"stdout","output_type":"stream","text":["AUC on Real Sentences:  0.9229819830246914\n","AUC on Reconstructed Sentences:  0.9188062885802468\n","AUC on Transferred Sentences:  0.10500956790123459\n"]}],"source":["# Evaluate TST using classifier\n","model.eval()\n","classifier_for_eval.eval()\n","\n","pred_orig=[]\n","pred_tsf=[]\n","pred_real=[]\n","\n","y_orig=[]\n","y_tsf=[]\n","y_real=[]\n","\n","for i,(taylor_lines, taylor_len, drake_lines, drake_len) in enumerate(test_loader):\n","  lines = torch.cat((taylor_lines, drake_lines), 0).to(device)    \n","  line_lens = torch.cat((taylor_len, drake_len), 0).to(device)\n","  labels = torch.cat((torch.ones(size=(len(taylor_lines),), dtype=torch.int32),torch.zeros(size=(len(drake_lines),),dtype=torch.int32))).to(device)\n","  classifier_labels = torch.cat((labels,1-labels, labels))\n","  \n","  rec_orig, pred_class, decode_orig, decode_tsf, pred_fake0, pred_fake1 = model(lines, line_lens, labels)\n","\n","  classifier_lines = torch.cat((torch.exp(decode_orig[2]), torch.exp(decode_tsf[2]), F.one_hot(lines[:,1:], vocab_size).to(torch.float)), 0)\n","\n","  rec_orig_len = first_eos(rec_orig[3]) + 1\n","  decode_orig_len = first_eos(decode_orig[3]) + 1\n","  decode_tsf_len = first_eos(decode_tsf[3]) + 1\n","  classifier_line_lens = torch.cat((decode_orig_len, decode_tsf_len, line_lens),0)\n","  \n","  pred_class = classifier_for_eval(classifier_lines, classifier_line_lens-1)\n","  pred_class = pred_class.cpu().detach().numpy()\n","  classifier_labels = classifier_labels.cpu().detach().numpy()\n","\n","  pred_orig.append(pred_class[:len(lines)])\n","  pred_tsf.append(pred_class[len(lines):-len(lines)])\n","  pred_real.append(pred_class[-len(lines):])\n","\n","  y_orig.append(classifier_labels[:len(lines)])\n","  y_tsf.append(classifier_labels[len(lines):-len(lines)])\n","  y_real.append(classifier_labels[-len(lines):])\n","\n","from sklearn.metrics import roc_auc_score\n","print(\"AUC on Real Sentences: \", roc_auc_score(np.concatenate(y_real), np.concatenate(pred_real)))\n","print(\"AUC on Reconstructed Sentences: \", roc_auc_score(np.concatenate(y_orig), np.concatenate(pred_orig)))\n","print(\"AUC on Transferred Sentences: \", roc_auc_score(np.concatenate(y_tsf), np.concatenate(pred_tsf)))\n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["m7FQaWQbojAb","S3i2hvtYoo30","ynBTHa5grUr7","q96M0-eW9OBl","HeRBfORu9P49","iaAnYmyv9gbu","NX25tl4G5vWa","kxvEOOu_9Zkt","t2mg2wYk30dv","wg_FbKl-T9xd","9XEGiaaa93DR","C1-4ipH195Oe","BoCYL-kD-Q4o","tWIVaV2PFxB0","-nyY5MVdIM62","pJaumBnKnA7G","vfpA5Nvd-oHa"],"machine_shape":"hm","name":"Training Loop.ipynb","toc_visible":true,"version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}