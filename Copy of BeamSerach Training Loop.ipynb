{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Copy of BeamSerach Training Loop.ipynb","provenance":[{"file_id":"1mU9OJvPQ5atsnfKF6B_JZxekmTjn15j0","timestamp":1637469603494},{"file_id":"1L95rT5EdI3YExnr2gNXWTdiirQWlrjvY","timestamp":1637335818257}],"collapsed_sections":["S3i2hvtYoo30","ynBTHa5grUr7","tBjKgl1MoKHy","wg_FbKl-T9xd"],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"m7FQaWQbojAb"},"source":["### Import and Setup"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wyz5JwHDopli","executionInfo":{"status":"ok","timestamp":1637461204900,"user_tz":300,"elapsed":4500,"user":{"displayName":"Katherine Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11324848797432579426"}},"outputId":"806c6928-7b6f-4896-9a33-0775f454b228"},"source":["!pip install unidecode"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting unidecode\n","  Downloading Unidecode-1.3.2-py3-none-any.whl (235 kB)\n","\u001b[?25l\r\u001b[K     |█▍                              | 10 kB 24.2 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 20 kB 24.3 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 30 kB 18.9 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 40 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |███████                         | 51 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 61 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 71 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 81 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 92 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 102 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 112 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 122 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 133 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 143 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 153 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 163 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 174 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 184 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 194 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 204 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 215 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 225 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 235 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 235 kB 5.2 MB/s \n","\u001b[?25hInstalling collected packages: unidecode\n","Successfully installed unidecode-1.3.2\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T8q-CET4os1p","executionInfo":{"status":"ok","timestamp":1637461234940,"user_tz":300,"elapsed":30046,"user":{"displayName":"Katherine Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11324848797432579426"}},"outputId":"83c7ff8e-646d-4d22-ab0c-a3b3ec14f22e"},"source":["import json\n","import re\n","from unidecode import unidecode\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","import nltk\n","nltk.download('punkt')\n","from collections import Counter\n","import operator\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"0ELBqq8kpAZS","executionInfo":{"status":"ok","timestamp":1637461342200,"user_tz":300,"elapsed":128,"user":{"displayName":"Katherine Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11324848797432579426"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","# assert device == \"cuda\"  \n","\n","seed = 42\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S3i2hvtYoo30"},"source":["### Data"]},{"cell_type":"code","metadata":{"id":"OGzVRiwZowGv","executionInfo":{"status":"ok","timestamp":1637461374397,"user_tz":300,"elapsed":30362,"user":{"displayName":"Katherine Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11324848797432579426"}}},"source":["f = open('/content/drive/Shareddrives/MIT NLP 8.864/Data/drake.json')\n","drake = json.load(f)\n","f.close()\n","\n","f = open('/content/drive/Shareddrives/MIT NLP 8.864/Data/tswift.json')\n","taylor = json.load(f)\n","f.close()\n","\n","drake = [drake['songs'][i]['lyrics'] for i in range(len(drake['songs']))]\n","taylor = [taylor['songs'][i]['lyrics'] for i in range(len(taylor['songs']))]\n","\n","taylor_lyrics = [re.sub('\\u2005', ' ', re.sub(r'[\\(\\[].*?[\\)\\]]', '', taylor[i])).split('\\n') for i in range(len(taylor))]\n","taylor_lyrics = [[unidecode(i) for i in taylor_lyrics[j]] for j in range(len(taylor_lyrics))]\n","taylor_lyrics = [[re.sub('\\d+EmbedShare URLCopyEmbedCopy', '', i) for i in taylor_lyrics[j]] for j in range(len(taylor_lyrics))]\n","taylor_lyrics = [[re.sub('\\d+.EmbedShare URLCopyEmbedCopy', '', i) for i in taylor_lyrics[j]] for j in range(len(taylor_lyrics))]\n","taylor_lyrics = [[re.sub('EmbedShare URLCopyEmbedCopy', '', i) for i in taylor_lyrics[j]] for j in range(len(taylor_lyrics))]\n","taylor_lyrics = [[i for i in taylor_lyrics[j] if i != ''] for j in range(len(taylor_lyrics))]\n","\n","drake_lyrics = [re.sub('\\u2005', ' ', re.sub(r'[\\(\\[].*?[\\)\\]]', '', drake[i])).split('\\n') for i in range(len(drake))]\n","drake_lyrics = [[unidecode(i) for i in drake_lyrics[j]] for j in range(len(drake_lyrics))]\n","drake_lyrics = [[re.sub('\\d+EmbedShare URLCopyEmbedCopy', '', i) for i in drake_lyrics[j]] for j in range(len(drake_lyrics))]\n","drake_lyrics = [[re.sub('\\d+.EmbedShare URLCopyEmbedCopy', '', i) for i in drake_lyrics[j]] for j in range(len(drake_lyrics))]\n","drake_lyrics = [[re.sub('EmbedShare URLCopyEmbedCopy', '', i) for i in drake_lyrics[j]] for j in range(len(drake_lyrics))]\n","drake_lyrics = [[i for i in drake_lyrics[j] if i != ''] for j in range(len(drake_lyrics))]\n","\n","taylor_lyrics = [[line1 + ', ' + line2 for line1,line2 in zip(song[0::2], song[1::2])] for song in taylor_lyrics]\n","drake_lyrics = [[line1 + ', ' + line2 for line1,line2 in zip(song[0::2], song[1::2])] for song in drake_lyrics]\n","\n","drake_tokenized = [[word_tokenize(drake_lyrics[i][j]) for j in range(len(drake_lyrics[i]))] for i in range(len(drake_lyrics))]\n","taylor_tokenized = [[word_tokenize(taylor_lyrics[i][j]) for j in range(len(taylor_lyrics[i]))] for i in range(len(taylor_lyrics))]\n","\n","drake_tokenized = [[[word.lower() for word in line] for line in song] for song in drake_tokenized]\n","taylor_tokenized = [[[word.lower() for word in line] for line in song] for song in taylor_tokenized]\n","\n","drake_length = sum([[len(sent) for sent in song] for song in drake_tokenized], [])\n","taylor_length = sum([[len(sent) for sent in song] for song in taylor_tokenized], [])\n","\n","drake_lyrics = sum([[sent for sent in song if (len(sent) >= 10 and len (sent) <= 30)] for song in drake_tokenized], [])\n","taylor_lyrics = sum([[sent for sent in song if (len(sent) >= 10 and len (sent) <= 30)] for song in taylor_tokenized], [])\n","\n","taylor_vocab = sum(taylor_lyrics,[])\n","drake_vocab = sum(drake_lyrics,[])\n","\n","def unique(list1):\n","     \n","    # insert the list to the set\n","    list_set = set(list1)\n","    # convert the set to the list\n","    unique_list = (list(list_set))\n","    return unique_list\n","\n","vocab = taylor_vocab + drake_vocab\n","vocab_counts = Counter(vocab)\n","vocab = unique(vocab)\n","vocab = ['<pad>','<unk>','<s>', '</s>'] + vocab\n","\n","from torch.utils import data\n","import torch\n","\n","# These IDs are reserved.\n","MAX_SENT_LENGTH = 30\n","MAX_SENT_LENGTH_PLUS_SOS_EOS = 32\n","PAD_INDEX = 0\n","UNK_INDEX = 1\n","SOS_INDEX = 2\n","EOS_INDEX = 3\n","RARE_WORD_TRESHOLD = 0\n","\n","vocab_counts['<pad>'] = RARE_WORD_TRESHOLD + 1\n","vocab_counts['<unk>'] = RARE_WORD_TRESHOLD + 1\n","vocab_counts['<s>'] = RARE_WORD_TRESHOLD + 1\n","vocab_counts['</s>'] = RARE_WORD_TRESHOLD + 1\n","\n","class TSTDataset(data.Dataset):\n","    def __init__(self, taylor_sentences, drake_sentences, vocab, vocab_counts, sampling=1.):\n","        self.taylor_sentences = taylor_sentences[:int(len(taylor_sentences) * sampling)]\n","        self.drake_sentences = drake_sentences[:int(len(drake_sentences) * sampling)]\n","\n","        self.max_seq_length = MAX_SENT_LENGTH_PLUS_SOS_EOS\n","        self.vocab = vocab\n","        self.vocab_counts = vocab_counts\n","\n","        self.v2id = {v : i for i, v in enumerate(self.vocab)}\n","        self.id2v = {val : key for key, val in self.v2id.items()}\n","    \n","    def __len__(self):\n","        return min(len(self.taylor_sentences), len(self.drake_sentences))\n","    \n","    def __getitem__(self, index):\n","        taylor_sent = self.taylor_sentences[index]\n","        taylor_len = len(taylor_sent) + 2   # add <s> and </s> to each sentence\n","        taylor_id = []\n","        for w in taylor_sent:\n","            if w not in self.vocab:\n","                w = '<unk>'\n","            if vocab_counts[w] <= RARE_WORD_TRESHOLD:\n","                w = '<unk>'\n","            taylor_id.append(self.v2id[w])\n","\n","        taylor_id = ([SOS_INDEX] + taylor_id + [EOS_INDEX] + [PAD_INDEX] *\n","                  (self.max_seq_length - taylor_len))\n","\n","        drake_sent = self.drake_sentences[index]\n","        drake_len = len(drake_sent) + 2   # add <s> and </s> to each sentence\n","        drake_id = []\n","        for w in drake_sent:\n","            if w not in self.vocab:\n","                w = '<unk>'\n","            if vocab_counts[w] <= RARE_WORD_TRESHOLD:\n","                w = '<unk>'\n","            drake_id.append(self.v2id[w])\n","\n","        drake_id = ([SOS_INDEX] + drake_id + [EOS_INDEX] + [PAD_INDEX] *\n","                  (self.max_seq_length - drake_len))\n","\n","        return torch.tensor(taylor_id), taylor_len, torch.tensor(drake_id), drake_len\n","\n","dataset = TSTDataset(taylor_lyrics, drake_lyrics, vocab, vocab_counts)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"eL-YPQouRkxy","executionInfo":{"status":"ok","timestamp":1637461374398,"user_tz":300,"elapsed":22,"user":{"displayName":"Katherine Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11324848797432579426"}}},"source":["test_pct = 0.2\n","valid_pct = 0.1\n","\n","train_dataset, test_dataset = torch.utils.data.random_split(dataset, [int(len(dataset)*(1-test_pct)),len(dataset)-int(len(dataset)*(1-test_pct))])\n","valid_dataset, train_dataset = torch.utils.data.random_split(train_dataset, [int(len(dataset)*valid_pct),len(train_dataset)-int(len(dataset)*valid_pct)])"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ynBTHa5grUr7"},"source":["### Encoder"]},{"cell_type":"code","metadata":{"id":"FieXza5erYUH","executionInfo":{"status":"ok","timestamp":1637461374399,"user_tz":300,"elapsed":21,"user":{"displayName":"Katherine Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11324848797432579426"}}},"source":["from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","\n","class Encoder(nn.Module):\n","  def __init__(self, input_size, hidden_size, dropout=0.):\n","    \"\"\"\n","    Inputs: \n","      - `input_size`: an int representing the RNN input size.\n","      - `hidden_size`: an int representing the RNN hidden size.\n","      - `dropout`: a float representing the dropout rate during training. Note\n","          that for 1-layer RNN this has no effect since dropout only applies to\n","          outputs of intermediate layers.\n","    \"\"\"\n","    super(Encoder, self).__init__()\n","    self.rnn = nn.GRU(input_size, hidden_size, num_layers=1, batch_first=True,\n","                      dropout=dropout, bidirectional=False)\n","\n","  def forward(self, inputs, lengths, init_state=None):\n","    \"\"\"\n","    Inputs:\n","      - `inputs`: a 3d-tensor of shape (batch_size, max_seq_length, embed_size)\n","          representing a batch of padded embedded word vectors of source\n","          sentences.\n","      - `lengths`: a 1d-tensor of shape (batch_size,) representing the sequence\n","          lengths of `inputs`.\n","\n","    Returns:\n","      - `outputs`: a 3d-tensor of shape\n","        (batch_size, max_seq_length, hidden_size).\n","      - `finals`: a 3d-tensor of shape (num_layers, batch_size, hidden_size).\n","      Hint: `outputs` and `finals` are both standard GRU outputs. Check:\n","      https://pytorch.org/docs/stable/nn.html#gru\n","    \"\"\"\n","    # Our variable-length inputs are padded to the same length for batching\n","    # Here we \"pack\" them for computational efficiency (see note below)\n","    packed = pack_padded_sequence(inputs, lengths.cpu(), batch_first=True,\n","                                  enforce_sorted=False)\n","    outputs, finals = self.rnn(packed, init_state)\n","    outputs, _ = pad_packed_sequence(outputs, batch_first=True,\n","                                     total_length=32)\n","    return outputs, finals"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rKiJAkvZGMWl"},"source":["### Decoder"]},{"cell_type":"markdown","metadata":{"id":"tBjKgl1MoKHy"},"source":["#### Generator"]},{"cell_type":"code","metadata":{"id":"E1Tdf6_283WY","executionInfo":{"status":"ok","timestamp":1637466571782,"user_tz":300,"elapsed":131,"user":{"displayName":"Katherine Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11324848797432579426"}}},"source":["class GeneratorTransferredSampled(nn.Module):\n","  \"\"\"Define standard linear + softmax generation step.\"\"\"\n","  def __init__(self, hidden_size, vocab_size, src_embed, gamma=0.1):\n","    \"\"\"\n","    Inputs:\n","      - `src_embed`: a 2d-tensor of shape (vocab_size, embed_size )\n","    \"\"\"\n","    super(GeneratorTransferredSampled, self).__init__()\n","    self.proj = nn.Linear(hidden_size, vocab_size, bias=True)\n","    self.gamma = gamma\n","    self.softmax = nn.Softmax(dim = 2)\n","    self.src_embed = src_embed\n","\n","  def embedding(self,x):\n","    return torch.matmul(x,self.src_embed.weight)\n","    \n","  def gumbel_softmax(self,logits, eps=1e-20):\n","    U = torch.rand(logits.shape).to(device)\n","    G = -torch.log(-torch.log(U + eps) + eps).to(device)\n","    return self.softmax((logits + G) / self.gamma)\n","\n","  def forward(self, x):\n","    logits = self.proj(x)\n","    prob = self.gumbel_softmax(logits)\n","    output = self.embedding(prob)\n","    word  = logits.argmax(dim = 2, keepdim = False)\n","    logits = torch.log(self.softmax(logits))\n","\n","    return output, prob, word, logits\n"],"execution_count":68,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Kj2fZ8ZYoHkb"},"source":["#### Beam Search"]},{"cell_type":"code","metadata":{"id":"JxSGMEzZ1bb7","executionInfo":{"status":"ok","timestamp":1637468816266,"user_tz":300,"elapsed":338,"user":{"displayName":"Katherine Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11324848797432579426"}}},"source":["from queue import PriorityQueue\n","class BeamSearchNode:\n","  def __init__(self, hiddenstate, previousNode, cur_embed, wordId, \n","               logProb, gumbel_logits,\n","               length ):\n","    self.h = hiddenstate\n","    self.prevNode = previousNode\n","    self.cur_embed = cur_embed\n","    self.wordid = wordId\n","    self.logp = logProb\n","    self.gumbel_logits = gumbel_logits\n","    self.leng = length\n","    \n","  def __lt__(self,other):\n","    return self.logp < other.logp\n","\n","  def eval(self, alpha=1.0):\n","    return self.logp \n","    # Add here a function for shaping a reward\n","    # reward = 0\n","    # return self.logp / float(self.leng - 1 + 1e-6) + alpha * reward'\n","\n","class BeamSearch:\n","  def __init__(self,decoder, beam_width, topk, line_embed,max_len,\n","               max_iter=2000):\n","    \"\"\"Use Beam Search to generate a full sentence with the given decoder model\n","  Inputs:\n","      - `decoder`: decoder module with forward_step_beam function\n","      - `beam_width` : the length of the beam \n","      - `max_len`: an int representing the maximum decoding length.\n","      - `max_iter`: The maximum decoding iteration\n","    \"\"\"\n","    self.decoder = decoder\n","    self.beam_width = beam_width\n","    self.topk = topk\n","    self.line_embed = line_embed\n","    self.max_len = max_len\n","    self.max_iter = max_iter\n","\n","  def beam_decode(self,inputs, encoder_finals):\n","    \"\"\"Use Beam Search to generate a full sentence with the given decoder model\n","    Inputs:\n","        - `inputs`: a 3d-tensor of shape (batch_size, 1, embed_size)\n","            representing a batch of padded embedded word vectors of SOS . \n","        - `encoder_finals`: a 3d-tensor of shape\n","            (num_enc_layers, batch_size, hidden_size) representing the final\n","            encoder hidden states used to initialize the initial decoder hidden\n","            states.\n","\n","    Returns:\n","        - `final_logp_batch`: a 2d-tensor of shape\n","            (batch_size, sentences_num) representing the probability of generating \n","            the sentence.\n","        - `final_hidden_batch`: a 4d-tensor of shape\n","            (sentences_num, num_layers, batch_size, hidden_size) representing \n","            the final hidden layer\n","        - `final_gumbel_logits_batch`: a 4d-tensor of shape\n","            (batch_size, sentences_num, max_len, vocab_size) representing \n","            the gumbel logits of each hidden layer output. \n","        - `decoded_batch`: a 3d-tensor of shape\n","            (batch_size,sentences_num,  max_len) representing output sentence and\n","            the corresponding word index (can be used for embedding)  \n","    \n","    \"\"\"\n","    decoded_batch,final_gumbel_logits_batch = [],[]\n","    final_hidden_batch, final_logp_batch = [],[]\n","    for i in range(inputs.shape[0]):\n","      decoder_input = inputs[i:i+1,:,:]\n","      # Number of sentence to generate\n","      endnodes = []\n","      number_required = self.topk\n","\n","      # starting node -  hidden vector, previous node, cur_embed, word id , logp, length\n","      node = BeamSearchNode(self.decoder.init_hidden(encoder_finals[:,i:i+1,:]), \n","                            None, decoder_input, [[SOS_INDEX]], 0, None, 1)\n","      nodes = PriorityQueue()\n","\n","      nodes.put((-node.eval(), node))\n","      qsize = 1\n","\n","      while qsize<=self.max_iter:\n","        tocheck = min(nodes.qsize(), self.beam_width)\n","        new_nodes = PriorityQueue()\n","        while tocheck>0:\n","          score, n = nodes.get()\n","          decoder_input = n.cur_embed\n","          decoder_hidden = n.h\n","          if n.leng > self.max_len:\n","            endnodes.append((score, n))\n","            # if we reached maximum # of sentences required\n","            if len(endnodes) >= number_required:\n","                break\n","\n","          # decode for one step using decoder\n","          hidden, _, gumbel_logits, wordId, logsoftmax_logits = decoder.forward_step_beam(decoder_input, \n","                                                                                        decoder_hidden)\n","          tocheck -= 1\n","          # PUT HERE REAL BEAM SEARCH OF TOP\n","          log_prob, indexes = torch.topk(logsoftmax_logits, self.beam_width)\n","          for new_k in range(self.beam_width):\n","            decoded_t = indexes[0][0][new_k].view(1, -1)\n","            log_p = log_prob[0][0][new_k]\n","            prev_embed = self.line_embed(decoded_t)\n","\n","            node = BeamSearchNode(decoder_hidden, n,prev_embed, decoded_t, \n","                                  n.logp + log_p, gumbel_logits,n.leng + 1)\n","            score = -node.eval()\n","            new_nodes.put((score, node))\n","          qsize += self.beam_width - 1\n","        nodes = new_nodes\n","\n","        if len(endnodes) >= number_required:\n","            break\n","        \n","\n","      # choose nbest paths, back trace them\n","      if len(endnodes) == 0:\n","          endnodes = [nodes.get() for _ in range(self.topk)]\n","\n","      utterances = []\n","      final_logps = []\n","      final_hiddens = []\n","      final_gumbel_logits = []\n","      for score, n in sorted(endnodes, key=operator.itemgetter(0)):\n","          end_node = n\n","          utterance = []\n","          gumbel_logits = []\n","          utterance.append(n.wordid[0][0])\n","          gumbel_logits =gumbel_logits + [n.gumbel_logits]\n","          # back trace\n","          while n.prevNode != None:\n","              n = n.prevNode\n","              utterance.append(n.wordid[0][0])\n","              if n.gumbel_logits is not None:\n","                gumbel_logits =gumbel_logits + [n.gumbel_logits]\n","\n","          utterance = torch.unsqueeze(torch.tensor(utterance[::-1][:self.max_len]), axis = 0)\n","          utterances.append(utterance)\n","          gumbel_logits = torch.cat(gumbel_logits, dim = 1)\n","          final_logp = end_node.logp \n","          final_hidden = end_node.h\n","          final_logps.append(final_logp)\n","          final_hiddens.append(torch.unsqueeze(torch.tensor(final_hidden), axis = 0))\n","          final_gumbel_logits.append(gumbel_logits)\n","        \n","      utterances = torch.cat(utterances, axis = 0 )\n","      decoded_batch.append(torch.unsqueeze(utterances, axis = 0))\n","      final_logp_batch.append(torch.unsqueeze(torch.tensor(final_logps), axis = 0))\n","      final_hiddens = torch.cat(final_hiddens, axis = 0)\n","      final_hidden_batch.append(final_hiddens)\n","      final_gumbel_logits = torch.cat(final_gumbel_logits, axis = 0)\n","      final_gumbel_logits_batch.append(torch.unsqueeze(final_gumbel_logits, axis = 0))\n","      # print(\"after decoded_batch:\")\n","      # print(decoded_batch)\n","\n","    # decoded_batch size = (batch, topk, sentence_len, 1)\n","    final_logp_batch = torch.cat(final_logp_batch, axis = 0)\n","    final_hidden_batch = torch.cat(final_hidden_batch, axis = 2)\n","    final_gumbel_logits_batch = torch.cat(final_gumbel_logits_batch, axis = 0)\n","    decoded_batch = torch.cat(decoded_batch, axis = 0)\n","    print(\"final_logp_batch.shape:\",final_logp_batch.shape)\n","    print(\"final_hidden_batch.shape:\",final_hidden_batch.shape)\n","    print(\"final_gumbel_logits_batch.shape:\",final_gumbel_logits_batch.shape)\n","    print(\"decoded_batch.shape:\",decoded_batch.shape)\n","    print(\"max_len:\", self.max_len)\n","    print(\"topk:\", self.topk)\n","    print(\"batch size:\", inputs.shape[0])\n","    return final_hidden_batch, final_logp_batch, final_gumbel_logits_batch, decoded_batch\n","\n","\n"],"execution_count":88,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uH6zk31WoNkJ"},"source":["#### Decoder"]},{"cell_type":"code","metadata":{"id":"plJpAyPu86yw","executionInfo":{"status":"ok","timestamp":1637468851039,"user_tz":300,"elapsed":143,"user":{"displayName":"Katherine Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11324848797432579426"}}},"source":["class Decoder(nn.Module):\n","  \"\"\"An RNN decoder + generator with GRU\"\"\"\n","\n","  def __init__(self, input_size, hidden_size, max_len,generator, embed_layer,\n","               num_layers = 1, dropout=0.):\n","    \"\"\"\n","      Inputs:\n","        - `input_size` , `hidden_size`, and `dropout` the same as in Encoder.\n","    \"\"\"\n","    super(Decoder, self).__init__()\n","    self.input_size = input_size\n","    self.hidden_size = hidden_size\n","    self.rnn = nn.GRU(input_size, hidden_size, num_layers=num_layers, \n","                      batch_first=True,\n","                      dropout=dropout, bidirectional=False)\n","    self.generator = generator\n","    self.max_len = max_len\n","    self.embed_layer = embed_layer\n","\n","  def forward_step(self, prev_embed, hidden):\n","    \"\"\"Unroll the decoder one step at a time.:\n","       Perform a single decoder step (1 word).\n","\n","       Inputs:\n","      - `prev_embed`: a 3d-tensor of shape (batch_size, 1, embed_size)\n","          representing the padded embedded word vectors at this step in training\n","      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size) representing\n","          the current hidden state.\n","\n","      Returns:\n","      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size)\n","          representing the current decoder hidden state.\n","      - `output`: a 3d-tensor of shape (batch_size, max_len, vocab_size)\n","          representing the total generated outputs.\n","    \"\"\"\n","\n","    pre_output,hidden = self.rnn(prev_embed,hidden)\n","    return hidden, pre_output\n","\n","  def forward_step_beam(self, prev_embed, hidden):\n","    \"\"\"Beam Search only: Unroll the decoder one step at a time.\n","    Inputs:\n","      - `input`: a 3d-tensor of shape (batch_size, 1, embed_size)\n","          representing the padded embedded word vectors at this step in training\n","      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size) representing\n","          the current hidden state.\n","    Returns:\n","      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size)\n","          representing the current decoder hidden state.\n","      - `output`: a 3d-tensor of shape (batch_size, 1, vocab_size)\n","          representing the total generated outputs.    \n","      - `gumbel_logits`: a 3d-tensor of shape\n","          (batch_size, 1, trg_vocab_size) representing the mapped decoder\n","          outputs from gumbel softmax.\n","      - `output_word`: a 2d-tensor of shape\n","          (batch_size, 1) representing output sentence and\n","          the corresponding word index (can be used for embedding)    \n","      - `logits`: a 2d-tensor of shape\n","          (batch_size, 1, trg_vocab_size) representing the mapped decoder\n","          outputs from log softmax \n","          \"\"\"\n","    hidden, pre_output = self.forward_step(prev_embed,hidden)\n","    output, gumbel_logits, output_word, logits = self.generator(pre_output)\n","    return  hidden, output, gumbel_logits, output_word, logits\n","\n","  def forward(self, input, encoder_finals,max_len, hidden=None):\n","    \"\"\" Decode the encoder finals to generate a sentence with at most max_len words\n","\n","    Inputs:\n","      - `inputs`: a 3d-tensor of shape (batch_size, 1, embed_size)\n","          representing a batch of padded embedded word vectors of SOS . \n","          If size is (batch_size,max_len, embed_size), then it is teacher forcing.\n","      - `encoder_finals`: a 3d-tensor of shape\n","          (num_enc_layers, batch_size, hidden_size) representing the final\n","          encoder hidden states used to initialize the initial decoder hidden\n","          states.\n","      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size) representing\n","          the value to be used to initialize the initial decoder hidden states.\n","          If None, then use `encoder_finals`.\n","      - `max_len`: an int representing the maximum decoding length.\n","\n","    Returns:\n","      - `hidden`: a 3d-tensor of shape\n","          (num_layers, batch_size, hidden_size) representing the final hidden\n","          state for each element in the batch.\n","      - `outputs`: a 3d-tensor of shape\n","          (batch_size, max_len, hidden_size) representing the raw decoder\n","          outputs (before mapping to a `trg_vocab_size`-dim vector).\n","      - `logits_vectors`: a 3d-tensor of shape\n","          (batch_size, max_len, trg_vocab_size) representing the mapped decoder\n","          outputs.\n","      - `words`: a 2d-tensor of shape\n","          (batch_size, max_len) representing output sentence and\n","          the corresponding word index (can be used for embedding)  \n","    \"\"\"\n","\n","    # Initialize decoder hidden state.\n","    if hidden is None:\n","      hidden = self.init_hidden(encoder_finals)\n","    output_vectors = []\n","    logits_vectors = []\n","    words = []\n","    for i in range(max_len) :\n","      hidden, prev_output = self.forward_step(input,hidden)\n","      input, logits, output_word, _ = self.generator(prev_output)\n","\n","      # input = torch.concat([input,torch.full(input.shape,style)], axis = -1)\n","      logits_vectors.append(logits)\n","      output_vectors.append(input)\n","      words.append(output_word)\n","    outputs = torch.cat(output_vectors, dim =1)\n","    logits_vectors = torch.cat(logits_vectors,dim = 1)\n","    words = torch.cat(words, axis = -1)\n","    return hidden, outputs , logits_vectors, words\n","\n","  def forward_teacher(self, input, encoder_finals,max_len=None, hidden=None):\n","    \"\"\"Unroll the decoder one step at a time.\n","\n","    Inputs:\n","      - `inputs`: a 3d-tensor of shape (batch_size,max_len, embed_size)\n","          representing a batch of padded embedded word vectors of original \n","          sentence and acts as  teacher forcing.\n","\n","    Returns:\n","      - `hidden`: a 3d-tensor of shape\n","          (num_layers, batch_size, hidden_size) representing the final hidden\n","          state for each element in the batch.\n","      - `outputs`: a 3d-tensor of shape\n","          (batch_size, max_len, hidden_size) representing the raw decoder\n","          outputs (before mapping to a `trg_vocab_size`-dim vector).\n","      - `logits_vectors`: a 3d-tensor of shape\n","          (batch_size, max_len, trg_vocab_size) representing the mapped decoder\n","          outputs each represents the probability? \n","      - `words`: a 2d-tensor of shape\n","          (batch_size, max_len) representing output sentence and\n","          the corresponding word index (can be used for embedding) \n","           \n","    \"\"\"\n","\n","    # Initialize decoder hidden state.\n","    if max_len is None:\n","      max_len = input.shape[1]\n","    if hidden is None:\n","      hidden = self.init_hidden(encoder_finals)\n","    output_vectors = []\n","    logits_vectors = []\n","    words = []\n","    for i in range(max_len) :\n","      hidden, prev_output = self.forward_step(input[:,i:i+1,:],hidden)\n","      output, logits , output_word, _ = self.generator(prev_output)\n","      logits_vectors.append(logits)\n","      output_vectors.append(output)\n","      words.append(output_word)\n","    outputs = torch.cat(output_vectors, dim =1)\n","    logits_vectors = torch.cat(logits_vectors,dim = 1)\n","    words = torch.cat(words, axis = -1)\n","    return hidden, outputs ,logits_vectors, words\n","\n","  def init_hidden(self, encoder_finals):\n","    \"\"\"Use encoder final hidden state to initialize decoder's first hidden\n","       state.\n","\n","       Input: `encoder_finals` is same as in forward()\n","\n","       Returns: \n","         - `decoder_init_hiddens`: a 3d-tensor of shape \n","              (num_layers, batch_size, hidden_size) representing the initial\n","              hidden state of the decoder for each element in the batch \n","    \"\"\"\n","    decoder_init_hiddens=encoder_finals\n","    return decoder_init_hiddens"],"execution_count":91,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wg_FbKl-T9xd"},"source":["### Classifier"]},{"cell_type":"code","metadata":{"id":"rjDXvknvT_PM","executionInfo":{"status":"ok","timestamp":1637461374835,"user_tz":300,"elapsed":141,"user":{"displayName":"Katherine Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11324848797432579426"}}},"source":["class LSTMClassifier(nn.Module):\n","  def __init__(self, embed_size, hidden_size, vocab_size, LSTMlayers=1, dropout = 0.2):\n","    super(LSTMClassifier, self).__init__()\n","    self.embed = nn.Linear(vocab_size, embed_size)\n","    self.lstm = nn.LSTM(input_size=embed_size, hidden_size=hidden_size, num_layers=LSTMlayers, batch_first=True)\n","    self.dropout = nn.Dropout(dropout)\n","    self.fc1 =  nn.Linear(in_features=hidden_size, out_features=hidden_size+1)\n","    self.fc2 = nn.Linear(hidden_size+1, 1)\n","\n","  def forward(self, seq):\n","    seq = self.embed(seq)\n","    output, (hidden,cell) = self.lstm(seq)\n","    output = self.dropout(output)\n","    output = self.fc1(output[:,-1])\n","    output = F.relu(output)\n","    output = self.fc2(output)\n","    return output\n","    return output.reshape(output.size(1), -1)"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_xuubDG0F5i3"},"source":["### Training"]},{"cell_type":"code","metadata":{"id":"yjs1mzrQEyef","executionInfo":{"status":"ok","timestamp":1637468853617,"user_tz":300,"elapsed":143,"user":{"displayName":"Katherine Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11324848797432579426"}}},"source":["### Work in progress\n","class TSTModel(nn.Module):\n","  def __init__(self, max_len, vocab_size, embed_size, hidden_size_z, hidden_size_y, \n","               line_embed, encoder, generator, decoder, classifier,beamSeasrch):\n","    super(TSTModel, self).__init__()\n","\n","    self.hidden_size = hidden_size_y + hidden_size_z\n","\n","    self.encoder = encoder\n","    self.generator = generator\n","    self.decoder = decoder\n","    self.classifier = classifier\n","    self.beamSeasrch = beamSeasrch\n","\n","    self.line_embed = line_embed\n","    self.y_embed_enc = nn.Embedding(2,hidden_size_y)\n","    self.y_embed_gen = nn.Embedding(2,hidden_size_y)\n","\n","    self.max_len = max_len\n","    self.vocab_size = vocab_size\n","    self.embed_size = embed_size\n","    self.hidden_size_z = hidden_size_z\n","    self.hidden_size_y = hidden_size_y\n","    \n","\n","  def forward(self, lines, line_lens, labels):\n","    encoded_lines = self.encode(lines, line_lens, labels)\n","    z = encoded_lines[1][-1][:,self.hidden_size_y:]\n","\n","    h0_orig = torch.cat((self.y_embed_gen(labels),z), 1)[None,:]\n","    h0_tsf = torch.cat((self.y_embed_gen(1-labels),z), 1)[None,:]\n","\n","    # Decode back into original form for reconstruction\n","    rec_orig = self.reconstruct(h0_orig, lines)\n","    # Decode into original and transferred forms for classification\n","    decode_orig = self.decode(h0_orig)\n","    decode_tsf = self.decode(h0_tsf)\n","\n","    classifier_lines = torch.cat((decode_orig[2], decode_tsf[2], F.one_hot(lines, self.vocab_size).to(torch.float)), 0)\n","    pred_class = self.classifier(classifier_lines)\n","\n","    return rec_orig, pred_class, decode_orig, decode_tsf\n","\n","  def forward_beam(self,lines, line_lens, labels):\n","    encoded_lines = self.encode(lines, line_lens, labels)\n","    z = encoded_lines[1][-1][:,self.hidden_size_y:]\n","\n","    h0_orig = torch.cat((self.y_embed_gen(labels),z), 1)[None,:]\n","    h0_tsf = torch.cat((self.y_embed_gen(1-labels),z), 1)[None,:]\n","\n","    decode_orig = self.decode_beam(h0_orig)\n","    decode_tsf = self.decode_beam(h0_tsf)\n","\n","    classifier_lines = torch.cat((decode_orig[2][:,-1,:,:], \n","                                  decode_tsf[2][:,-1,:,:], \n","                                  F.one_hot(lines, self.vocab_size).to(torch.float)), 0)\n","    pred_class = self.classifier(classifier_lines)\n","\n","    return pred_class, decode_orig, decode_tsf\n","\n","\n","  def encode(self, lines, line_lens, labels):\n","    init_state = torch.cat((self.y_embed_enc(labels), torch.zeros((len(lines),self.hidden_size_z), device=device)), 1)[None,:].to(device)\n","    return self.encoder(self.line_embed(lines), line_lens, init_state)\n","\n","  def reconstruct(self, h0, lines):\n","    original = self.line_embed(lines)\n","    return self.decoder.forward_teacher(original,h0)\n","\n","  def decode(self, h0):\n","    target = self.line_embed(torch.tensor([SOS_INDEX]).repeat(h0.size()[1],1).to(device))\n","    return self.decoder.forward(target,h0,self.max_len)\n","\n","  def decode_beam(self,h0):\n","    target = self.line_embed(torch.tensor([SOS_INDEX]).repeat(h0.size()[1],1).to(device))\n","    return self.beamSeasrch.beam_decode(target, h0)"],"execution_count":92,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":701},"id":"O8jaGcIiNQUi","executionInfo":{"status":"error","timestamp":1637469393879,"user_tz":300,"elapsed":539569,"user":{"displayName":"Katherine Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11324848797432579426"}},"outputId":"b621de13-c13a-43c1-c7bb-f4e26fcf06bf"},"source":["epochs = 1\n","class_epochs = 0\n","lr = 1e-3\n","batch_size = 32\n","print_every = 100\n","\n","max_len = dataset.max_seq_length\n","vocab_size = len(vocab)\n","embed_size = 256\n","hidden_size_z = 256\n","hidden_size_y = 128\n","hidden_size = hidden_size_z + hidden_size_y\n","dropout = 0.2\n","\n","TAYLOR_STYLE=1 # for information only, don't change\n","DRAKE_STYLE=0  # for information only, don't change\n","train_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","valid_loader = data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n","\n","line_embed = nn.Embedding(vocab_size, embed_size)\n","encoder = Encoder(embed_size,hidden_size)\n","generator = GeneratorTransferredSampled(hidden_size,vocab_size, line_embed)\n","decoder = Decoder(embed_size, hidden_size, max_len=vocab_size, generator = generator,\n","                  embed_layer = line_embed, dropout=dropout)\n","classifier = LSTMClassifier(embed_size, hidden_size_z, vocab_size, dropout =0.2)\n","beamSeasrch = BeamSearch(decoder, 3,3,line_embed,max_len)\n","model = TSTModel(dataset.max_seq_length, vocab_size, embed_size, hidden_size_z, \n","                 hidden_size_y, line_embed, \n","                 encoder, generator, decoder, classifier, beamSeasrch).to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=lr) \n","rec_loss = nn.CrossEntropyLoss(reduction=\"mean\",ignore_index = PAD_INDEX)\n","class_loss = nn.BCEWithLogitsLoss()\n","\n","epoch_losses = []\n","for epoch in range(epochs):\n","  epoch_loss = 0\n","  epoch_class_loss = 0\n","  epoch_rec_loss = 0\n","  model.train()\n","  for i,(taylor_lines, taylor_len, drake_lines, drake_len) in enumerate(train_loader):\n","    lines = torch.cat((taylor_lines, drake_lines), 0).to(device)    \n","    line_lens = torch.cat((taylor_len, drake_len), 0).to(device)\n","    labels = torch.cat((torch.ones(size=(len(taylor_lines),), dtype=torch.int32),torch.zeros(size=(len(drake_lines),),dtype=torch.int32))).to(device)\n","    classifier_labels = torch.cat((labels,1-labels, labels)).unsqueeze(1)\n","\n","    rec_orig, pred_class, decode_orig, decode_tsf = model.forward(lines, line_lens, labels)\n","\n","    pred_class_beam, decode_orig_beam, decode_tsf_beam = model.forward_beam(lines, line_lens, labels)\n","\n","    loss_class = class_loss(pred_class, classifier_labels.to(torch.float))\n","    loss_rec = rec_loss(input=rec_orig[-2].permute(0,2,1), target=lines)\n","    \n","\n","    # print(rec_orig[-2].contiguous().view(-1, rec_orig[-2].size(-1)).size())\n","    # print(lines.contiguous().view(-1).size())\n","    # print(rec_loss_sum(input=rec_orig[-2].permute(0,2,1), target=lines) / rec_loss(input=rec_orig[-2].permute(0,2,1), target=lines))\n","  \n","    loss = loss_class + loss_rec\n","    \n","    epoch_loss += loss\n","    epoch_class_loss += loss_class\n","    epoch_rec_loss += loss_rec\n","\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","    \n","    if model.training and i % print_every == 0:\n","      print(\"Epoch Step: %d Loss: %f\" % (i, loss / lines.size(0)))\n","    break\n","\n","  epoch_losses.append(epoch_loss)\n","  print(\"Finished Epoch \", epoch)\n","  print(\"Epoch Loss\", epoch_loss.item())\n","  print(\"Classification Loss\", epoch_class_loss.item())\n","  print(\"Reconstruction Loss\", epoch_rec_loss.item())"],"execution_count":93,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"]},{"output_type":"stream","name":"stdout","text":["final_logp_batch.shape: torch.Size([64, 3])\n","final_hidden_batch.shape: torch.Size([3, 1, 64, 384])\n","final_gumbel_logits_batch.shape: torch.Size([64, 3, 32, 12850])\n","decoded_batch.shape: torch.Size([64, 3, 32])\n","max_len: 32\n","topk: 3\n","batch size: 64\n","final_logp_batch.shape: torch.Size([64, 3])\n","final_hidden_batch.shape: torch.Size([3, 1, 64, 384])\n","final_gumbel_logits_batch.shape: torch.Size([64, 3, 32, 12850])\n","decoded_batch.shape: torch.Size([64, 3, 32])\n","max_len: 32\n","topk: 3\n","batch size: 64\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-93-4b4808d73dc3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"W8UAfICAG4Km","executionInfo":{"status":"aborted","timestamp":1637443817270,"user_tz":300,"elapsed":17,"user":{"displayName":"Katherine Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11324848797432579426"}}},"source":["decode_beam()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J9ATTK09gakn","executionInfo":{"status":"aborted","timestamp":1637431080509,"user_tz":300,"elapsed":15,"user":{"displayName":"Katherine Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11324848797432579426"}}},"source":["correct_pred = 0\n","correct_pred_all = 0\n","model.eval()\n","with torch.no_grad():\n","  for i,(taylor_lines, taylor_len, drake_lines, drake_len) in enumerate(valid_loader):\n","    lines = torch.cat((taylor_lines, drake_lines), 0).to(device)    \n","    line_lens = torch.cat((taylor_len, drake_len), 0).to(device)\n","    labels = torch.cat((torch.ones(size=(len(taylor_lines),), dtype=torch.int32),torch.zeros(size=(len(drake_lines),),dtype=torch.int32))).to(device)\n","    classifier_labels = torch.cat((labels,1-labels, labels)).unsqueeze(1)\n","\n","    rec_orig,pred_class, decode_orig,decode_tsf =  model(lines, line_lens, labels)\n","    correct_pred += torch.sum(1*(1*(torch.sigmoid(pred_class[-len(lines):])>=0.5) == labels.unsqueeze(1)))\n","    correct_pred_all += torch.sum(1*(1*(torch.sigmoid(pred_class)>=0.5) == classifier_labels))\n","  print(correct_pred / (2.*len(valid_dataset)))\n","  print(correct_pred_all / (3.*2.*len(valid_dataset)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A6LmSVOGr4Ha","executionInfo":{"status":"aborted","timestamp":1637431080510,"user_tz":300,"elapsed":16,"user":{"displayName":"Katherine Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11324848797432579426"}}},"source":["with torch.no_grad():\n","  for i,(taylor_lines, taylor_len, drake_lines, drake_len) in enumerate(valid_loader):\n","    lines = torch.cat((taylor_lines, drake_lines), 0).to(device)    \n","    line_lens = torch.cat((taylor_len, drake_len), 0).to(device)\n","    labels = torch.cat((torch.ones(size=(len(taylor_lines),), dtype=torch.int32),torch.zeros(size=(len(drake_lines),),dtype=torch.int32))).to(device)\n","    classifier_labels = torch.cat((labels,1-labels, labels)).unsqueeze(1)\n","\n","    rec_orig,pred_class,decode_orig,decode_tsf =  model(lines, line_lens, labels)\n","    \n","    break"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"458K4WLmsec2","executionInfo":{"status":"aborted","timestamp":1637431080510,"user_tz":300,"elapsed":16,"user":{"displayName":"Katherine Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11324848797432579426"}}},"source":["def lookup_words(x, vocab):\n","  return [vocab[i] for i in x]\n","\n","print(lookup_words(decode_orig[3][12], vocab))\n","print(lookup_words(decode_tsf[3][12], vocab))\n","print(lookup_words(rec_orig[3][12], vocab))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iIzcL2bFkKWT","executionInfo":{"status":"aborted","timestamp":1637431080511,"user_tz":300,"elapsed":17,"user":{"displayName":"Katherine Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11324848797432579426"}}},"source":["def greedy_decode(model, src_ids, src_lengths, max_len):\n","  \"\"\"Greedily decode a sentence for EncoderDecoder.\"\"\"\n","\n","  with torch.no_grad():\n","    _, encoder_finals = model.encode(src_ids, src_lengths)\n","    prev_y = torch.ones(1, 1).fill_(SOS_INDEX).type_as(src_ids)\n","\n","  output = []\n","  hidden = None\n","\n","  for i in range(max_len):\n","    with torch.no_grad():\n","      hidden, outputs = model.decode(encoder_finals, prev_y, hidden)\n","      prob = model.generator(outputs[:, -1])\n","\n","    _, next_word = torch.max(prob, dim=1)\n","    next_word = next_word.data.item()\n","    output.append(next_word)\n","    prev_y = torch.ones(1, 1).type_as(src_ids).fill_(next_word)\n","\n","  output = np.array(output)\n","\n","  # Cut off everything starting from </s>.\n","  first_eos = np.where(output == EOS_INDEX)[0]\n","  if len(first_eos) > 0:\n","    output = output[:first_eos[0]]\n","  return output\n","\n","def lookup_words(x, vocab):\n","  return [vocab[i] for i in x]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4MXLVujjsglx","executionInfo":{"status":"aborted","timestamp":1637431080512,"user_tz":300,"elapsed":17,"user":{"displayName":"Katherine Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11324848797432579426"}}},"source":["# class_loss = nn.BCEWithLogitsLoss()\n","# optim_class = torch.optim.Adam(classifier.parameters(), lr=lr)\n","# classifier = classifier.to(device)\n","# classifier.train()\n","# for epoch in range(class_epochs):\n","#   epoch_class_loss = 0\n","#   for i,(taylor_lines, taylor_len, drake_lines, drake_len) in enumerate(train_loader):\n","#     lines = torch.cat((taylor_lines, drake_lines), 0).to(device)    \n","#     line_lens = torch.cat((taylor_len, drake_len), 0).to(device)\n","#     labels = torch.cat((torch.ones(size=(len(taylor_lines),), dtype=torch.int32),torch.zeros(size=(len(drake_lines),),dtype=torch.int32))).unsqueeze(1).to(device)\n","    \n","#     pred_class = classifier(F.one_hot(lines, vocab_size).to(torch.float))\n","#     loss_class = class_loss(input=pred_class, target=labels.to(torch.float))\n","#     epoch_class_loss += loss_class\n","\n","#     optim_class.zero_grad()\n","#     loss_class.backward()\n","#     optim_class.step()\n","#   print(epoch_class_loss)"],"execution_count":null,"outputs":[]}]}