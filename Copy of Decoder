{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Decoder","provenance":[{"file_id":"1hUm5UwjeW2RVXC2zj7F0IWGygpGWv7eq","timestamp":1636123209339}],"collapsed_sections":["enSkGLFhcjvN"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"xPxKTvl6M7fS"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","assert device == \"cuda\"  \n","\n","seed = 42\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DrHREQ42qjf6"},"source":["TAYLOR_STYLE = 1\n","DRAKE_STYLE = 0"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N2n26_ARh-_z"},"source":["# Generator"]},{"cell_type":"code","metadata":{"id":"3LqmnHVWM3Ot"},"source":["class Generator(nn.Module):\n","  \"\"\"Define standard linear + softmax generation step.\"\"\"\n","  def __init__(self, hidden_size, vocab_size):\n","    super(Generator, self).__init__()\n","    self.proj = nn.Linear(hidden_size, vocab_size, bias=False)\n","\n","  def forward(self, x):\n","    return F.log_softmax(self.proj(x), dim=-1)\n","\n","class GeneratorTransferredSampled(nn.Module):\n","  \"\"\"Define standard linear + softmax generation step.\"\"\"\n","  def __init__(self, hidden_size, vocab_size, src_embed, gamma=0.1):\n","    \"\"\"\n","    Inputs:\n","      - `src_embed`: a 2d-tensor of shape (vocab_size, embed_size )\n","    \"\"\"\n","    super(GeneratorTransferredSampled, self).__init__()\n","    self.proj = nn.Linear(hidden_size, vocab_size, bias=True)\n","    self.gamma = gamma\n","    self.softmax = nn.Softmax(dim = 2)\n","    self.src_embed = src_embed\n","\n","  def embedding(self,x):\n","    return torch.matmul(x,self.src_embed)\n","    \n","  def gumbel_softmax(self,logits, eps=1e-20):\n","    U = torch.rand(logits.shape)\n","    G = -torch.log(-torch.log(U + eps) + eps)\n","    return self.softmax((logits + G) / self.gamma)\n","\n","  def forward(self, x):\n","    logits = self.proj(x)\n","    prob = self.gumbel_softmax(logits)\n","    output = self.embedding(prob)\n","    word  = logits.argmax(dim = 2, keepdim = False)\n","    return output, logits, word\n","\n","class GeneratorTransferredMax(nn.Module):\n","  \"\"\"Define standard linear + softmax generation step.\"\"\"\n","  def __init__(self, hidden_size, vocab_size, embedding_lookup):\n","    super(GeneratorTransferredMax, self).__init__()\n","    self.proj = nn.Linear(hidden_size, vocab_size, bias=True)\n","    # self.softmax = nn.Softmax(vocab_size)\n","    self.embedding_lookup = embedding_lookup\n","\n","\n","  def forward(self, x):\n","    logits = self.proj(x)\n","    word = logits.argmax(dim=2, keepdim = False)\n","    output = self.embedding_lookup(word)\n","    return output, logits, word"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TjLgiN3piAs9"},"source":["# Decoder"]},{"cell_type":"code","metadata":{"id":"MhQU2huNI4tp"},"source":["class Decoder(nn.Module):\n","  \"\"\"An RNN decoder + generator with GRU\"\"\"\n","\n","  def __init__(self, input_size, hidden_size, max_len,generator, num_layers = 1, dropout=0.):\n","    \"\"\"\n","      Inputs:\n","        - `input_size` , `hidden_size`, and `dropout` the same as in Encoder.\n","    \"\"\"\n","    super(Decoder, self).__init__()\n","    self.input_size = input_size\n","    self.hidden_size = hidden_size\n","    self.rnn = nn.GRU(input_size, hidden_size, num_layers=num_layers, batch_first=True,\n","                      dropout=dropout, bidirectional=False)\n","    self.generator = generator\n","    self.max_len = max_len\n","\n","  def forward_step(self, prev_embed, hidden):\n","    \"\"\"Helper function for forward below:\n","       Perform a single decoder step (1 word).\n","\n","       Inputs:\n","      - `prev_embed`: a 3d-tensor of shape (batch_size, 1, embed_size = vocab_size)\n","          representing the padded embedded word vectors at this step in training\n","      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size) representing\n","          the current hidden state.\n","\n","      Returns:\n","      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size)\n","          representing the current decoder hidden state.\n","      - `output`: a 3d-tensor of shape (batch_size, max_len, vocab_size)\n","          representing the total generated outputs.\n","    \"\"\"\n","\n","    pre_output,hidden =self.rnn(prev_embed,hidden)\n","    return hidden, pre_output\n","\n","  def forward(self, input, encoder_finals,max_len, style, hidden=None):\n","    \"\"\"Unroll the decoder one step at a time.\n","\n","    Inputs:\n","      - `inputs`: a 3d-tensor of shape (batch_size, 1, embed_size)\n","          representing a batch of padded embedded word vectors of SOS . \n","          If size is (batch_size,max_len, embed_size), then it is teacher forcing.\n","      - `encoder_finals`: a 3d-tensor of shape\n","          (num_enc_layers, batch_size, hidden_size) representing the final\n","          encoder hidden states used to initialize the initial decoder hidden\n","          states.\n","      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size) representing\n","          the value to be used to initialize the initial decoder hidden states.\n","          If None, then use `encoder_finals`.\n","      - `max_len`: an int representing the maximum decoding length.\n","      - `style`: TAYLOR_STYLE or DRAKE_STYLE\n","\n","    Returns:\n","      - `hidden`: a 3d-tensor of shape\n","          (num_layers, batch_size, hidden_size) representing the final hidden\n","          state for each element in the batch.\n","      - `outputs`: a 3d-tensor of shape\n","          (batch_size, max_len, hidden_size) representing the raw decoder\n","          outputs (before mapping to a `trg_vocab_size`-dim vector).\n","      - `logits_vectors`: a 3d-tensor of shape\n","          (batch_size, max_len, trg_vocab_size) representing the mapped decoder\n","          outputs.\n","      - `words`: a 3d-tensor of shape\n","          (batch_size, max_len, 1) representing output sentence and\n","          the corresponding word index (can be used for embedding)  \n","    \"\"\"\n","\n","    # Initialize decoder hidden state.\n","    if hidden is None:\n","      hidden = self.init_hidden(encoder_finals)\n","    output_vectors = []\n","    logits_vectors = []\n","    words = []\n","    for i in range(max_len) :\n","      hidden, prev_output = self.forward_step(input,hidden)\n","      input, logits, output_word = self.generator(prev_output)\n","      if style == 'target':\n","        input = torch.concat([input,torch.full(input.shape,style)], axis = -1)\n","      else:\n","        input = torch.concat([input,torch.full(input.shape,style)], axis = -1)\n","          \n","      logits_vectors.append(logits)\n","      output_vectors.append(input)\n","      words.append(output_word)\n","    outputs = torch.cat(output_vectors, dim =1)\n","    logits_vectors = torch.cat(logits_vectors,dim = 1)\n","    words = torch.cat(words, axis = -1)\n","    return hidden, outputs ,logits_vectors, words\n","\n","  def forward_teacher(self, input, encoder_finals,max_len=None, hidden=None):\n","    \"\"\"Unroll the decoder one step at a time.\n","\n","    Inputs:\n","      - `inputs`: a 3d-tensor of shape (batch_size,max_len, embed_size)\n","          representing a batch of padded embedded word vectors of original \n","          sentence and acts as  teacher forcing.\n","\n","    Returns:\n","      - `hidden`: a 3d-tensor of shape\n","          (num_layers, batch_size, hidden_size) representing the final hidden\n","          state for each element in the batch.\n","      - `outputs`: a 3d-tensor of shape\n","          (batch_size, max_len, hidden_size) representing the raw decoder\n","          outputs (before mapping to a `trg_vocab_size`-dim vector).\n","      - `logits_vectors`: a 3d-tensor of shape\n","          (batch_size, max_len, trg_vocab_size) representing the mapped decoder\n","          outputs each represents the probability? \n","      - `words`: a 3d-tensor of shape\n","          (batch_size, max_len, 1) representing output sentence and\n","          the corresponding word index (can be used for embedding)      \n","    \"\"\"\n","\n","    # Initialize decoder hidden state.\n","    if max_len is None:\n","      max_len = input.shape[1]\n","    if hidden is None:\n","      hidden = self.init_hidden(encoder_finals)\n","    output_vectors = []\n","    logits_vectors = []\n","    words = []\n","    for i in range(max_len) :\n","      hidden, prev_output = self.forward_step(input[:,i:i+1,:],hidden)\n","      output, logits , output_word= self.generator(prev_output)\n","      logits_vectors.append(logits)\n","      output_vectors.append(output)\n","      words.append(output_word)\n","    outputs = torch.cat(output_vectors, dim =1)\n","    logits_vectors = torch.cat(logits_vectors,dim = 1)\n","    words = torch.cat(words, axis = -1)\n","    return hidden, outputs ,logits_vectors, words\n","\n","  def init_hidden(self, encoder_finals):\n","    \"\"\"Use encoder final hidden state to initialize decoder's first hidden\n","       state.\n","\n","       Input: `encoder_finals` is same as in forward()\n","\n","       Returns: \n","         - `decoder_init_hiddens`: a 3d-tensor of shape \n","              (num_layers, batch_size, hidden_size) representing the initial\n","              hidden state of the decoder for each element in the batch \n","    \"\"\"\n","    decoder_init_hiddens=encoder_finals\n","    return decoder_init_hiddens"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"enSkGLFhcjvN"},"source":["# Homework 1 Decoder"]},{"cell_type":"code","metadata":{"id":"6xC70TErJTIM"},"source":["class DecoderHW(nn.Module):\n","  \"\"\"An RNN decoder with GRU\"\"\"\n","\n","  def __init__(self, input_size, hidden_size, dropout=0.):\n","    \"\"\"\n","      Inputs:\n","        - `input_size`, `hidden_size`, and `dropout` the same as in Encoder.\n","    \"\"\"\n","    super(Decoder, self).__init__()\n","    self.input_size = input_size\n","    self.hidden_size = hidden_size\n","    self.rnn = nn.GRU(input_size, hidden_size, num_layers=1, batch_first=True,\n","                      dropout=dropout, bidirectional=False)\n","\n","  def forward_step(self, prev_embed, hidden):\n","    \"\"\"Helper function for forward below:\n","       Perform a single decoder step (1 word).\n","\n","       Inputs:\n","      - `prev_embed`: a 3d-tensor of shape (batch_size, 1, embed_size)\n","          representing the padded embedded word vectors at this step in training\n","      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size) representing\n","          the current hidden state.\n","\n","      Returns:\n","      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size)\n","          representing the current decoder hidden state.\n","      - `pre_output`: a 3d-tensor of shape (batch_size, 1, hidden_size)\n","          representing the total decoder output for one step\n","    \"\"\"\n","\n","    pre_output,hidden =self.rnn(prev_embed,hidden)\n","    return hidden, pre_output\n","    \n","  def forward(self, inputs, encoder_finals, hidden=None, max_len=None):\n","    \"\"\"Unroll the decoder one step at a time.\n","\n","    Inputs:\n","      - `inputs`: a 3d-tensor of shape (batch_size, max_seq_length, embed_size)\n","          representing a batch of padded embedded word vectors of target\n","          sentences (for teacher-forcing during training).\n","      - `encoder_finals`: a 3d-tensor of shape\n","          (num_enc_layers, batch_size, hidden_size) representing the final\n","          encoder hidden states used to initialize the initial decoder hidden\n","          states.\n","      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size) representing\n","          the value to be used to initialize the initial decoder hidden states.\n","          If None, then use `encoder_finals`.\n","      - `max_len`: an int representing the maximum decoding length.\n","\n","    Returns:\n","      - `hidden`: a 3d-tensor of shape\n","          (num_layers, batch_size, hidden_size) representing the final hidden\n","          state for each element in the batch.\n","      - `pre_output_vectors`: a 3d-tensor of shape\n","          (batch_size, max_len, hidden_size) representing the raw decoder\n","          outputs (before mapping to a `trg_vocab_size`-dim vector).\n","    \"\"\"\n","\n","    # The maximum number of steps to unroll the RNN.\n","    if max_len is None:\n","      max_len = inputs.size(1)\n","\n","    # Initialize decoder hidden state.\n","    if hidden is None:\n","      hidden = self.init_hidden(encoder_finals)\n","    prev_output_vectors = []\n","    \n","    for i in range(min(inputs.shape[1],max_len)) :\n","      hidden, prev_output = self.forward_step(input,hidden)\n","      prev_output_vectors.append(prev_output)\n","    prev_output_vectors = torch.cat(prev_output_vectors, dim =1)\n","    return hidden, prev_output_vectors\n","\n","  def init_hidden(self, encoder_finals):\n","    \"\"\"Use encoder final hidden state to initialize decoder's first hidden\n","       state.\n","\n","       Input: `encoder_finals` is same as in forward()\n","\n","       Returns: \n","         - `decoder_init_hiddens`: a 3d-tensor of shape \n","              (num_layers, batch_size, hidden_size) representing the initial\n","              hidden state of the decoder for each element in the batch \n","    \"\"\"\n","    decoder_init_hiddens=encoder_finals\n","    return decoder_init_hiddens"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SjxiQXKUL0UZ"},"source":["## Greedy Decoder to visualize result\n"]},{"cell_type":"code","metadata":{"id":"ZSzS6opFm9zR"},"source":["def greedy_decode(max_len, src_ids, encoder_finals, decoder, embedding, target_ids = None, teach_force = False):\n","  \"\"\"Greedily decode a transformed sentence for EncoderDecoder with Decoder class implemented above.\n","  SOS_INDEX: Start of sentence index.\n","  EOS_INDEX: End of sentence index.\n","  If ori_sent is None, then it is self-feeding. Else it is teacher forcing\n","\n","  Inputs:\n","      - `max_len`: an int representing the maximum decoding length (=max_seq_length).\n","      - `src_ids`: a 2d-tensor of shape (batch_size, max_seq_length) representing\n","        a batch of source sentences of word ids.\n","      - `encoder_finals`: a 3d-tensor of shape\n","          (num_enc_layers, batch_size, hidden_size) representing the final\n","          encoder hidden states used to initialize the initial decoder hidden\n","          states.\n","      - `decoder`: an `Decoder` object.\n","      - `embedding`: an nn.Embedding object representing the lookup table for\n","          input (source) sentences.\n","      - `teach_force`: a boolean representing whether the source ids should be input\n","  \n","  Returns:\n","      - `outout`: a 3d-tensor of shape\n","          (batch_size, max_seq_length, hidden_size) representing the final hidden\n","          state for each element in the batch.\n","      - `output_word`: a 2d-tensor of shape (batch_size, max_seq_length) \"\"\"\n","  batch_size = encoder_finals.shape[1]\n","  \n","\n","  # output_word = []\n","  output = []\n","  hidden = None\n","\n","  # if teach_force:\n","  #   for i in range(max_len):\n","  #     with torch.no_grad():\n","  #       hidden, outputs = decoder(embedding(src_ids[:,i:i+1]),encoder_finals, hidden)\n","  #       prob = model.generator(outputs[:, -1])\n","\n","  #     output.append(prob)\n","  #     _, next_word = torch.max(prob, dim=1)\n","  #     next_word = next_word.data[0]\n","  #     output_word.append(next_word)\n","  #     prev_y = torch.ones(batch_size, 1).type_as(src_ids).fill_(next_word)\n","  # else:\n","  with torch.no_grad():\n","    prev_y = torch.ones(batch_size, 1).fill_(SOS_INDEX).type_as(src_ids)\n","  for i in range(max_len-1):\n","    with torch.no_grad():\n","      hidden, outputs = decoder(embedding(prev_y), encoder_finals, hidden)\n","      prob = model.generator(outputs[:, -1])\n","\n","    output.append(prob)\n","    _, next_word = torch.max(prob, dim=1, keepdim = True)\n","    next_word = next_word.data\n","    output_word.append(next_word)\n","    prev_y = torch.ones(batch_size, 1).type_as(src_ids).fill_(next_word)\n","\n","    \n","\n","  output = np.concatenate(output, axis = 1)\n","  output_word = np.concatenate(output_word, axis = -1)\n","\n","  # Cut off everything starting from </s>.\n","  # first_eos = np.where(output == EOS_INDEX)[0]\n","  # if len(first_eos) > 0:\n","  #   output = output[:first_eos[0]]\n","  return output, output_word"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UV7cn87ML2uE"},"source":["## Beam Search"]},{"cell_type":"code","metadata":{"id":"dU9O49n3L3mR"},"source":["import copy\n","class BeamNode:\n","  def __init__(self, hidden, input, wordInd, nll, prob):\n","        '''\n","        hidden: hiddenstate\n","        input: input\n","        wordInd: word indices\n","        nll: negative loss likelihood\n","        '''\n","        self.hidden = hidden\n","        self.input = input\n","        self.wordInd = wordInd\n","        self.nll = nll\n","        self.prob = prob\n","\n","\n","def beam_search(self,max_len,src_ids, encoder_finals, decoder, embedding, beam_size):\n","  \"\"\"Beam Search a transformed sentence for EncoderDecoder with Decoder class implemented above.\n","  SOS_INDEX: Start of sentence index.\n","  EOS_INDEX: End of sentence index.\n","  If ori_sent is None, then it is self-feeding. Else it is teacher forcing\n","\n","  Inputs:\n","      - `max_len`: an int representing the maximum decoding length.\n","      - `src_ids`: a 2d-tensor of shape (batch_size, max_seq_length) representing\n","        a batch of source sentences of word ids.\n","      - `encoder_finals`: a 3d-tensor of shape\n","          (num_enc_layers, batch_size, hidden_size) representing the final\n","          encoder hidden states used to initialize the initial decoder hidden\n","          states.\n","      - `decoder`: an `Decoder` object.\n","      - `embedding`: an nn.Embedding object representing the lookup table for\n","          input (source) sentences.\n","  \n","  Returns:\n","      - `outout`: a 3d-tensor of shape \n","          (batch_size, max_seq_length, hidden_size) representing the final hidden\n","          state (log softmax) for each element in the batch.\n","      - `output_word`: a 2d-tensor of shape (batch_size, max_seq_length)\"\"\"\n","\n","  batch_size = encoder_finals.shape[1]\n","  output = []\n","  with torch.no_grad():\n","    prev_y = torch.ones(batch_size, 1).fill_(SOS_INDEX).type_as(src_ids)\n","\n","  init_state = BeamState(h, \n","                         prev_y[:,0],\n","                         [[] for i in range(batch_size)], \n","                         [0] * batch_size)\n","  beam = [init_state]\n","\n","  for t in range(max_len):\n","    exp = [[] for i in range(batch_size)]\n","    for state in beam:\n","      with torch.no_grad():\n","        hidden, outputs = decoder(embedding(src_ids[:,i:i+1]),encoder_finals, hidden)\n","        prob = model.generator(outputs[:, -1])\n","      output, next_word = torch.topk(prob, beam_size, dim=-1)\n","      \n","      for i in range(batch_size):\n","          for l in range(beam_width):\n","              exp[i].append(BeamState(hidden[:,i:i+1,:], next_word[i,l],\n","                  state.wordInd[i] + [next_word[i,l]],\n","                  state.nll[i] - prob[i,l]))\n","\n","      beam = [copy.deepcopy(init_state) for _ in range(beam_size)]\n","      for i in range(batch_size):\n","          a = sorted(exp[i], key=lambda k: k.nll)\n","          for k in range(beam_size):\n","              beam[k].h[i] = a[k].h\n","              beam[k].inp[i] = a[k].inp\n","              beam[k].wordInd[i] = a[k].wordInd\n","              beam[k].nll[i] = a[k].nll\n","              beam[k].prob.append(prob)\n","\n","  return np.concatenate(beam[0].prob, axis = 1)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"57W7jcmUtOJA"},"source":["# Attention Based (not finished)"]},{"cell_type":"code","metadata":{"id":"5W9fwlH_NBdt"},"source":["class BahdanauAttention(nn.Module):\n","  \"\"\"Implements Bahdanau (MLP) attention.\"\"\"\n","\n","  def __init__(self, hidden_size, key_size=None, query_size=None):\n","    super(BahdanauAttention, self).__init__()\n","\n","    key_size = hidden_size if key_size is None else key_size\n","    query_size = hidden_size if query_size is None else query_size\n","\n","    self.key_layer = nn.Linear(key_size, hidden_size, bias=False)\n","    self.query_layer = nn.Linear(query_size, hidden_size, bias=False)\n","    self.energy_layer = nn.Linear(hidden_size, 1, bias=False)\n","\n","    # To store attention scores.\n","    self.alphas = None\n","        \n","  def forward(self, query=None, proj_key=None, value=None, mask=None):\n","    assert mask is not None, \"mask is required\"\n","\n","    # We first project the query (the decoder state).\n","    # The projected keys (the encoder states) were already pre-computated.\n","    query = self.query_layer(query)\n","\n","    # Calculate scores.\n","    scores = self.energy_layer(torch.tanh(query + proj_key))\n","    scores = scores.squeeze(2).unsqueeze(1)\n","\n","    # Mask out invalid positions.\n","    # The mask marks valid positions so we invert it using `mask & 0`.\n","    scores.data.masked_fill_(mask == 0, -float('inf'))\n","\n","    # Turn scores to probabilities.\n","    alphas = F.softmax(scores, dim=-1)\n","    self.alphas = alphas        \n","\n","    # The context vector is the weighted sum of the values.\n","    context = torch.bmm(alphas, value)\n","\n","    # context shape: [B, 1, 2D], alphas shape: [B, 1, M]\n","    return context, alphas"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_fEDceFotPbw"},"source":["class AttentionDecoder(nn.Module):\n","  \"\"\"An attention-based RNN decoder.\"\"\"\n","\n","  def __init__(self, input_size, hidden_size, attention, dropout=0.):\n","    \"\"\"\n","      Inputs:\n","        - `input_size`, `hidden_size`, and `dropout` the same as in Encoder.\n","    \"\"\"\n","    super(AttentionDecoder, self).__init__()\n","\n","    self.attention = attention\n","\n","    self.rnn = nn.GRU(input_size + hidden_size, hidden_size, batch_first=True,\n","                      dropout=dropout)\n","\n","    # To initialize from the final encoder state.\n","    self.bridge = nn.Linear(hidden_size, hidden_size, bias=True)\n","\n","    self.dropout_layer = nn.Dropout(p=dropout)\n","    self.pre_output_layer = nn.Linear(hidden_size + hidden_size + input_size,\n","                                      hidden_size, bias=False)\n","\n","  def forward_step(self, prev_embed, encoder_hidden, src_mask, proj_key,\n","                   hidden):\n","    \"\"\"Perform a single decoder step (1 word)\"\"\"\n","\n","    # Compute context vector using attention mechanism.\n","    query = hidden[-1].unsqueeze(1)\n","\n","    context, attn_probs = self.attention(query=query, proj_key=proj_key,\n","                                         value=encoder_hidden, mask=src_mask)\n","\n","    # Update RNN hidden state.\n","    rnn_input = torch.cat([prev_embed, context], dim=2)\n","    output, hidden = self.rnn(rnn_input, hidden)\n","\n","    pre_output = torch.cat([prev_embed, output, context], dim=2)\n","\n","    pre_output = self.dropout_layer(pre_output)\n","    pre_output = self.pre_output_layer(pre_output)\n","\n","    return output, hidden, pre_output\n","    \n","  def forward(self,  encoder_hiddens, encoder_finals,  src_mask,\n","              trg_mask,inputs=None, hidden=None, max_len=None):\n","    \"\"\"Unroll the decoder one step at a time.\n","    \n","    Inputs:\n","      - `inputs`: a 3d-tensor of shape (batch_size, max_seq_length, embed_size)\n","          representing a batch of padded embedded word vectors of target\n","          sentences (if teacher-forcing during training).\n","      - `encoder_hiddens`: a 3d-tensor of shape\n","          (batch_size, max_seq_length, hidden_size) representing the encoder\n","          outputs for each decoding step to attend to. \n","      - `encoder_finals`: a 3d-tensor of shape\n","          (num_enc_layers, batch_size, hidden_size) representing the final\n","          encoder hidden states used to initialize the initial decoder hidden\n","          states.\n","      - `src_mask`: a 3d-tensor of shape (batch_size, 1, max_seq_length)\n","          representing the mask for source sentences.\n","      - `trg_mask`: a 3d-tensor of shape (batch_size, 1, max_seq_length)\n","          representing the mask for target sentences.\n","      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size) representing\n","          the value to be used to initialize the initial decoder hidden states.\n","          If None, then use `encoder_finals`.\n","      - `max_len`: an int representing the maximum decoding length.\n","\n","    Returns:\n","      - `hidden`: a 3d-tensor of shape\n","          (num_layers, batch_size, hidden_size) representing the final hidden\n","          state for each element in the batch.\n","      - `pre_output_vectors`: a 3-d tensor of shape\n","          (batch_size, max_len, hidden_size) representing the raw decoder\n","          outputs (before mapping to a `trg_vocab_size`-dim vector).\n","    \"\"\"\n","\n","    # The maximum number of steps to unroll the RNN.\n","    if max_len is None:\n","      max_len = trg_mask.size(-1)\n","\n","    if hidden is None:\n","      hidden = self.init_hidden(encoder_finals)\n","\n","    # Pre-compute projected encoder hidden states (the \"keys\" for the attention\n","    # mechanism). This is only done for efficiency.\n","    proj_key = self.attention.key_layer(encoder_hiddens)\n","\n","    # Here we store all intermediate hidden states and pre-output vectors.\n","    decoder_states = []\n","    pre_output_vectors = []\n","\n","    # Unroll the decoder RNN for `max_len` steps.\n","    for i in range(max_len):\n","      if inputs is not None:\n","        prev_embed = inputs[:, i].unsqueeze(1)\n","      else:\n","        prev_embed = pre_output\n","      output, hidden, pre_output = self.forward_step(prev_embed,\n","                                                     encoder_hiddens, src_mask,\n","                                                     proj_key, hidden)\n","      decoder_states.append(output)\n","      pre_output_vectors.append(pre_output)\n","\n","    decoder_states = torch.cat(decoder_states, dim=1)\n","    pre_output_vectors = torch.cat(pre_output_vectors, dim=1)\n","    return hidden, pre_output_vectors\n","\n","  def init_hidden(self, encoder_finals):\n","    \"\"\"Use encoder final hidden state to initialize decoder's first hidden\n","    state.\"\"\"\n","    decoder_init_hiddens = torch.tanh(self.bridge(encoder_finals))\n","    return decoder_init_hiddens"],"execution_count":null,"outputs":[]}]}