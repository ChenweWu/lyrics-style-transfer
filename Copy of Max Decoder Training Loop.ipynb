{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Copy of Max Decoder Training Loop.ipynb","provenance":[],"collapsed_sections":["m7FQaWQbojAb"],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"m7FQaWQbojAb"},"source":["### Import and Setup"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wyz5JwHDopli","executionInfo":{"status":"ok","timestamp":1637701429422,"user_tz":300,"elapsed":5295,"user":{"displayName":"Katherine Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11324848797432579426"}},"outputId":"009982c8-70aa-479f-bcd1-0394cdf6ff1b"},"source":["!pip install unidecode"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting unidecode\n","  Downloading Unidecode-1.3.2-py3-none-any.whl (235 kB)\n","\u001b[?25l\r\u001b[K     |█▍                              | 10 kB 22.9 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 20 kB 26.9 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 30 kB 22.3 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 40 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |███████                         | 51 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 61 kB 12.5 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 71 kB 11.1 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 81 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 92 kB 10.8 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 102 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 112 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 122 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 133 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 143 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 153 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 163 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 174 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 184 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 194 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 204 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 215 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 225 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 235 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 235 kB 11.7 MB/s \n","\u001b[?25hInstalling collected packages: unidecode\n","Successfully installed unidecode-1.3.2\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T8q-CET4os1p","executionInfo":{"status":"ok","timestamp":1637701451336,"user_tz":300,"elapsed":21920,"user":{"displayName":"Katherine Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11324848797432579426"}},"outputId":"54c51a38-f8e2-490d-9423-97ad226a3974"},"source":["import json\n","import re\n","from unidecode import unidecode\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","import nltk\n","nltk.download('punkt')\n","from collections import Counter\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"0ELBqq8kpAZS","executionInfo":{"status":"ok","timestamp":1637701457509,"user_tz":300,"elapsed":6175,"user":{"displayName":"Katherine Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11324848797432579426"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","assert device == \"cuda\"  \n","\n","seed = 42\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S3i2hvtYoo30"},"source":["### Data"]},{"cell_type":"code","metadata":{"id":"OGzVRiwZowGv","executionInfo":{"status":"ok","timestamp":1637701489162,"user_tz":300,"elapsed":31656,"user":{"displayName":"Katherine Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11324848797432579426"}}},"source":["f = open('/content/drive/Shareddrives/MIT NLP 8.864/Data/drake.json')\n","drake = json.load(f)\n","f.close()\n","\n","f = open('/content/drive/Shareddrives/MIT NLP 8.864/Data/tswift.json')\n","taylor = json.load(f)\n","f.close()\n","\n","drake = [drake['songs'][i]['lyrics'] for i in range(len(drake['songs']))]\n","taylor = [taylor['songs'][i]['lyrics'] for i in range(len(taylor['songs']))]\n","\n","taylor_lyrics = [re.sub('\\u2005', ' ', re.sub(r'[\\(\\[].*?[\\)\\]]', '', taylor[i])).split('\\n') for i in range(len(taylor))]\n","taylor_lyrics = [[unidecode(i) for i in taylor_lyrics[j]] for j in range(len(taylor_lyrics))]\n","taylor_lyrics = [[re.sub('\\d+EmbedShare URLCopyEmbedCopy', '', i) for i in taylor_lyrics[j]] for j in range(len(taylor_lyrics))]\n","taylor_lyrics = [[re.sub('\\d+.EmbedShare URLCopyEmbedCopy', '', i) for i in taylor_lyrics[j]] for j in range(len(taylor_lyrics))]\n","taylor_lyrics = [[re.sub('EmbedShare URLCopyEmbedCopy', '', i) for i in taylor_lyrics[j]] for j in range(len(taylor_lyrics))]\n","taylor_lyrics = [[i for i in taylor_lyrics[j] if i != ''] for j in range(len(taylor_lyrics))]\n","\n","drake_lyrics = [re.sub('\\u2005', ' ', re.sub(r'[\\(\\[].*?[\\)\\]]', '', drake[i])).split('\\n') for i in range(len(drake))]\n","drake_lyrics = [[unidecode(i) for i in drake_lyrics[j]] for j in range(len(drake_lyrics))]\n","drake_lyrics = [[re.sub('\\d+EmbedShare URLCopyEmbedCopy', '', i) for i in drake_lyrics[j]] for j in range(len(drake_lyrics))]\n","drake_lyrics = [[re.sub('\\d+.EmbedShare URLCopyEmbedCopy', '', i) for i in drake_lyrics[j]] for j in range(len(drake_lyrics))]\n","drake_lyrics = [[re.sub('EmbedShare URLCopyEmbedCopy', '', i) for i in drake_lyrics[j]] for j in range(len(drake_lyrics))]\n","drake_lyrics = [[i for i in drake_lyrics[j] if i != ''] for j in range(len(drake_lyrics))]\n","\n","taylor_lyrics = [[line1 + ', ' + line2 for line1,line2 in zip(song[0::2], song[1::2])] for song in taylor_lyrics]\n","drake_lyrics = [[line1 + ', ' + line2 for line1,line2 in zip(song[0::2], song[1::2])] for song in drake_lyrics]\n","\n","drake_tokenized = [[word_tokenize(drake_lyrics[i][j]) for j in range(len(drake_lyrics[i]))] for i in range(len(drake_lyrics))]\n","taylor_tokenized = [[word_tokenize(taylor_lyrics[i][j]) for j in range(len(taylor_lyrics[i]))] for i in range(len(taylor_lyrics))]\n","\n","drake_tokenized = [[[word.lower() for word in line] for line in song] for song in drake_tokenized]\n","taylor_tokenized = [[[word.lower() for word in line] for line in song] for song in taylor_tokenized]\n","\n","drake_length = sum([[len(sent) for sent in song] for song in drake_tokenized], [])\n","taylor_length = sum([[len(sent) for sent in song] for song in taylor_tokenized], [])\n","\n","drake_lyrics = sum([[sent for sent in song if (len(sent) >= 10 and len (sent) <= 30)] for song in drake_tokenized], [])\n","taylor_lyrics = sum([[sent for sent in song if (len(sent) >= 10 and len (sent) <= 30)] for song in taylor_tokenized], [])\n","\n","taylor_vocab = sum(taylor_lyrics,[])\n","drake_vocab = sum(drake_lyrics,[])\n","\n","def unique(list1):\n","     \n","    # insert the list to the set\n","    list_set = set(list1)\n","    # convert the set to the list\n","    unique_list = (list(list_set))\n","    return unique_list\n","\n","vocab = taylor_vocab + drake_vocab\n","vocab_counts = Counter(vocab)\n","vocab = unique(vocab)\n","vocab = ['<pad>','<unk>','<s>', '</s>'] + vocab\n","\n","from torch.utils import data\n","import torch\n","\n","# These IDs are reserved.\n","MAX_SENT_LENGTH = 30\n","MAX_SENT_LENGTH_PLUS_SOS_EOS = 32\n","PAD_INDEX = 0\n","UNK_INDEX = 1\n","SOS_INDEX = 2\n","EOS_INDEX = 3\n","RARE_WORD_TRESHOLD = 0\n","\n","vocab_counts['<pad>'] = RARE_WORD_TRESHOLD + 1\n","vocab_counts['<unk>'] = RARE_WORD_TRESHOLD + 1\n","vocab_counts['<s>'] = RARE_WORD_TRESHOLD + 1\n","vocab_counts['</s>'] = RARE_WORD_TRESHOLD + 1\n","\n","class TSTDataset(data.Dataset):\n","    def __init__(self, taylor_sentences, drake_sentences, vocab, vocab_counts, sampling=1.):\n","        self.taylor_sentences = taylor_sentences[:int(len(taylor_sentences) * sampling)]\n","        self.drake_sentences = drake_sentences[:int(len(drake_sentences) * sampling)]\n","\n","        self.max_seq_length = MAX_SENT_LENGTH_PLUS_SOS_EOS\n","        self.vocab = vocab\n","        self.vocab_counts = vocab_counts\n","\n","        self.v2id = {v : i for i, v in enumerate(self.vocab)}\n","        self.id2v = {val : key for key, val in self.v2id.items()}\n","    \n","    def __len__(self):\n","        return min(len(self.taylor_sentences), len(self.drake_sentences))\n","    \n","    def __getitem__(self, index):\n","        taylor_sent = self.taylor_sentences[index]\n","        taylor_len = len(taylor_sent) + 2   # add <s> and </s> to each sentence\n","        taylor_id = []\n","        for w in taylor_sent:\n","            if w not in self.vocab:\n","                w = '<unk>'\n","            if vocab_counts[w] <= RARE_WORD_TRESHOLD:\n","                w = '<unk>'\n","            taylor_id.append(self.v2id[w])\n","\n","        taylor_id = ([SOS_INDEX] + taylor_id + [EOS_INDEX] + [PAD_INDEX] *\n","                  (self.max_seq_length - taylor_len))\n","\n","        drake_sent = self.drake_sentences[index]\n","        drake_len = len(drake_sent) + 2   # add <s> and </s> to each sentence\n","        drake_id = []\n","        for w in drake_sent:\n","            if w not in self.vocab:\n","                w = '<unk>'\n","            if vocab_counts[w] <= RARE_WORD_TRESHOLD:\n","                w = '<unk>'\n","            drake_id.append(self.v2id[w])\n","\n","        drake_id = ([SOS_INDEX] + drake_id + [EOS_INDEX] + [PAD_INDEX] *\n","                  (self.max_seq_length - drake_len))\n","\n","        return torch.tensor(taylor_id), taylor_len, torch.tensor(drake_id), drake_len\n","\n","dataset = TSTDataset(taylor_lyrics, drake_lyrics, vocab, vocab_counts)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"eL-YPQouRkxy","executionInfo":{"status":"ok","timestamp":1637701489164,"user_tz":300,"elapsed":9,"user":{"displayName":"Katherine Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11324848797432579426"}}},"source":["test_pct = 0.2\n","valid_pct = 0.1\n","\n","train_dataset, test_dataset = torch.utils.data.random_split(dataset, [int(len(dataset)*(1-test_pct)),len(dataset)-int(len(dataset)*(1-test_pct))])\n","valid_dataset, train_dataset = torch.utils.data.random_split(train_dataset, [int(len(dataset)*valid_pct),len(train_dataset)-int(len(dataset)*valid_pct)])"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ynBTHa5grUr7"},"source":["### Encoder"]},{"cell_type":"code","metadata":{"id":"FieXza5erYUH","executionInfo":{"status":"ok","timestamp":1637701489164,"user_tz":300,"elapsed":5,"user":{"displayName":"Katherine Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11324848797432579426"}}},"source":["from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","\n","class Encoder(nn.Module):\n","  def __init__(self, input_size, hidden_size, dropout=0.):\n","    \"\"\"\n","    Inputs: \n","      - `input_size`: an int representing the RNN input size.\n","      - `hidden_size`: an int representing the RNN hidden size.\n","      - `dropout`: a float representing the dropout rate during training. Note\n","          that for 1-layer RNN this has no effect since dropout only applies to\n","          outputs of intermediate layers.\n","    \"\"\"\n","    super(Encoder, self).__init__()\n","    self.rnn = nn.GRU(input_size, hidden_size, num_layers=1, batch_first=True,\n","                      dropout=dropout, bidirectional=False)\n","\n","  def forward(self, inputs, lengths, init_state=None):\n","    \"\"\"\n","    Inputs:\n","      - `inputs`: a 3d-tensor of shape (batch_size, max_seq_length, embed_size)\n","          representing a batch of padded embedded word vectors of source\n","          sentences.\n","      - `lengths`: a 1d-tensor of shape (batch_size,) representing the sequence\n","          lengths of `inputs`.\n","\n","    Returns:\n","      - `outputs`: a 3d-tensor of shape\n","        (batch_size, max_seq_length, hidden_size).\n","      - `finals`: a 3d-tensor of shape (num_layers, batch_size, hidden_size).\n","      Hint: `outputs` and `finals` are both standard GRU outputs. Check:\n","      https://pytorch.org/docs/stable/nn.html#gru\n","    \"\"\"\n","    # Our variable-length inputs are padded to the same length for batching\n","    # Here we \"pack\" them for computational efficiency (see note below)\n","    packed = pack_padded_sequence(inputs, lengths.cpu(), batch_first=True,\n","                                  enforce_sorted=False)\n","    outputs, finals = self.rnn(packed, init_state)\n","    outputs, _ = pad_packed_sequence(outputs, batch_first=True,\n","                                     total_length=MAX_SENT_LENGTH_PLUS_SOS_EOS)\n","    return outputs, finals"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rKiJAkvZGMWl"},"source":["### Decoder"]},{"cell_type":"code","metadata":{"id":"E1Tdf6_283WY","executionInfo":{"status":"ok","timestamp":1637701490212,"user_tz":300,"elapsed":6,"user":{"displayName":"Katherine Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11324848797432579426"}}},"source":["class GeneratorTransferredSampled(nn.Module):\n","  \"\"\"Define standard linear + softmax generation step.\"\"\"\n","  def __init__(self, hidden_size, vocab_size, src_embed, gamma=0.001):\n","    \"\"\"\n","    Inputs:\n","      - `src_embed`: a 2d-tensor of shape (vocab_size, embed_size )\n","    \"\"\"\n","    super(GeneratorTransferredSampled, self).__init__()\n","    self.proj = nn.Linear(hidden_size, vocab_size, bias=True)\n","    self.gamma = gamma\n","    self.logsoftmax = nn.LogSoftmax(dim = 2)\n","    self.src_embed = src_embed\n","\n","  def embedding(self,x):\n","    return torch.matmul(x,self.src_embed.weight)\n","    \n","  def gumbel_softmax(self,logits, eps=1e-20):\n","    U = torch.rand(logits.shape).to(device)\n","    G = -torch.log(-torch.log(U + eps) + eps).to(device)\n","    return self.logsoftmax((logits + G) / self.gamma)\n","\n","  def forward(self, x):\n","    logits = self.proj(x)\n","    prob = self.logsoftmax(logits)\n","    output = self.embedding(prob)\n","    word  = logits.argmax(dim = 2, keepdim = False)\n","\n","    return output, prob, word\n","\n","  def forward_gumbel(self, x):\n","    logits = self.proj(x)\n","    prob = self.gumbel_softmax(logits)\n","    output = self.embedding(prob)\n","    word  = logits.argmax(dim = 2, keepdim = False)\n","\n","    return output, prob, word"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"plJpAyPu86yw","executionInfo":{"status":"ok","timestamp":1637701875583,"user_tz":300,"elapsed":241,"user":{"displayName":"Katherine Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11324848797432579426"}}},"source":["\n","class Decoder(nn.Module):\n","  \"\"\"An RNN decoder + generator with GRU\"\"\"\n","\n","  def __init__(self, input_size, hidden_size, max_len,generator, num_layers = 1, dropout=0.):\n","    \"\"\"\n","      Inputs:\n","        - `input_size` , `hidden_size`, and `dropout` the same as in Encoder.\n","    \"\"\"\n","    super(Decoder, self).__init__()\n","    self.input_size = input_size\n","    self.hidden_size = hidden_size\n","    self.rnn = nn.GRU(input_size, hidden_size, num_layers=num_layers, batch_first=True,\n","                      dropout=dropout, bidirectional=False)\n","    self.generator = generator\n","    self.max_len = max_len\n","    self.dropout_layer = nn.Dropout(p=dropout)\n","    self.rnn_to_pre = nn.Linear(input_size + hidden_size,\n","                                hidden_size, bias=False)\n","\n","  def forward_step(self, prev_embed, hidden):\n","    \"\"\"Helper function for forward below:\n","       Perform a single decoder step (1 word).\n","\n","       Inputs:\n","      - `prev_embed`: a 3d-tensor of shape (batch_size, 1, embed_size = vocab_size)\n","          representing the padded embedded word vectors at this step in training\n","      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size) representing\n","          the current hidden state.\n","\n","      Returns:\n","      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size)\n","          representing the current decoder hidden state.\n","      - `output`: a 3d-tensor of shape (batch_size, max_len, vocab_size)\n","          representing the total generated outputs.\n","    \"\"\"\n","    pre_output, hidden = self.rnn(prev_embed, hidden)\n","    pre_output = torch.cat([prev_embed, pre_output], dim=2)\n","    pre_output = self.dropout_layer(pre_output)\n","    pre_output = self.rnn_to_pre(pre_output)\n","    pre_output = torch.tanh(pre_output)\n","\n","    return hidden, pre_output\n","\n","    ### Your code here!\n","    pre_output, hidden = self.rnn(prev_embed, hidden)\n","    pre_output = torch.cat([prev_embed, pre_output], dim=2)\n","    pre_output = self.dropout_layer(pre_output)\n","    pre_output = self.rnn_to_pre(pre_output)\n","    pre_output = self.pre_activation(pre_output)\n","\n","  def forward(self, input, encoder_finals,max_len, hidden=None):\n","    \"\"\"Unroll the decoder one step at a time.\n","\n","    Inputs:\n","      - `inputs`: a 3d-tensor of shape (batch_size, 1, embed_size)\n","          representing a batch of padded embedded word vectors of SOS . \n","          If size is (batch_size,max_len, embed_size), then it is teacher forcing.\n","      - `encoder_finals`: a 3d-tensor of shape\n","          (num_enc_layers, batch_size, hidden_size) representing the final\n","          encoder hidden states used to initialize the initial decoder hidden\n","          states.\n","      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size) representing\n","          the value to be used to initialize the initial decoder hidden states.\n","          If None, then use `encoder_finals`.\n","      - `max_len`: an int representing the maximum decoding length.\n","      - `style`: TAYLOR_STYLE or DRAKE_STYLE\n","\n","    Returns:\n","      - `hidden`: a 3d-tensor of shape\n","          (num_layers, batch_size, hidden_size) representing the final hidden\n","          state for each element in the batch.\n","      - `outputs`: a 3d-tensor of shape\n","          (batch_size, max_len, hidden_size) representing the raw decoder\n","          outputs (before mapping to a `trg_vocab_size`-dim vector).\n","      - `logits_vectors`: a 3d-tensor of shape\n","          (batch_size, max_len, trg_vocab_size) representing the mapped decoder\n","          outputs.\n","      - `words`: a 3d-tensor of shape\n","          (batch_size, max_len, 1) representing output sentence and\n","          the corresponding word index (can be used for embedding)  \n","    \"\"\"\n","\n","    # Initialize decoder hidden state.\n","    if hidden is None:\n","      hidden = self.init_hidden(encoder_finals)\n","    output_vectors = []\n","    logits_vectors = []\n","    words = []\n","    hidden_states = []\n","    hidden_states.append(hidden[-1][:,None,:])\n","    for i in range(max_len-1) :\n","      hidden, prev_output = self.forward_step(input,hidden)\n","      input, logits, output_word = self.generator.forward(prev_output)\n","      # input, logits, output_word = self.generator(prev_output)\n","\n","      # input = torch.concat([input,torch.full(input.shape,style)], axis = -1)\n","      logits_vectors.append(logits)\n","      output_vectors.append(input)\n","      words.append(output_word)\n","      hidden_states.append(prev_output)\n","\n","    outputs = torch.cat(output_vectors, dim =1)\n","    logits_vectors = torch.cat(logits_vectors,dim = 1)\n","    words = torch.cat(words, axis = -1)\n","    hidden_states = torch.cat(hidden_states, axis = 1)\n","    return hidden, outputs , logits_vectors, words, hidden_states\n","\n","  def forward_teacher(self, input, encoder_finals, max_len=None, hidden=None):\n","    \"\"\"Unroll the decoder one step at a time.\n","\n","    Inputs:\n","      - `inputs`: a 3d-tensor of shape (batch_size,max_len, embed_size)\n","          representing a batch of padded embedded word vectors of original \n","          sentence and acts as  teacher forcing.\n","\n","    Returns:\n","      - `hidden`: a 3d-tensor of shape\n","          (num_layers, batch_size, hidden_size) representing the final hidden\n","          state for each element in the batch.\n","      - `outputs`: a 3d-tensor of shape\n","          (batch_size, max_len, hidden_size) representing the raw decoder\n","          outputs (before mapping to a `trg_vocab_size`-dim vector).\n","      - `logits_vectors`: a 3d-tensor of shape\n","          (batch_size, max_len, trg_vocab_size) representing the mapped decoder\n","          outputs each represents the probability? \n","      - `words`: a 3d-tensor of shape\n","          (batch_size, max_len, 1) representing output sentence and\n","          the corresponding word index (can be used for embedding)      \n","    \"\"\"\n","\n","    # Initialize decoder hidden state.\n","    if max_len is None:\n","      max_len = input.shape[1]\n","    if hidden is None:\n","      hidden = self.init_hidden(encoder_finals)\n","    output_vectors = []\n","    logits_vectors = []\n","    words = []\n","    hidden_states = []\n","    hidden_states.append(hidden[-1][:,None,:])\n","    for i in range(max_len):\n","      hidden, prev_output = self.forward_step(input[:,i:i+1,:],hidden)\n","      output, logits, output_word = self.generator(prev_output)\n","      \n","      logits_vectors.append(logits)\n","      output_vectors.append(output)\n","      words.append(output_word)\n","      hidden_states.append(prev_output)\n","\n","    outputs = torch.cat(output_vectors, dim =1)\n","    logits_vectors = torch.cat(logits_vectors,dim = 1)\n","    words = torch.cat(words, axis = -1)\n","    hidden_states = torch.cat(hidden_states, axis = 1)\n","    return hidden, outputs , logits_vectors, words, hidden_states\n","\n","  def init_hidden(self, encoder_finals):\n","    \"\"\"Use encoder final hidden state to initialize decoder's first hidden\n","       state.\n","\n","       Input: `encoder_finals` is same as in forward()\n","\n","       Returns: \n","         - `decoder_init_hiddens`: a 3d-tensor of shape \n","              (num_layers, batch_size, hidden_size) representing the initial\n","              hidden state of the decoder for each element in the batch \n","    \"\"\"\n","    decoder_init_hiddens = torch.tanh(encoder_finals)\n","    return decoder_init_hiddens"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SlnB42ru9uBI","executionInfo":{"status":"ok","timestamp":1637702544806,"user_tz":300,"elapsed":324853,"user":{"displayName":"Katherine Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11324848797432579426"}},"outputId":"4eef8b20-1c6d-44d6-d6d0-83da75d3fcb1"},"source":["### Work in progress\n","class EncoderDecoder(nn.Module):\n","  def __init__(self, encoder, decoder, line_embed, generator):\n","    super(EncoderDecoder, self).__init__()\n","\n","    self.encoder = encoder\n","    self.decoder = decoder\n","    self.line_embed = line_embed\n","    self.generator = generator\n","\n","  def forward(self, lines, line_lens):\n","    encoder_hiddens, encoder_finals = self.encode(lines, line_lens)\n","    del encoder_hiddens\n","    return self.reconstruct(encoder_finals, lines[:, :-1]), self.decode(encoder_finals)\n","    # decode_orig = self.decode(h0_orig)\n","    # return rec_orig, decode_orig\n","\n","  def encode(self, lines, line_lens):\n","    return self.encoder(self.line_embed(lines), line_lens)\n","    \n","  def reconstruct(self, h0, lines):\n","    original = self.line_embed(lines)\n","    return self.decoder.forward_teacher(original,h0)\n","\n","  def decode(self, h0):\n","    target = self.line_embed(torch.tensor([SOS_INDEX]).repeat(h0.size()[1],1).to(device))\n","    return self.decoder.forward(target,h0,max_len)\n","\n","epochs = 3\n","lr = 1e-3\n","batch_size = 32\n","print_every = 100\n","max_len = dataset.max_seq_length\n","vocab_size = len(vocab)\n","embed_size = 256\n","hidden_size = 256\n","dropout = 0.2\n","gamma = 0.001\n","\n","train_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","valid_loader = data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n","\n","line_embed = nn.Embedding(vocab_size, embed_size)\n","encoder = Encoder(embed_size,hidden_size)\n","generator = GeneratorTransferredSampled(hidden_size,vocab_size, line_embed, gamma = gamma)\n","decoder = Decoder(embed_size, hidden_size, max_len=vocab_size, generator = generator,dropout=dropout)\n","model = EncoderDecoder(encoder, decoder, line_embed, generator).to(device)\n","optimizer_model = torch.optim.Adam(model.parameters(), lr=lr)\n","rec_loss = nn.NLLLoss(reduction=\"mean\",ignore_index = PAD_INDEX)\n","\n","for epoch in range(epochs):\n","  epoch_rec_loss = 0\n","  epoch_tokens = 0\n","  model.train()\n","  for i,(taylor_lines, taylor_len, drake_lines, drake_len) in enumerate(train_loader):\n","    lines = torch.cat((taylor_lines, drake_lines), 0).to(device)    \n","    line_lens = torch.cat((taylor_len, drake_len), 0).to(device)\n","\n","    # Train model\n","    rec_orig,dec_orig  = model(lines, line_lens)    \n","    loss_rec = rec_loss(input=rec_orig[2].permute(0,2,1), target=lines[:, 1:])\n","\n","    optimizer_model.zero_grad()\n","    loss_rec.backward()\n","    optimizer_model.step()\n","    \n","    epoch_rec_loss += loss_rec.item() * line_lens.sum().item()\n","    epoch_tokens += line_lens.sum().item()\n","\n","  print(\"Finished Training Epoch \", epoch)\n","  print(\"Training PPL\", np.exp(epoch_rec_loss / float(epoch_tokens)))\n","  val_loss = 0\n","  val_tokens = 0\n","\n","  for i,(taylor_lines, taylor_len, drake_lines, drake_len) in enumerate(valid_loader):\n","    lines = torch.cat((taylor_lines, drake_lines), 0).to(device)    \n","    line_lens = torch.cat((taylor_len, drake_len), 0).to(device)\n","\n","    rec_orig,dec_orig  = model(lines, line_lens)    \n","    loss_rec = rec_loss(input=rec_orig[2].permute(0,2,1), target=lines[:, 1:])\n","\n","    val_loss += loss_rec.item() * line_lens.sum().item()\n","    val_tokens += line_lens.sum().item()\n","  \n","  print(\"Valid PPL\", np.exp(val_loss / float(val_tokens)))"],"execution_count":17,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n"]},{"output_type":"stream","name":"stdout","text":["Finished Training Epoch  0\n","Training PPL 221.76302979114053\n","Valid PPL 112.68749254539021\n","Finished Training Epoch  1\n","Training PPL 75.18844053075715\n","Valid PPL 70.16073769462727\n","Finished Training Epoch  2\n","Training PPL 47.76703979213335\n","Valid PPL 54.0301458856892\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rcD2PiZWro7z","executionInfo":{"status":"ok","timestamp":1637702617076,"user_tz":300,"elapsed":368,"user":{"displayName":"Katherine Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11324848797432579426"}},"outputId":"d3ab4fc1-be3a-40e2-b7e6-aa7cbb8ddefb"},"source":["rec_orig[3]"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 8366,  8089,  4939, 12474,  7582,  1019,  2897,  9061,  2856,  8462,\n","          8366,  8089,  4939, 11497,  9061,   704,  8366, 10812,  1019, 12503,\n","          8089,  8089,  6419,  6419,  8089,  5860,  9061,     3,     3,     3,\n","             3],\n","        [ 8131,  4173,  4891,  7135,  8462,  8089,  8089,  4939,  7135,  7135,\n","          8462, 12487,  8462,  8366,  8131,  5860,  8447,  7135,   860,  8697,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3],\n","        [  609,  9061,  3277,  8447,  5370,  8447, 11524,  8462,   609, 11634,\n","         12474, 11073,  8089,  4830,  8089,  4939,  1019,  8462,     3,  8089,\n","          8089,  4939,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3],\n","        [ 8366,  6310,  9946,  9946,  8462,  8366,  4479,   811,  8366,  8366,\n","          8366,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3],\n","        [ 8756,  4891,  9061,  9061,   860,   704,  8366,  8462,  8089,  8089,\n","          4939,  8756, 12474, 11073,  8131,  8697,  8366,  8131,     3,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3],\n","        [ 7135,  7135,  4479,  7135,  8462,  8462,  8366,  7135,  7135,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3],\n","        [ 8131,  8131,  8131,  4173,  1821,  1732,  8462,   860, 12474, 11073,\n","         12503,  8447, 11232,     3,     3,     3,     3,     3,     3,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3],\n","        [ 8366,   649,  9286,  1732,  2856,  8462,  8089,  8089,  4939,   760,\n","          7576,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3],\n","        [  860,  8447, 11524,  8710,  8447,   860,  8697,  8462,  8089,  8089,\n","          4939,  8447, 12503,  7135,     3,     3,     3,     3,     3,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3],\n","        [ 6419,  9061,  9061,  9092,  9061,  9061,  8443,  1019,  6419,  8462,\n","          8089,  4939,  8447,   704,  5370,     3,     3,     3,     3,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3],\n","        [  609,   609,   609, 11524,  8462,   609, 11634,  8447,   609,  9036,\n","             3,  8089,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3],\n","        [ 8089,  4939, 10408,  8089,  8089,  4939,  1821,  1019,  8447,  8447,\n","          6139,  8462,  8089,  4939,  8447,  5985, 12503,   860,  6139,  8089,\n","          1019,   860,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3],\n","        [ 8447,  8447, 11524,  5370,  8447,  9440,  8462,  8089,  4891,  8089,\n","          4939,  1821,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3],\n","        [ 8089,  8954,   609,  8447, 11524,  8366,  8462,  8089,  8089,  8954,\n","          8089,  8089,  8954,  6710,   609,   609,  4830,     3,     3,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3],\n","        [ 8089,  4939,   760,  9061, 12503,  8131, 12503, 11524,  8462,  8089,\n","          4939,  8131,  8089,  4939,  8131,  9061,  4939,  9061,  8462,  9061,\n","         11634,  9061,  9061,  9061,   704,     3,     3,     3,     3,     3,\n","             3],\n","        [ 6419, 11634,  9061, 12503,  8462,  8131, 12474,  8131,  8131,  8131,\n","          4173,  8462,  9061,  8462,   867, 10408,  8954, 11479,  8089,  4871,\n","         10028,  9061,   704,     3,     3,     3,     3,     3,     3,     3,\n","             3],\n","        [  609,  6710,  6419, 11634,  8089,  1732,  8131,  8131,  9036,   609,\n","          8089,  9440,  8462,  8089,  8954,   609,   214,   609,  4852,  8089,\n","          5860,  5860, 12474, 11073,  8089,     3,     3,     3,     3,     3,\n","             3],\n","        [ 8089,  4939, 11497,  7135,  8131,  8131,  8366,  8089,  8366,  6267,\n","          8462,  8089,  8089,  4939,  1821,  8366,  8089,  8089,  8089,  4939,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3]], device='cuda:0')"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PPPXJVFcqG2X","executionInfo":{"status":"ok","timestamp":1637702579389,"user_tz":300,"elapsed":642,"user":{"displayName":"Katherine Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11324848797432579426"}},"outputId":"0fc08c30-7b5c-4a57-b897-aca66cfea6f8"},"source":["def lookup_words(x, vocab):\n","  return [vocab[i] for i in x]\n","\n","idx=5\n","print(lookup_words(lines[idx], vocab))\n","print(lookup_words(rec_orig[3][idx], vocab))\n","print(lookup_words(dec_orig[3][idx], vocab))\n","\n","print(len(lookup_words(rec_orig[3][idx], vocab)))\n","print(len(lookup_words(dec_orig[3][idx], vocab)))"],"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["['<s>', 'rihanna', '-', 'close', 'to', 'you', ',', 'ariana', 'grande', '-', 'everyday', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n","['to', 'to', '``', 'to', ',', ',', 'and', 'to', 'to', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n","['got', 'my', 'whole', 'whole', 'only', 'no', 'many', 'only', 'nice', 'sparks', 'only', 'whole', 'only', 'my', 'young', 'only', 'many', 'fuck', 'young', 'long', 'the', 'only', 'whole', 'many', 'only', 'enchanted', 'many', 'whole', 'nice', 'my', 'nice']\n","31\n","31\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LO47V3RWEs6J","executionInfo":{"status":"ok","timestamp":1637644597289,"user_tz":300,"elapsed":1304,"user":{"displayName":"Hammaad Adam","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03287553409115045279"}},"outputId":"f72bbef8-2c62-4ed8-c885-a61f06b605cd"},"source":["def lookup_words(x, vocab):\n","  return [vocab[i] for i in x]\n","\n","idx=5\n","print(lookup_words(lines[idx], vocab))\n","print(lookup_words(rec_orig[3][idx], vocab))\n","print(lookup_words(dec_orig[3][idx], vocab))\n","\n","print(len(lookup_words(rec_orig[3][idx], vocab)))\n","print(len(lookup_words(dec_orig[3][idx], vocab)))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['<s>', 'i', \"'d\", 'hold', 'you', 'as', 'the', 'water', 'rushes', 'in', ',', 'if', 'i', 'could', 'dance', 'with', 'you', 'again', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n","['i', \"'m\", 'be', 'you', 'the', 'you', 'way', 'i', ',', 'the', 'i', 'you', \"'m\", 'be', ',', 'you', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n","['i', 'party', 'championships', 'championships', 'magnet', 'truest', 'championships', 'case', 'biggie', 'contracts', 'this', 'mission', '11/9/2018', 'remote', 'drop-top', 'worn', 'witches', 'championships', 'avoidin', 'magnet', 'podrug', 'bleach', 'velcro', 'sparkly', 'mission', 'championships', 'championships', 'reclinin', 'vsekh', 'championships', 'forgiveness', 'toughen']\n","31\n","32\n"]}]},{"cell_type":"markdown","metadata":{"id":"wg_FbKl-T9xd"},"source":["### Classifier"]},{"cell_type":"code","metadata":{"id":"rjDXvknvT_PM"},"source":["class LSTMClassifier(nn.Module):\n","  def __init__(self, embed_size, hidden_size, vocab_size, LSTMlayers=1, dropout = 0.2):\n","    super(LSTMClassifier, self).__init__()\n","    self.embed = nn.Linear(vocab_size, embed_size)\n","    self.lstm = nn.LSTM(input_size=embed_size, hidden_size=hidden_size, num_layers=LSTMlayers, batch_first=True)\n","    self.dropout = nn.Dropout(dropout)\n","    self.fc1 =  nn.Linear(in_features=hidden_size, out_features=hidden_size+1)\n","    self.fc2 = nn.Linear(hidden_size+1, 1)\n","\n","  def forward(self, seq):\n","    seq = self.embed(seq)\n","    output, (hidden,cell) = self.lstm(seq)\n","    output = self.dropout(output)\n","    output = self.fc1(output[:,-1])\n","    output = F.relu(output)\n","    output = self.fc2(output)\n","    return output\n","    return output.reshape(output.size(1), -1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yorUFRiIAmTX"},"source":["class LSTMDiscriminator(nn.Module):\n","  def __init__(self, input_size, hidden_size, LSTMlayers=1, dropout = 0.2):\n","    super(LSTMDiscriminator, self).__init__()\n","    self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=LSTMlayers, batch_first=True)\n","    self.dropout = nn.Dropout(dropout)\n","    self.fc1 =  nn.Linear(in_features=hidden_size, out_features=hidden_size+1)\n","    self.fc2 = nn.Linear(hidden_size+1, 1)\n","\n","  def forward(self, seq):\n","    output, (hidden,cell) = self.lstm(seq)\n","    output = self.dropout(output)\n","    output = self.fc1(output[:,-1])\n","    output = F.relu(output)\n","    output = self.fc2(output)\n","    return output\n","    return output.reshape(output.size(1), -1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_xuubDG0F5i3"},"source":["### Training"]},{"cell_type":"code","metadata":{"id":"yjs1mzrQEyef"},"source":["class TSTModel(nn.Module):\n","  def __init__(self, max_len, vocab_size, embed_size, hidden_size_z, hidden_size_y, line_embed, encoder, generator, decoder, classifier):\n","    super(TSTModel, self).__init__()\n","\n","    self.hidden_size = hidden_size_y + hidden_size_z\n","\n","    self.encoder = encoder\n","    self.generator = generator\n","    self.decoder = decoder\n","    self.classifier = classifier\n","\n","    self.line_embed = line_embed\n","    self.y_embed_enc = nn.Embedding(2,hidden_size_y)\n","    self.y_embed_gen = nn.Embedding(2,hidden_size_y)\n","\n","    self.max_len = max_len\n","    self.vocab_size = vocab_size\n","    self.embed_size = embed_size\n","    self.hidden_size_z = hidden_size_z\n","    self.hidden_size_y = hidden_size_y\n","\n","  def forward(self, lines, line_lens, labels):\n","    encoded_lines = self.encode(lines, line_lens, labels)\n","    z = encoded_lines[1][-1][:,self.hidden_size_y:]\n","\n","    h0_orig = torch.cat((self.y_embed_gen(labels),z), 1)[None,:]\n","    h0_tsf = torch.cat((self.y_embed_gen(1-labels),z), 1)[None,:]\n","\n","    # Decode back into original form for reconstruction\n","    rec_orig = self.reconstruct(h0_orig, lines[:, :-1])\n","\n","    # Decode into original and transferred forms for classification\n"," \n","    decode_orig = self.decode(h0_orig)\n","    decode_tsf = self.decode(h0_tsf)\n","    \n","    half = int(lines.size(0) / 2)\n","\n","    discrim1_input = torch.cat((rec_orig[4][:half], decode_tsf[4][half:]))\n","    discrim0_input = torch.cat((rec_orig[4][half:], decode_tsf[4][:half]))\n","\n","    classifier_lines = torch.cat((decode_orig[2], decode_tsf[2], F.one_hot(lines[:,1:], self.vocab_size).to(torch.float)), 0)\n","    pred_class = self.classifier(classifier_lines)\n","\n","    # return rec_orig, decode_orig\n","    return rec_orig, pred_class, decode_orig, decode_tsf, discrim0_input, discrim1_input \n","\n","  def encode(self, lines, line_lens, labels):\n","    init_state = torch.cat((self.y_embed_enc(labels), torch.zeros((len(lines),self.hidden_size_z), device=device)), 1)[None,:].to(device)\n","    return self.encoder(self.line_embed(lines), line_lens, init_state)\n","\n","  def reconstruct(self, h0, lines):\n","    original = self.line_embed(lines)\n","    return self.decoder.forward_teacher(original,h0)\n","\n","  def decode(self, h0):\n","    target = self.line_embed(torch.tensor([SOS_INDEX]).repeat(h0.size()[1],1).to(device))\n","    return self.decoder.forward(target,h0,self.max_len)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O8jaGcIiNQUi","executionInfo":{"status":"ok","timestamp":1637701050650,"user_tz":300,"elapsed":683952,"user":{"displayName":"Hammaad Adam","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03287553409115045279"}},"outputId":"fff0f0bb-c9e7-48cb-9e2e-b4677e6501b7"},"source":["epochs = 4\n","class_epochs = 0\n","lr = 1e-3\n","batch_size = 32\n","print_every = 100\n","\n","max_len = dataset.max_seq_length\n","vocab_size = len(vocab)\n","embed_size = 256\n","hidden_size_z = 256\n","hidden_size_y = 128\n","hidden_size = hidden_size_z + hidden_size_y\n","dropout = 0.2\n","gamma = 0.001\n","\n","TAYLOR_STYLE=1 # for information only, don't change\n","DRAKE_STYLE=0  # for information only, don't change\n","train_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","valid_loader = data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n","\n","line_embed = nn.Embedding(vocab_size, embed_size)\n","encoder = Encoder(embed_size,hidden_size)\n","generator = GeneratorTransferredSampled(hidden_size,vocab_size, line_embed, gamma = gamma)\n","decoder = Decoder(embed_size, hidden_size, max_len=vocab_size, generator = generator,dropout=dropout)\n","classifier = LSTMClassifier(embed_size, hidden_size_z, vocab_size, dropout =0.2)\n","\n","discriminator0 = LSTMDiscriminator(hidden_size, hidden_size).to(device)\n","discriminator1 = LSTMDiscriminator(hidden_size, hidden_size).to(device)\n","\n","model = TSTModel(dataset.max_seq_length, vocab_size, embed_size, hidden_size_z, hidden_size_y, line_embed, encoder, generator, decoder, classifier).to(device)\n","optimizer_model = torch.optim.Adam(model.parameters(), lr=lr) \n","optimizer_discr = torch.optim.Adam(list(discriminator0.parameters()) + list(discriminator1.parameters()), lr=lr) \n","\n","# rec_loss = nn.CrossEntropyLoss(reduction=\"mean\",ignore_index = PAD_INDEX)\n","rec_loss = nn.NLLLoss(reduction=\"mean\",ignore_index = PAD_INDEX)\n","class_loss = nn.BCEWithLogitsLoss(reduction=\"mean\")\n","discr_loss = nn.BCEWithLogitsLoss(reduction=\"mean\")\n","\n","epoch_losses = []\n","for epoch in range(epochs):\n","  epoch_loss = 0\n","  epoch_class_loss = 0\n","  epoch_rec_loss = 0\n","  epoch_adv_loss = 0\n","  epoch_loss_d = 0\n","  epoch_tokens = 0\n","  model.train()\n","  for i,(taylor_lines, taylor_len, drake_lines, drake_len) in enumerate(train_loader):\n","    lines = torch.cat((taylor_lines, drake_lines), 0).to(device)    \n","    line_lens = torch.cat((taylor_len, drake_len), 0).to(device)\n","    labels = torch.cat((torch.ones(size=(len(taylor_lines),), dtype=torch.int32),torch.zeros(size=(len(drake_lines),),dtype=torch.int32))).to(device)\n","    classifier_labels = torch.cat((labels,1-labels, labels)).unsqueeze(1)\n","    \n","    fake_labels = torch.cat((torch.zeros(size=(len(taylor_lines),), dtype=torch.int32),torch.ones(size=(len(taylor_lines),),dtype=torch.int32))).to(device)\n","    fake_labels = fake_labels.unsqueeze(1)\n","\n","    # Train discriminator\n","\n","    # rec_orig, pred_class, decode_orig, decode_tsf, pred_fake0, pred_fake1 = model(lines, line_lens, labels)\n","    # pred_fake0 = discriminator0(pred_fake0)\n","    # pred_fake1 = discriminator1(pred_fake1)\n","    # loss_d0 = discr_loss(pred_fake0, fake_labels.to(torch.float))\n","    # loss_d1 = discr_loss(pred_fake1, fake_labels.to(torch.float))\n","    # loss_d = loss_d0 + loss_d1\n","\n","    # optimizer_discr.zero_grad()\n","    # loss_d.backward()\n","    # optimizer_discr.step()\n","\n","    # Train model\n","\n","    rec_orig, pred_class, decode_orig, decode_tsf, pred_fake0, pred_fake1 = model(lines, line_lens, labels)\n","\n","    # pred_fake0 = discriminator0(pred_fake0)\n","    # pred_fake1 = discriminator1(pred_fake1)\n","\n","    loss_rec = rec_loss(input=rec_orig[2].permute(0,2,1), target=lines[:, 1:])\n","    loss_class = class_loss(pred_class, classifier_labels.to(torch.float))\n","    # loss_adv0 = class_loss(pred_fake0[len(drake_lines):], fake_labels[len(drake_lines):].to(torch.float))\n","    # loss_avd1 = class_loss(pred_fake1[len(taylor_lines):], fake_labels[len(taylor_lines):].to(torch.float))\n","\n","    loss = loss_rec + loss_class\n","    # loss = loss_rec + loss_class - (loss_adv0 + loss_avd1)\n","    # loss = loss_rec\n","\n","    optimizer_model.zero_grad()\n","    loss.backward()\n","    optimizer_model.step()\n","    \n","    epoch_loss += loss.item()\n","    # epoch_loss_d += loss_d.item()\n","    epoch_class_loss += loss_class.item()\n","    epoch_rec_loss += loss_rec.item() * line_lens.sum().item()\n","    epoch_tokens += line_lens.sum().item()\n","    # epoch_adv_loss += (loss_adv0.item() + loss_avd1.item())\n","\n","    if model.training and i % print_every == 0:\n","      print(\"Epoch Step: %d Loss: %f\" % (i, loss.item()))\n","  \n","  epoch_losses.append(epoch_loss)\n","  print(\"Finished Training Epoch \", epoch)\n","  print(\"Epoch Loss\", epoch_loss)\n","  print(\"Classification Loss\", epoch_class_loss)\n","  print(\"Training PPL\", np.exp(epoch_rec_loss / float(epoch_tokens)))\n","  print(\"Adversarial Loss\", epoch_adv_loss)\n","  print(\"Discriminator Loss\", epoch_loss_d)\n","  val_loss = 0\n","  val_tokens = 0\n","  val_class_loss = 0\n","  correct_pred = 0\n","  correct_pred_all = 0\n","\n","  for i,(taylor_lines, taylor_len, drake_lines, drake_len) in enumerate(valid_loader):\n","    lines = torch.cat((taylor_lines, drake_lines), 0).to(device)    \n","    line_lens = torch.cat((taylor_len, drake_len), 0).to(device)\n","    labels = torch.cat((torch.ones(size=(len(taylor_lines),), dtype=torch.int32),torch.zeros(size=(len(drake_lines),),dtype=torch.int32))).to(device)\n","    classifier_labels = torch.cat((labels,1-labels, labels)).unsqueeze(1)\n","    \n","    fake_labels = torch.cat((torch.zeros(size=(len(taylor_lines),), dtype=torch.int32),torch.ones(size=(len(taylor_lines),),dtype=torch.int32))).to(device)\n","    fake_labels = fake_labels.unsqueeze(1)\n","\n","    rec_orig, pred_class, decode_orig, decode_tsf, pred_fake0, pred_fake1 = model(lines, line_lens, labels)\n","    loss_rec = rec_loss(input=rec_orig[2].permute(0,2,1), target=lines[:, 1:])\n","    loss_class = class_loss(pred_class, classifier_labels.to(torch.float))\n","\n","    val_loss += loss_rec.item() * line_lens.sum().item()\n","    val_tokens += line_lens.sum().item()\n","    # val_class_loss += loss_class.item()*classifier_labels.size(0)\n","\n","    correct_pred += torch.sum(1*(1*(torch.sigmoid(pred_class[-len(lines):])>=0.5) == labels.unsqueeze(1)))\n","    correct_pred_all += torch.sum(1*(1*(torch.sigmoid(pred_class)>=0.5) == classifier_labels))\n","  \n","  print(\"Valid PPL\", np.exp(val_loss / float(val_tokens)))\n","  print(\"Valid Classification Loss\", val_class_loss / float(len(valid_loader)*6))\n","  print(\"Classification Accuracy on Generated\", correct_pred / (2.*len(valid_dataset)))\n","  print(\"Classification Accuracy on All\", correct_pred_all / (3.*2.*len(valid_dataset)))"],"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n"]},{"output_type":"stream","name":"stdout","text":["Epoch Step: 0 Loss: 10.175980\n","Epoch Step: 100 Loss: 5.812898\n","Epoch Step: 200 Loss: 5.229771\n","Finished Training Epoch  0\n","Epoch Loss 1268.1461277008057\n","Classification Loss 146.50422883033752\n","Training PPL 194.30359768013915\n","Adversarial Loss 0\n","Discriminator Loss 0\n","Valid PPL 102.0429002991721\n","Valid Classification Loss 21.186714926073627\n","Epoch Step: 0 Loss: 5.206198\n","Epoch Step: 100 Loss: 4.831620\n","Epoch Step: 200 Loss: 4.839772\n","Finished Training Epoch  1\n","Epoch Loss 1033.8395166397095\n","Classification Loss 146.94581472873688\n","Training PPL 64.15095521318808\n","Adversarial Loss 0\n","Discriminator Loss 0\n","Valid PPL 56.711391119523014\n","Valid Classification Loss 21.753116400011123\n","Epoch Step: 0 Loss: 4.339231\n","Epoch Step: 100 Loss: 4.438923\n","Epoch Step: 200 Loss: 4.322330\n","Finished Training Epoch  2\n","Epoch Loss 922.5583679676056\n","Classification Loss 147.4784973859787\n","Training PPL 38.08967630460905\n","Adversarial Loss 0\n","Discriminator Loss 0\n","Valid PPL 42.72483216231617\n","Valid Classification Loss 21.139044161765806\n","Epoch Step: 0 Loss: 3.954730\n","Epoch Step: 100 Loss: 3.789691\n","Epoch Step: 200 Loss: 3.768595\n","Finished Training Epoch  3\n","Epoch Loss 836.1452085971832\n","Classification Loss 146.11313849687576\n","Training PPL 25.542056164655303\n","Adversarial Loss 0\n","Discriminator Loss 0\n","Valid PPL 33.785504059102585\n","Valid Classification Loss 21.758964444360426\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J9ATTK09gakn","executionInfo":{"status":"ok","timestamp":1637647438591,"user_tz":300,"elapsed":19342,"user":{"displayName":"Hammaad Adam","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03287553409115045279"}},"outputId":"2d552042-b6bd-4621-a1e7-d4ef461a788e"},"source":["correct_pred = 0\n","correct_pred_all = 0\n","model.eval()\n","with torch.no_grad():\n","  for i,(taylor_lines, taylor_len, drake_lines, drake_len) in enumerate(valid_loader):\n","    lines = torch.cat((taylor_lines, drake_lines), 0).to(device)    \n","    line_lens = torch.cat((taylor_len, drake_len), 0).to(device)\n","    labels = torch.cat((torch.ones(size=(len(taylor_lines),), dtype=torch.int32),torch.zeros(size=(len(drake_lines),),dtype=torch.int32))).to(device)\n","    classifier_labels = torch.cat((labels,1-labels, labels)).unsqueeze(1)\n","\n","    rec_orig,pred_class, decode_orig,decode_tsf,d0,d1 =  model(lines, line_lens, labels)\n","    correct_pred += torch.sum(1*(1*(torch.sigmoid(pred_class[-len(lines):])>=0.5) == labels.unsqueeze(1)))\n","    correct_pred_all += torch.sum(1*(1*(torch.sigmoid(pred_class)>=0.5) == classifier_labels))\n","  print(correct_pred / (2.*len(valid_dataset)))\n","  print(correct_pred_all / (3.*2.*len(valid_dataset)))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(0.8684, device='cuda:0')\n","tensor(0.6228, device='cuda:0')\n"]}]},{"cell_type":"code","metadata":{"id":"A6LmSVOGr4Ha"},"source":["def lookup_words(x, vocab):\n","  return [vocab[i] for i in x]\n","\n","with torch.no_grad():\n","  for i,(taylor_lines, taylor_len, drake_lines, drake_len) in enumerate(train_loader):\n","    lines = torch.cat((taylor_lines, drake_lines), 0).to(device)    \n","    line_lens = torch.cat((taylor_len, drake_len), 0).to(device)\n","    labels = torch.cat((torch.ones(size=(len(taylor_lines),), dtype=torch.int32),torch.zeros(size=(len(drake_lines),),dtype=torch.int32))).to(device)\n","    # classifier_labels = torch.cat((labels,1-labels, labels)).unsqueeze(1)\n","\n","    # rec_orig,pred_class,decode_orig,decode_tsf,_,_ =  model(lines, line_lens, labels)\n","\n","    rec_orig,pred_class, dec_orig,dec_tsf,d0,d1 =  model(lines, line_lens, labels)\n","    \n","    break\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O_sa15FklmJj","executionInfo":{"status":"ok","timestamp":1637701085120,"user_tz":300,"elapsed":163,"user":{"displayName":"Hammaad Adam","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03287553409115045279"}},"outputId":"7f9e7f53-bc17-44a7-c140-2dc115b148a8"},"source":["idx=32\n","print(lookup_words(lines[idx], vocab))\n","print(lookup_words(rec_orig[3][idx], vocab))\n","print(lookup_words(dec_orig[3][idx], vocab))\n","print(lookup_words(dec_tsf[3][idx], vocab))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['<s>', 'i', \"'m\", 'blem', 'for', 'real', ',', 'i', 'might', 'just', 'say', 'how', 'i', 'feel', ',', 'do', \"n't\", 'switch', 'on', 'me', ',', 'i', 'got', 'big', 'plans', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n","['i', \"'m\", 'just', 'for', 'real', ',', 'i', 'do', 'do', 'do', 'i', 'i', \"'m\", ',', 'do', \"n't\", 'know', 'up', 'me', ',', 'i', 'do', 'ta', 'old', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n","['i', 'swipes', 'swipes', 'swipes', 'swipes', 'swipes', 'swipes', 'swipes', 'swipes', 'swipes', 'swipes', 'swipes', 'swipes', 'swipes', 'swipes', 'swipes', 'swipes', 'swipes', 'swipes', 'swipes', 'swipes', 'swipes', 'swipes', 'swipes', 'swipes', 'swipes', 'swipes', 'swipes', 'swipes', 'swipes', 'swipes']\n","['i', 'swipes', 'swipes', 'swipes', 'swipes', 'swipes', 'swipes', 'swipes', 'swipes', 'swipes', 'swipes', 'swipes', 'swipes', 'swipes', 'swipes', 'swipes', 'swipes', 'swipes', 'swipes', 'swipes', 'swipes', 'swipes', 'swipes', 'swipes', 'swipes', 'swipes', 'swipes', 'swipes', 'swipes', 'swipes', 'swipes']\n"]}]}]}