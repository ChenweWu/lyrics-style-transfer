{"cells":[{"cell_type":"markdown","metadata":{"id":"m7FQaWQbojAb"},"source":["### Import and Setup"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6072,"status":"ok","timestamp":1637904977561,"user":{"displayName":"Katherine Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11324848797432579426"},"user_tz":300},"id":"Wyz5JwHDopli","outputId":"d87cec9d-d61c-4080-fee0-97f099b30ac8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting unidecode\n","  Downloading Unidecode-1.3.2-py3-none-any.whl (235 kB)\n","\u001b[K     |████████████████████████████████| 235 kB 5.5 MB/s \n","\u001b[?25hInstalling collected packages: unidecode\n","Successfully installed unidecode-1.3.2\n"]}],"source":["!pip install unidecode"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":105668,"status":"ok","timestamp":1637905083225,"user":{"displayName":"Katherine Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11324848797432579426"},"user_tz":300},"id":"T8q-CET4os1p","outputId":"6e2f9021-5381-4989-bf90-39dc79169b2d"},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","Mounted at /content/drive\n"]}],"source":["import json\n","import re\n","from unidecode import unidecode\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","import nltk\n","nltk.download('punkt')\n","from collections import Counter\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":5979,"status":"ok","timestamp":1637905089195,"user":{"displayName":"Katherine Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11324848797432579426"},"user_tz":300},"id":"0ELBqq8kpAZS"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","assert device == \"cuda\"  \n","\n","seed = 42\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)"]},{"cell_type":"markdown","metadata":{"id":"S3i2hvtYoo30"},"source":["### Data"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":28174,"status":"ok","timestamp":1637906228209,"user":{"displayName":"Katherine Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11324848797432579426"},"user_tz":300},"id":"OGzVRiwZowGv"},"outputs":[],"source":["f = open('/content/drive/Shareddrives/MIT NLP 8.864/Data/drake.json')\n","drake = json.load(f)\n","f.close()\n","\n","f = open('/content/drive/Shareddrives/MIT NLP 8.864/Data/tswift.json')\n","taylor = json.load(f)\n","f.close()\n","\n","drake = [drake['songs'][i]['lyrics'] for i in range(len(drake['songs']))]\n","taylor = [taylor['songs'][i]['lyrics'] for i in range(len(taylor['songs']))]\n","\n","taylor_lyrics = [re.sub('\\u2005', ' ', re.sub(r'[\\(\\[].*?[\\)\\]]', '', taylor[i])).split('\\n') for i in range(len(taylor))]\n","taylor_lyrics = [[unidecode(i) for i in taylor_lyrics[j]] for j in range(len(taylor_lyrics))]\n","taylor_lyrics = [[re.sub('\\d+EmbedShare URLCopyEmbedCopy', '', i) for i in taylor_lyrics[j]] for j in range(len(taylor_lyrics))]\n","taylor_lyrics = [[re.sub('\\d+.EmbedShare URLCopyEmbedCopy', '', i) for i in taylor_lyrics[j]] for j in range(len(taylor_lyrics))]\n","taylor_lyrics = [[re.sub('EmbedShare URLCopyEmbedCopy', '', i) for i in taylor_lyrics[j]] for j in range(len(taylor_lyrics))]\n","taylor_lyrics = [[i for i in taylor_lyrics[j] if i != ''] for j in range(len(taylor_lyrics))]\n","\n","drake_lyrics = [re.sub('\\u2005', ' ', re.sub(r'[\\(\\[].*?[\\)\\]]', '', drake[i])).split('\\n') for i in range(len(drake))]\n","drake_lyrics = [[unidecode(i) for i in drake_lyrics[j]] for j in range(len(drake_lyrics))]\n","drake_lyrics = [[re.sub('\\d+EmbedShare URLCopyEmbedCopy', '', i) for i in drake_lyrics[j]] for j in range(len(drake_lyrics))]\n","drake_lyrics = [[re.sub('\\d+.EmbedShare URLCopyEmbedCopy', '', i) for i in drake_lyrics[j]] for j in range(len(drake_lyrics))]\n","drake_lyrics = [[re.sub('EmbedShare URLCopyEmbedCopy', '', i) for i in drake_lyrics[j]] for j in range(len(drake_lyrics))]\n","drake_lyrics = [[i for i in drake_lyrics[j] if i != ''] for j in range(len(drake_lyrics))]\n","\n","taylor_lyrics = [[line1 + ', ' + line2 for line1,line2 in zip(song[0::2], song[1::2])] for song in taylor_lyrics]\n","drake_lyrics = [[line1 + ', ' + line2 for line1,line2 in zip(song[0::2], song[1::2])] for song in drake_lyrics]\n","\n","drake_tokenized = [[word_tokenize(drake_lyrics[i][j]) for j in range(len(drake_lyrics[i]))] for i in range(len(drake_lyrics))]\n","taylor_tokenized = [[word_tokenize(taylor_lyrics[i][j]) for j in range(len(taylor_lyrics[i]))] for i in range(len(taylor_lyrics))]\n","\n","drake_tokenized = [[[word.lower() for word in line] for line in song] for song in drake_tokenized]\n","taylor_tokenized = [[[word.lower() for word in line] for line in song] for song in taylor_tokenized]\n","\n","drake_length = sum([[len(sent) for sent in song] for song in drake_tokenized], [])\n","taylor_length = sum([[len(sent) for sent in song] for song in taylor_tokenized], [])\n","\n","drake_lyrics = sum([[sent for sent in song if (len(sent) \u003e= 10 and len (sent) \u003c= 30)] for song in drake_tokenized], [])\n","taylor_lyrics = sum([[sent for sent in song if (len(sent) \u003e= 10 and len (sent) \u003c= 30)] for song in taylor_tokenized], [])\n","\n","taylor_vocab = sum(taylor_lyrics,[])\n","drake_vocab = sum(drake_lyrics,[])\n","\n","def unique(list1):\n","     \n","    # insert the list to the set\n","    list_set = set(list1)\n","    # convert the set to the list\n","    unique_list = (list(list_set))\n","    return unique_list\n","\n","vocab = taylor_vocab + drake_vocab\n","vocab_counts = Counter(vocab)\n","vocab = unique(vocab)\n","vocab = ['\u003cpad\u003e','\u003cunk\u003e','\u003cs\u003e', '\u003c/s\u003e'] + vocab\n","\n","from torch.utils import data\n","import torch\n","\n","# These IDs are reserved.\n","MAX_SENT_LENGTH = 30\n","MAX_SENT_LENGTH_PLUS_SOS_EOS = 32\n","PAD_INDEX = 0\n","UNK_INDEX = 1\n","SOS_INDEX = 2\n","EOS_INDEX = 3\n","RARE_WORD_TRESHOLD = 0\n","\n","vocab_counts['\u003cpad\u003e'] = RARE_WORD_TRESHOLD + 1\n","vocab_counts['\u003cunk\u003e'] = RARE_WORD_TRESHOLD + 1\n","vocab_counts['\u003cs\u003e'] = RARE_WORD_TRESHOLD + 1\n","vocab_counts['\u003c/s\u003e'] = RARE_WORD_TRESHOLD + 1\n","\n","class TSTDataset(data.Dataset):\n","    def __init__(self, taylor_sentences, drake_sentences, vocab, vocab_counts, sampling=1.):\n","        self.taylor_sentences = taylor_sentences[:int(len(taylor_sentences) * sampling)]\n","        self.drake_sentences = drake_sentences[:int(len(drake_sentences) * sampling)]\n","\n","        self.max_seq_length = MAX_SENT_LENGTH_PLUS_SOS_EOS\n","        self.vocab = vocab\n","        self.vocab_counts = vocab_counts\n","\n","        self.v2id = {v : i for i, v in enumerate(self.vocab)}\n","        self.id2v = {val : key for key, val in self.v2id.items()}\n","    \n","    def __len__(self):\n","        return min(len(self.taylor_sentences), len(self.drake_sentences))\n","    \n","    def __getitem__(self, index):\n","        taylor_sent = self.taylor_sentences[index]\n","        taylor_len = len(taylor_sent) + 2   # add \u003cs\u003e and \u003c/s\u003e to each sentence\n","        taylor_id = []\n","        for w in taylor_sent:\n","            if w not in self.vocab:\n","                w = '\u003cunk\u003e'\n","            if vocab_counts[w] \u003c= RARE_WORD_TRESHOLD:\n","                w = '\u003cunk\u003e'\n","            taylor_id.append(self.v2id[w])\n","\n","        taylor_id = ([SOS_INDEX] + taylor_id + [EOS_INDEX] + [PAD_INDEX] *\n","                  (self.max_seq_length - taylor_len))\n","\n","        drake_sent = self.drake_sentences[index]\n","        drake_len = len(drake_sent) + 2   # add \u003cs\u003e and \u003c/s\u003e to each sentence\n","        drake_id = []\n","        for w in drake_sent:\n","            if w not in self.vocab:\n","                w = '\u003cunk\u003e'\n","            if vocab_counts[w] \u003c= RARE_WORD_TRESHOLD:\n","                w = '\u003cunk\u003e'\n","            drake_id.append(self.v2id[w])\n","\n","        drake_id = ([SOS_INDEX] + drake_id + [EOS_INDEX] + [PAD_INDEX] *\n","                  (self.max_seq_length - drake_len))\n","\n","        return torch.tensor(taylor_id), taylor_len, torch.tensor(drake_id), drake_len\n","\n","dataset = TSTDataset(taylor_lyrics, drake_lyrics, vocab, vocab_counts)\n","\n","test_pct = 0.2\n","valid_pct = 0.1\n","\n","train_dataset, test_dataset = torch.utils.data.random_split(dataset, [int(len(dataset)*(1-test_pct)),len(dataset)-int(len(dataset)*(1-test_pct))])\n","valid_dataset, train_dataset = torch.utils.data.random_split(train_dataset, [int(len(dataset)*valid_pct),len(train_dataset)-int(len(dataset)*valid_pct)])"]},{"cell_type":"markdown","metadata":{"id":"ynBTHa5grUr7"},"source":["### Encoder"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1637906228210,"user":{"displayName":"Katherine Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11324848797432579426"},"user_tz":300},"id":"FieXza5erYUH"},"outputs":[],"source":["from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","\n","class Encoder(nn.Module):\n","  def __init__(self, input_size, hidden_size, dropout=0.):\n","    \"\"\"\n","    Inputs: \n","      - `input_size`: an int representing the RNN input size.\n","      - `hidden_size`: an int representing the RNN hidden size.\n","      - `dropout`: a float representing the dropout rate during training. Note\n","          that for 1-layer RNN this has no effect since dropout only applies to\n","          outputs of intermediate layers.\n","    \"\"\"\n","    super(Encoder, self).__init__()\n","    self.rnn = nn.GRU(input_size, hidden_size, num_layers=1, batch_first=True,\n","                      dropout=dropout, bidirectional=False)\n","\n","  def forward(self, inputs, lengths, init_state=None):\n","    \"\"\"\n","    Inputs:\n","      - `inputs`: a 3d-tensor of shape (batch_size, max_seq_length, embed_size)\n","          representing a batch of padded embedded word vectors of source\n","          sentences.\n","      - `lengths`: a 1d-tensor of shape (batch_size,) representing the sequence\n","          lengths of `inputs`.\n","\n","    Returns:\n","      - `outputs`: a 3d-tensor of shape\n","        (batch_size, max_seq_length, hidden_size).\n","      - `finals`: a 3d-tensor of shape (num_layers, batch_size, hidden_size).\n","      Hint: `outputs` and `finals` are both standard GRU outputs. Check:\n","      https://pytorch.org/docs/stable/nn.html#gru\n","    \"\"\"\n","    # Our variable-length inputs are padded to the same length for batching\n","    # Here we \"pack\" them for computational efficiency (see note below)\n","    packed = pack_padded_sequence(inputs, lengths.cpu(), batch_first=True,\n","                                  enforce_sorted=False)\n","    outputs, finals = self.rnn(packed, init_state)\n","    outputs, _ = pad_packed_sequence(outputs, batch_first=True,\n","                                     total_length=MAX_SENT_LENGTH_PLUS_SOS_EOS)\n","    return outputs, finals"]},{"cell_type":"markdown","metadata":{"id":"rKiJAkvZGMWl"},"source":["### Decoder"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":127,"status":"ok","timestamp":1637906228322,"user":{"displayName":"Katherine Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11324848797432579426"},"user_tz":300},"id":"E1Tdf6_283WY"},"outputs":[],"source":["class GeneratorTransferredSampled(nn.Module):\n","  \"\"\"Define standard linear + softmax generation step.\"\"\"\n","  def __init__(self, hidden_size, vocab_size, src_embed, gamma=0.001):\n","    \"\"\"\n","    Inputs:\n","      - `src_embed`: a 2d-tensor of shape (vocab_size, embed_size )\n","    \"\"\"\n","    super(GeneratorTransferredSampled, self).__init__()\n","    self.proj = nn.Linear(hidden_size, vocab_size, bias=True)\n","    self.gamma = gamma\n","    self.logsoftmax = nn.LogSoftmax(dim = 2)\n","    self.softmax = nn.Softmax(dim = 2)\n","    self.src_embed = src_embed\n","\n","  def embedding(self,x):\n","    return torch.matmul(x,self.src_embed.weight)\n","    \n","  def gumbel_softmax(self,logits, eps=1e-20):\n","    U = torch.rand(logits.shape).to(device)\n","    G = -torch.log(-torch.log(U + eps) + eps).to(device)\n","    return self.logsoftmax((logits + G) / self.gamma)\n","\n","  def gumbel(self,logits, eps=1e-20):\n","    U = torch.rand(logits.shape).to(device)\n","    G = -torch.log(-torch.log(U + eps) + eps).to(device)\n","    return (logits + G) / self.gamma\n","\n","  def forward(self, x):\n","    logits = self.proj(x)\n","    logprob = self.logsoftmax(logits)\n","    prob = self.softmax(logits)\n","    output = self.embedding(prob)\n","    word  = logits.argmax(dim = 2, keepdim = False)\n","\n","    return output, logprob, word\n","\n","  def forward_gumbel(self, x):\n","    logits = self.proj(x)\n","    prob = self.softmax(self.gumbel(logits))\n","    logprob = self.logsoftmax(self.gumbel(logits))\n","    output = self.embedding(prob)\n","    word  = logits.argmax(dim = 2, keepdim = False)\n","\n","    return output, logprob, word"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":277,"status":"ok","timestamp":1637906228598,"user":{"displayName":"Katherine Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11324848797432579426"},"user_tz":300},"id":"H4amKWmOxDk2"},"outputs":[],"source":["from queue import PriorityQueue\n","class BeamSearchNode:\n","  def __init__(self, hiddenstate, previousNode, cur_embed, wordId, \n","               logProb, gumbel_logits,\n","               length ):\n","    self.h = hiddenstate\n","    self.prevNode = previousNode\n","    self.cur_embed = cur_embed\n","    self.wordid = wordId\n","    self.logp = logProb\n","    self.gumbel_logits = gumbel_logits\n","    self.leng = length\n","    \n","  def __lt__(self,other):\n","    return self.logp \u003c other.logp\n","\n","  def eval(self, alpha=1.0):\n","    return self.logp \n","    # Add here a function for shaping a reward\n","    # reward = 0\n","    # return self.logp / float(self.leng - 1 + 1e-6) + alpha * reward'\n","\n","class BeamSearch:\n","  def __init__(self,decoder, beam_width, topk, line_embed,max_len,\n","               max_iter=2000):\n","    \"\"\"Use Beam Search to generate a full sentence with the given decoder model\n","  Inputs:\n","      - `decoder`: decoder module with forward_step_beam function\n","      - `beam_width` : the length of the beam \n","      - `max_len`: an int representing the maximum decoding length.\n","      - `max_iter`: The maximum decoding iteration\n","    \"\"\"\n","    self.decoder = decoder\n","    self.beam_width = beam_width\n","    self.topk = topk\n","    self.line_embed = line_embed\n","    self.max_len = max_len\n","    self.max_iter = max_iter\n","\n","  def beam_decode(self,inputs, encoder_finals):\n","    \"\"\"Use Beam Search to generate a full sentence with the given decoder model\n","    Inputs:\n","        - `inputs`: a 3d-tensor of shape (batch_size, 1, embed_size)\n","            representing a batch of padded embedded word vectors of SOS . \n","        - `encoder_finals`: a 3d-tensor of shape\n","            (num_enc_layers, batch_size, hidden_size) representing the final\n","            encoder hidden states used to initialize the initial decoder hidden\n","            states.\n","\n","    Returns:\n","        - `final_logp_batch`: a 2d-tensor of shape\n","            (batch_size, sentences_num) representing the probability of generating \n","            the sentence.\n","        - `final_hidden_batch`: a 4d-tensor of shape\n","            (sentences_num, num_layers, batch_size, hidden_size) representing \n","            the final hidden layer\n","        - `final_gumbel_logits_batch`: a 4d-tensor of shape\n","            (batch_size, sentences_num, max_len, vocab_size) representing \n","            the gumbel logits of each hidden layer output. \n","        - `decoded_batch`: a 3d-tensor of shape\n","            (batch_size,sentences_num,  max_len) representing output sentence and\n","            the corresponding word index (can be used for embedding)  \n","    \n","    \"\"\"\n","    decoded_batch,final_gumbel_logits_batch = [],[]\n","    final_hidden_batch, final_logp_batch = [],[]\n","    for i in range(inputs.shape[0]):\n","      decoder_input = inputs[i:i+1,:,:]\n","      # Number of sentence to generate\n","      endnodes = []\n","      number_required = self.topk\n","\n","      # starting node -  hidden vector, previous node, cur_embed, word id , logp, length\n","      node = BeamSearchNode(self.decoder.init_hidden(encoder_finals[:,i:i+1,:]), \n","                            None, decoder_input, [[SOS_INDEX]], 0, None, 1)\n","      nodes = PriorityQueue()\n","\n","      nodes.put((-node.eval(), node))\n","      qsize = 1\n","\n","      while qsize\u003c=self.max_iter:\n","        tocheck = min(nodes.qsize(), self.beam_width)\n","        new_nodes = PriorityQueue()\n","        while tocheck\u003e0:\n","          score, n = nodes.get()\n","          decoder_input = n.cur_embed\n","          decoder_hidden = n.h\n","          if n.leng \u003e self.max_len:\n","            endnodes.append((score, n))\n","            # if we reached maximum # of sentences required\n","            if len(endnodes) \u003e= number_required:\n","                break\n","\n","          # decode for one step using decoder\n","          hidden, _, gumbel_logits, wordId, logsoftmax_logits = decoder.forward_step_beam(decoder_input, \n","                                                                                        decoder_hidden)\n","          tocheck -= 1\n","          # PUT HERE REAL BEAM SEARCH OF TOP\n","          log_prob, indexes = torch.topk(logsoftmax_logits, self.beam_width)\n","          for new_k in range(self.beam_width):\n","            decoded_t = indexes[0][0][new_k].view(1, -1)\n","            log_p = log_prob[0][0][new_k]\n","            prev_embed = self.line_embed(decoded_t)\n","\n","            node = BeamSearchNode(decoder_hidden, n,prev_embed, decoded_t, \n","                                  n.logp + log_p, gumbel_logits,n.leng + 1)\n","            score = -node.eval()\n","            new_nodes.put((score, node))\n","          qsize += self.beam_width - 1\n","        nodes = new_nodes\n","\n","        if len(endnodes) \u003e= number_required:\n","            break\n","        \n","\n","      # choose nbest paths, back trace them\n","      if len(endnodes) == 0:\n","          endnodes = [nodes.get() for _ in range(self.topk)]\n","\n","      utterances = []\n","      final_logps = []\n","      final_hiddens = []\n","      final_gumbel_logits = []\n","      for score, n in sorted(endnodes, key=operator.itemgetter(0)):\n","          end_node = n\n","          utterance = []\n","          gumbel_logits = []\n","          utterance.append(n.wordid[0][0])\n","          gumbel_logits =gumbel_logits + [n.gumbel_logits]\n","          # back trace\n","          while n.prevNode != None:\n","              n = n.prevNode\n","              utterance.append(n.wordid[0][0])\n","              if n.gumbel_logits is not None:\n","                gumbel_logits =gumbel_logits + [n.gumbel_logits]\n","\n","          utterance = torch.unsqueeze(torch.tensor(utterance[::-1][:self.max_len]), axis = 0)\n","          utterances.append(utterance)\n","          gumbel_logits = torch.cat(gumbel_logits, dim = 1)\n","          final_logp = end_node.logp \n","          final_hidden = end_node.h\n","          final_logps.append(final_logp)\n","          final_hiddens.append(torch.unsqueeze(torch.tensor(final_hidden), axis = 0))\n","          final_gumbel_logits.append(gumbel_logits)\n","        \n","      utterances = torch.cat(utterances, axis = 0 )\n","      decoded_batch.append(torch.unsqueeze(utterances, axis = 0))\n","      final_logp_batch.append(torch.unsqueeze(torch.tensor(final_logps), axis = 0))\n","      final_hiddens = torch.cat(final_hiddens, axis = 0)\n","      final_hidden_batch.append(final_hiddens)\n","      final_gumbel_logits = torch.cat(final_gumbel_logits, axis = 0)\n","      final_gumbel_logits_batch.append(torch.unsqueeze(final_gumbel_logits, axis = 0))\n","      # print(\"after decoded_batch:\")\n","      # print(decoded_batch)\n","\n","    # decoded_batch size = (batch, topk, sentence_len, 1)\n","    final_logp_batch = torch.cat(final_logp_batch, axis = 0)\n","    final_hidden_batch = torch.cat(final_hidden_batch, axis = 2)\n","    final_gumbel_logits_batch = torch.cat(final_gumbel_logits_batch, axis = 0)\n","    decoded_batch = torch.cat(decoded_batch, axis = 0)\n","    print(\"final_logp_batch.shape:\",final_logp_batch.shape)\n","    print(\"final_hidden_batch.shape:\",final_hidden_batch.shape)\n","    print(\"final_gumbel_logits_batch.shape:\",final_gumbel_logits_batch.shape)\n","    print(\"decoded_batch.shape:\",decoded_batch.shape)\n","    print(\"max_len:\", self.max_len)\n","    print(\"topk:\", self.topk)\n","    print(\"batch size:\", inputs.shape[0])\n","    return final_hidden_batch, final_logp_batch, final_gumbel_logits_batch, decoded_batch\n","\n","\n"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":133,"status":"ok","timestamp":1637906228729,"user":{"displayName":"Katherine Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11324848797432579426"},"user_tz":300},"id":"plJpAyPu86yw"},"outputs":[],"source":["class Decoder(nn.Module):\n","  \"\"\"An RNN decoder + generator with GRU\"\"\"\n","\n","  def __init__(self, input_size, hidden_size, max_len,generator, num_layers = 1, dropout=0.):\n","    \"\"\"\n","      Inputs:\n","        - `input_size` , `hidden_size`, and `dropout` the same as in Encoder.\n","    \"\"\"\n","    super(Decoder, self).__init__()\n","    self.input_size = input_size\n","    self.hidden_size = hidden_size\n","    self.rnn = nn.GRU(input_size, hidden_size, num_layers=num_layers, batch_first=True,\n","                      dropout=dropout, bidirectional=False)\n","    self.generator = generator\n","    self.max_len = max_len\n","    self.dropout_layer = nn.Dropout(p=dropout)\n","    self.rnn_to_pre = nn.Linear(input_size + hidden_size,\n","                                hidden_size, bias=False)\n","\n","  def forward_step(self, prev_embed, hidden):\n","    \"\"\"Helper function for forward below:\n","       Perform a single decoder step (1 word).\n","\n","       Inputs:\n","      - `prev_embed`: a 3d-tensor of shape (batch_size, 1, embed_size = vocab_size)\n","          representing the padded embedded word vectors at this step in training\n","      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size) representing\n","          the current hidden state.\n","\n","      Returns:\n","      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size)\n","          representing the current decoder hidden state.\n","      - `output`: a 3d-tensor of shape (batch_size, max_len, vocab_size)\n","          representing the total generated outputs.\n","    \"\"\"\n","    pre_output, hidden = self.rnn(prev_embed, hidden)\n","    pre_output = torch.cat([prev_embed, pre_output], dim=2)\n","    pre_output = self.dropout_layer(pre_output)\n","    pre_output = self.rnn_to_pre(pre_output)\n","    pre_output = torch.tanh(pre_output)\n","\n","    return hidden, pre_output\n","\n","    ### Your code here!\n","    pre_output, hidden = self.rnn(prev_embed, hidden)\n","    pre_output = torch.cat([prev_embed, pre_output], dim=2)\n","    pre_output = self.dropout_layer(pre_output)\n","    pre_output = self.rnn_to_pre(pre_output)\n","    pre_output = self.pre_activation(pre_output)\n","\n","  def forward_step_beam(self, prev_embed, hidden):\n","    \"\"\"Beam Search only: Unroll the decoder one step at a time.\n","    Inputs:\n","      - `input`: a 3d-tensor of shape (batch_size, 1, embed_size)\n","          representing the padded embedded word vectors at this step in training\n","      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size) representing\n","          the current hidden state.\n","    Returns:\n","      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size)\n","          representing the current decoder hidden state.\n","      - `output`: a 3d-tensor of shape (batch_size, 1, vocab_size)\n","          representing the total generated outputs.    \n","      - `gumbel_logits`: a 3d-tensor of shape\n","          (batch_size, 1, trg_vocab_size) representing the mapped decoder\n","          outputs from gumbel softmax.\n","      - `output_word`: a 2d-tensor of shape\n","          (batch_size, 1) representing output sentence and\n","          the corresponding word index (can be used for embedding)    \n","      - `logits`: a 2d-tensor of shape\n","          (batch_size, 1, trg_vocab_size) representing the mapped decoder\n","          outputs from log softmax \n","          \"\"\"\n","    hidden, pre_output = self.forward_step(prev_embed,hidden)\n","    output, gumbel_logits, output_word, logits = self.generator(pre_output)\n","    return  hidden, output, gumbel_logits, output_word, logits\n","    \n","  def forward(self, input, encoder_finals,max_len, hidden=None):\n","    \"\"\"Unroll the decoder one step at a time.\n","\n","    Inputs:\n","      - `inputs`: a 3d-tensor of shape (batch_size, 1, embed_size)\n","          representing a batch of padded embedded word vectors of SOS . \n","          If size is (batch_size,max_len, embed_size), then it is teacher forcing.\n","      - `encoder_finals`: a 3d-tensor of shape\n","          (num_enc_layers, batch_size, hidden_size) representing the final\n","          encoder hidden states used to initialize the initial decoder hidden\n","          states.\n","      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size) representing\n","          the value to be used to initialize the initial decoder hidden states.\n","          If None, then use `encoder_finals`.\n","      - `max_len`: an int representing the maximum decoding length.\n","      - `style`: TAYLOR_STYLE or DRAKE_STYLE\n","\n","    Returns:\n","      - `hidden`: a 3d-tensor of shape\n","          (num_layers, batch_size, hidden_size) representing the final hidden\n","          state for each element in the batch.\n","      - `outputs`: a 3d-tensor of shape\n","          (batch_size, max_len, hidden_size) representing the raw decoder\n","          outputs (before mapping to a `trg_vocab_size`-dim vector).\n","      - `logits_vectors`: a 3d-tensor of shape\n","          (batch_size, max_len, trg_vocab_size) representing the mapped decoder\n","          outputs.\n","      - `words`: a 3d-tensor of shape\n","          (batch_size, max_len, 1) representing output sentence and\n","          the corresponding word index (can be used for embedding)  \n","    \"\"\"\n","\n","    # Initialize decoder hidden state.\n","    if hidden is None:\n","      hidden = self.init_hidden(encoder_finals)\n","    output_vectors = []\n","    logits_vectors = []\n","    words = []\n","    hidden_states = []\n","    hidden_states.append(hidden[-1][:,None,:])\n","    for i in range(max_len-1) :\n","      hidden, prev_output = self.forward_step(input,hidden)\n","      input, logits, output_word = self.generator.forward_gumbel(prev_output)\n","      # input, logits, output_word = self.generator(prev_output)\n","\n","      # input = torch.concat([input,torch.full(input.shape,style)], axis = -1)\n","      logits_vectors.append(logits)\n","      output_vectors.append(input)\n","      words.append(output_word)\n","      hidden_states.append(prev_output)\n","\n","    outputs = torch.cat(output_vectors, dim =1)\n","    logits_vectors = torch.cat(logits_vectors,dim = 1)\n","    words = torch.cat(words, axis = -1)\n","    hidden_states = torch.cat(hidden_states, axis = 1)\n","    return hidden, outputs , logits_vectors, words, hidden_states\n","\n","  def forward_teacher(self, input, encoder_finals, max_len=None, hidden=None):\n","    \"\"\"Unroll the decoder one step at a time.\n","\n","    Inputs:\n","      - `inputs`: a 3d-tensor of shape (batch_size,max_len, embed_size)\n","          representing a batch of padded embedded word vectors of original \n","          sentence and acts as  teacher forcing.\n","\n","    Returns:\n","      - `hidden`: a 3d-tensor of shape\n","          (num_layers, batch_size, hidden_size) representing the final hidden\n","          state for each element in the batch.\n","      - `outputs`: a 3d-tensor of shape\n","          (batch_size, max_len, hidden_size) representing the raw decoder\n","          outputs (before mapping to a `trg_vocab_size`-dim vector).\n","      - `logits_vectors`: a 3d-tensor of shape\n","          (batch_size, max_len, trg_vocab_size) representing the mapped decoder\n","          outputs each represents the probability? \n","      - `words`: a 3d-tensor of shape\n","          (batch_size, max_len, 1) representing output sentence and\n","          the corresponding word index (can be used for embedding)      \n","    \"\"\"\n","\n","    # Initialize decoder hidden state.\n","    if max_len is None:\n","      max_len = input.shape[1]\n","    if hidden is None:\n","      hidden = self.init_hidden(encoder_finals)\n","    output_vectors = []\n","    logits_vectors = []\n","    words = []\n","    hidden_states = []\n","    hidden_states.append(hidden[-1][:,None,:])\n","    for i in range(max_len):\n","      hidden, prev_output = self.forward_step(input[:,i:i+1,:],hidden)\n","      output, logits, output_word = self.generator(prev_output)\n","      \n","      logits_vectors.append(logits)\n","      output_vectors.append(output)\n","      words.append(output_word)\n","      hidden_states.append(prev_output)\n","\n","    outputs = torch.cat(output_vectors, dim =1)\n","    logits_vectors = torch.cat(logits_vectors,dim = 1)\n","    words = torch.cat(words, axis = -1)\n","    hidden_states = torch.cat(hidden_states, axis = 1)\n","    return hidden, outputs , logits_vectors, words, hidden_states\n","\n","  def init_hidden(self, encoder_finals):\n","    \"\"\"Use encoder final hidden state to initialize decoder's first hidden\n","       state.\n","\n","       Input: `encoder_finals` is same as in forward()\n","\n","       Returns: \n","         - `decoder_init_hiddens`: a 3d-tensor of shape \n","              (num_layers, batch_size, hidden_size) representing the initial\n","              hidden state of the decoder for each element in the batch \n","    \"\"\"\n","    decoder_init_hiddens = torch.tanh(encoder_finals)\n","    return decoder_init_hiddens"]},{"cell_type":"markdown","metadata":{"id":"NX25tl4G5vWa"},"source":["### Attention Decoder"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1637906228729,"user":{"displayName":"Katherine Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11324848797432579426"},"user_tz":300},"id":"VrWt0I5N6ijP"},"outputs":[],"source":["class BahdanauAttention(nn.Module):\n","    \"\"\"Implements Bahdanau (MLP) attention\"\"\"\n","    \n","    def __init__(self, hidden_size, key_size=None, query_size=None):\n","        super(BahdanauAttention, self).__init__()\n","        \n","        # We assume a bi-directional encoder so key_size is 2*hidden_size\n","        key_size = 2 * hidden_size if key_size is None else key_size\n","        query_size = hidden_size if query_size is None else query_size\n","\n","        self.key_layer = nn.Linear(key_size, hidden_size, bias=False)\n","        self.query_layer = nn.Linear(query_size, hidden_size, bias=False)\n","        self.energy_layer = nn.Linear(hidden_size, 1, bias=False)\n","        \n","        # to store attention scores\n","        self.alphas = None\n","        \n","    def forward(self, query=None, proj_key=None, value=None, mask=None):\n","        assert mask is not None, \"mask is required\"\n","\n","        # We first project the query (the decoder state).\n","        # The projected keys (the encoder states) were already pre-computated.\n","        query = self.query_layer(query)\n","        \n","        # Calculate scores.\n","        scores = self.energy_layer(torch.tanh(query + proj_key))\n","        scores = scores.squeeze(2).unsqueeze(1)\n","        \n","        # Mask out invalid positions.\n","        # The mask marks valid positions so we invert it using `mask \u0026 0`.\n","        scores.data.masked_fill_(mask == 0, -float('inf'))\n","        \n","        # Turn scores to probabilities.\n","        alphas = F.softmax(scores, dim=-1)\n","        self.alphas = alphas        \n","        \n","        # The context vector is the weighted sum of the values.\n","        context = torch.bmm(alphas, value)\n","        \n","        # context shape: [B, 1, 2D], alphas shape: [B, 1, M]\n","        return context, alphas"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":243,"status":"ok","timestamp":1637906229206,"user":{"displayName":"Katherine Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11324848797432579426"},"user_tz":300},"id":"KKIgeUK75xSm"},"outputs":[],"source":["class AttentionDecoder(nn.Module):\n","  \"\"\"An RNN decoder + generator with GRU\"\"\"\n","\n","  def __init__(self, input_size, hidden_size, attention, max_len,generator, num_layers = 1, dropout=0.):\n","    \"\"\"\n","      Inputs:\n","        - `input_size` , `hidden_size`, and `dropout` the same as in Encoder.\n","    \"\"\"\n","    super(AttentionDecoder, self).__init__()\n","    self.input_size = input_size\n","    self.hidden_size = hidden_size\n","    # self.rnn = nn.GRU(input_size, hidden_size, num_layers=num_layers, batch_first=True,\n","    #                   dropout=dropout, bidirectional=False)\n","    self.rnn = nn.GRU(input_size + hidden_size, hidden_size, num_layers,\n","                          batch_first=True, dropout=dropout)\n","    \n","    self.generator = generator\n","    self.max_len = max_len\n","    self.dropout_layer = nn.Dropout(p=dropout)\n","    # self.rnn_to_pre = nn.Linear(input_size + hidden_size,\n","    #                             hidden_size, bias=False)\n","    self.rnn_to_pre = nn.Linear(hidden_size + hidden_size + input_size,\n","                                hidden_size, bias=False)\n","    self.attention = attention\n","\n","  def forward_step(self, prev_embed, encoder_hidden, src_mask, proj_key, hidden):\n","    \"\"\"Helper function for forward below:\n","       Perform a single decoder step (1 word).\n","\n","       Inputs:\n","      - `prev_embed`: a 3d-tensor of shape (batch_size, 1, embed_size = vocab_size)\n","          representing the padded embedded word vectors at this step in training\n","      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size) representing\n","          the current hidden state.\n","\n","      Returns:\n","      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size)\n","          representing the current decoder hidden state.\n","      - `output`: a 3d-tensor of shape (batch_size, max_len, vocab_size)\n","          representing the total generated outputs.\n","    \"\"\"\n","\n","    # compute context vector using attention mechanism\n","    query = hidden[-1].unsqueeze(1)  # [#layers, B, D] -\u003e [B, 1, D]\n","    context, attn_probs = self.attention(\n","        query=query, proj_key=proj_key,\n","        value=encoder_hidden, mask=src_mask)\n","    \n","    # print(\"Input Hidden, isnan:\",torch.linalg.norm(hidden))\n","    # print(\"Query, isnan:\",torch.linalg.norm(query))\n","    # print(\"Context, isnan:\",torch.linalg.norm(context))\n","    # print(\"Attention Probs, isnan:\",torch.linalg.norm(attn_probs))\n","    \n","    # RNN\n","    rnn_input = torch.cat([prev_embed, context], dim=2)\n","    # print(\"RNN Input, isnan:\",torch.linalg.norm(hidden))\n","    output, hidden = self.rnn(rnn_input, hidden)\n","\n","    # print(\"Output Hidden, isnan:\",torch.linalg.norm(hidden))\n","    # print(\"RNN Output, isnan:\",torch.linalg.norm(prev_embed))\n","    # print(\"Prev Embedding, isnan:\",torch.linalg.norm(prev_embed))\n","    \n","    pre_output = torch.cat([prev_embed, output, context], dim=2)\n","    pre_output = self.dropout_layer(pre_output)\n","    pre_output = self.rnn_to_pre(pre_output)\n","    \n","    return hidden, pre_output\n","\n","  def forward(self, input, encoder_hidden, encoder_finals, src_mask, max_len, hidden=None):\n","    \"\"\"Unroll the decoder one step at a time.\n","\n","    Inputs:\n","      - `inputs`: a 3d-tensor of shape (batch_size, 1, embed_size)\n","          representing a batch of padded embedded word vectors of SOS . \n","          If size is (batch_size,max_len, embed_size), then it is teacher forcing.\n","      - `encoder_finals`: a 3d-tensor of shape\n","          (num_enc_layers, batch_size, hidden_size) representing the final\n","          encoder hidden states used to initialize the initial decoder hidden\n","          states.\n","      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size) representing\n","          the value to be used to initialize the initial decoder hidden states.\n","          If None, then use `encoder_finals`.\n","      - `max_len`: an int representing the maximum decoding length.\n","      - `style`: TAYLOR_STYLE or DRAKE_STYLE\n","\n","    Returns:\n","      - `hidden`: a 3d-tensor of shape\n","          (num_layers, batch_size, hidden_size) representing the final hidden\n","          state for each element in the batch.\n","      - `outputs`: a 3d-tensor of shape\n","          (batch_size, max_len, hidden_size) representing the raw decoder\n","          outputs (before mapping to a `trg_vocab_size`-dim vector).\n","      - `logits_vectors`: a 3d-tensor of shape\n","          (batch_size, max_len, trg_vocab_size) representing the mapped decoder\n","          outputs.\n","      - `words`: a 3d-tensor of shape\n","          (batch_size, max_len, 1) representing output sentence and\n","          the corresponding word index (can be used for embedding)  \n","    \"\"\"\n","\n","    # Initialize decoder hidden state.\n","    if hidden is None:\n","      hidden = self.init_hidden(encoder_finals)\n","\n","    proj_key = self.attention.key_layer(encoder_hidden)\n","\n","    output_vectors = []\n","    logits_vectors = []\n","    words = []\n","    hidden_states = []\n","    hidden_states.append(hidden[-1][:,None,:])\n","    \n","    for i in range(max_len-1) :\n","      # print(\"Step \",i)\n","      # print(\"Step \",i, \" Input, isnan:\",torch.isnan(hidden).sum())\n","      # print(\"Step \",i, \" Input Hidden, isnan:\",torch.isnan(hidden).sum())\n","      # print(\"Step \",i, \" Proj Key, isnan:\",torch.isnan(hidden).sum())\n","      hidden, prev_output = self.forward_step(input,encoder_hidden, src_mask, proj_key, hidden)\n","      # print(\"Step \",i, \" Pre-Output, isnan:\",torch.isnan(prev_output).sum())\n","      # input, logits, output_word = self.generator.forward_gumbel(prev_output)\n","      input, logits, output_word = self.generator(prev_output)\n","\n","      logits_vectors.append(logits)\n","      output_vectors.append(input)\n","      words.append(output_word)\n","      hidden_states.append(prev_output)\n","\n","    outputs = torch.cat(output_vectors, dim =1)\n","    logits_vectors = torch.cat(logits_vectors,dim = 1)\n","    words = torch.cat(words, axis = -1)\n","    hidden_states = torch.cat(hidden_states, axis = 1)\n","\n","    return hidden, outputs , logits_vectors, words, hidden_states\n","\n","  def forward_teacher(self, input, encoder_hidden, encoder_finals, src_mask, max_len=None, hidden=None):\n","    \"\"\"Unroll the decoder one step at a time.\n","\n","    Inputs:\n","      - `inputs`: a 3d-tensor of shape (batch_size,max_len, embed_size)\n","          representing a batch of padded embedded word vectors of original \n","          sentence and acts as  teacher forcing.\n","\n","    Returns:\n","      - `hidden`: a 3d-tensor of shape\n","          (num_layers, batch_size, hidden_size) representing the final hidden\n","          state for each element in the batch.\n","      - `outputs`: a 3d-tensor of shape\n","          (batch_size, max_len, hidden_size) representing the raw decoder\n","          outputs (before mapping to a `trg_vocab_size`-dim vector).\n","      - `logits_vectors`: a 3d-tensor of shape\n","          (batch_size, max_len, trg_vocab_size) representing the mapped decoder\n","          outputs each represents the probability? \n","      - `words`: a 3d-tensor of shape\n","          (batch_size, max_len, 1) representing output sentence and\n","          the corresponding word index (can be used for embedding)      \n","    \"\"\"\n","\n","    # Initialize decoder hidden state.\n","    if max_len is None:\n","      max_len = input.shape[1]\n","    if hidden is None:\n","      hidden = self.init_hidden(encoder_finals)\n","    \n","    proj_key = self.attention.key_layer(encoder_hidden)\n","\n","    output_vectors = []\n","    logits_vectors = []\n","    words = []\n","    hidden_states = []\n","    hidden_states.append(hidden[-1][:,None,:])\n","\n","    for i in range(max_len):\n","      hidden, prev_output = self.forward_step(input[:,i:i+1,:], encoder_hidden, src_mask, proj_key, hidden)\n","      \n","      output, logits, output_word = self.generator(prev_output)\n","      logits_vectors.append(logits)\n","      output_vectors.append(output)\n","      words.append(output_word)\n","      hidden_states.append(prev_output)\n","\n","    outputs = torch.cat(output_vectors, dim =1)\n","    logits_vectors = torch.cat(logits_vectors,dim = 1)\n","    words = torch.cat(words, axis = -1)\n","    hidden_states = torch.cat(hidden_states, axis = 1)\n","    return hidden, outputs , logits_vectors, words, hidden_states\n","\n","  def init_hidden(self, encoder_finals):\n","    \"\"\"Use encoder final hidden state to initialize decoder's first hidden\n","       state.\n","\n","       Input: `encoder_finals` is same as in forward()\n","\n","       Returns: \n","         - `decoder_init_hiddens`: a 3d-tensor of shape \n","              (num_layers, batch_size, hidden_size) representing the initial\n","              hidden state of the decoder for each element in the batch \n","    \"\"\"\n","    decoder_init_hiddens = torch.tanh(encoder_finals)\n","    return decoder_init_hiddens"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":188253,"status":"ok","timestamp":1637773273265,"user":{"displayName":"Hammaad Adam","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03287553409115045279"},"user_tz":300},"id":"bJgy5Rqt7efj","outputId":"7e5c2b37-c7cd-4906-94cb-71ff411918a4"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n"]},{"name":"stdout","output_type":"stream","text":["Finished Training Epoch  0\n","Training PPL 186.3012933666054\n","Valid PPL 36.84848284237294\n","Finished Training Epoch  1\n","Training PPL 13.585992878438688\n","Valid PPL 7.053865752802458\n","Finished Training Epoch  2\n","Training PPL 4.822962307204506\n","Valid PPL 4.504301768389754\n"]}],"source":["### Work in progress\n","class EncoderDecoderAttention(nn.Module):\n","  def __init__(self, encoder, decoder, line_embed, generator):\n","    super(EncoderDecoderAttention, self).__init__()\n","\n","    self.encoder = encoder\n","    self.decoder = decoder\n","    self.line_embed = line_embed\n","    self.generator = generator\n","\n","  def forward(self, lines, line_lens):\n","    encoder_hidden, encoder_finals = self.encode(lines, line_lens)\n","    src_mask = (lines != PAD_INDEX).unsqueeze(-2)\n","    # return self.reconstruct(encoder_hidden, encoder_finals, lines[:, :-1], src_mask)\n","    return self.reconstruct(encoder_hidden, encoder_finals, lines[:, :-1], src_mask), self.decode(encoder_hidden, encoder_finals, src_mask)\n","\n","  def encode(self, lines, line_lens):\n","    return self.encoder(self.line_embed(lines), line_lens)\n","    \n","  def reconstruct(self, encoder_hidden, h0, lines, src_mask):\n","    original = self.line_embed(lines)\n","    return self.decoder.forward_teacher(original,encoder_hidden, h0, src_mask)\n","\n","  def decode(self, encoder_hidden, h0, src_mask):\n","    target = self.line_embed(torch.tensor([SOS_INDEX]).repeat(h0.size()[1],1).to(device))\n","    return self.decoder.forward(target, encoder_hidden, h0, src_mask, max_len)\n","\n","epochs = 3\n","lr = 1e-3\n","batch_size = 32\n","print_every = 100\n","max_len = dataset.max_seq_length\n","vocab_size = len(vocab)\n","embed_size = 256\n","hidden_size = 256\n","dropout = 0.2\n","gamma = 0.001\n","\n","train_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","valid_loader = data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n","\n","line_embed = nn.Embedding(vocab_size, embed_size)\n","encoder = Encoder(embed_size,hidden_size)\n","generator = GeneratorTransferredSampled(hidden_size,vocab_size, line_embed, gamma = gamma)\n","attention = BahdanauAttention(hidden_size, key_size=hidden_size)\n","decoder = AttentionDecoder(embed_size, hidden_size, attention=attention, max_len=vocab_size, generator = generator,dropout=dropout)\n","model = EncoderDecoderAttention(encoder, decoder, line_embed, generator).to(device)\n","optimizer_model = torch.optim.Adam(model.parameters(), lr=lr)\n","rec_loss = nn.NLLLoss(reduction=\"mean\",ignore_index = PAD_INDEX)\n","\n","for epoch in range(epochs):\n","  epoch_rec_loss = 0\n","  epoch_tokens = 0\n","  model.train()\n","  for i,(taylor_lines, taylor_len, drake_lines, drake_len) in enumerate(train_loader):\n","    lines = torch.cat((taylor_lines, drake_lines), 0).to(device)    \n","    line_lens = torch.cat((taylor_len, drake_len), 0).to(device)\n","\n","    # Train model\n","    rec_orig,dec_orig  = model(lines, line_lens)   \n","    # rec_orig  = model(lines, line_lens)    \n","    loss_rec = rec_loss(input=dec_orig[2].permute(0,2,1), target=lines[:, 1:])\n","\n","    optimizer_model.zero_grad()\n","    loss_rec.backward()\n","    optimizer_model.step()\n","    \n","    epoch_rec_loss += loss_rec.item() * line_lens.sum().item()\n","    epoch_tokens += line_lens.sum().item()\n","\n","  print(\"Finished Training Epoch \", epoch)\n","  print(\"Training PPL\", np.exp(epoch_rec_loss / float(epoch_tokens)))\n","  val_loss = 0\n","  val_tokens = 0\n","\n","  for i,(taylor_lines, taylor_len, drake_lines, drake_len) in enumerate(valid_loader):\n","    lines = torch.cat((taylor_lines, drake_lines), 0).to(device)    \n","    line_lens = torch.cat((taylor_len, drake_len), 0).to(device)\n","\n","    rec_orig,dec_orig  = model(lines, line_lens)  \n","    # rec_orig  = model(lines, line_lens)    \n","    loss_rec = rec_loss(input=dec_orig[2].permute(0,2,1), target=lines[:, 1:])\n","\n","    val_loss += loss_rec.item() * line_lens.sum().item()\n","    val_tokens += line_lens.sum().item()\n","  \n","  print(\"Valid PPL\", np.exp(val_loss / float(val_tokens)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n-8K-9QC4W25"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":135,"status":"ok","timestamp":1637773052647,"user":{"displayName":"Hammaad Adam","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03287553409115045279"},"user_tz":300},"id":"UO_EaE4pb-wm","outputId":"685e9ae9-29fa-4682-ce2c-351f72f3b770"},"outputs":[{"data":{"text/plain":["tensor([[ 4619,  9239,  9239,  ...,  9268, 12570,  9268],\n","        [ 6947,  2453,  6338,  ...,  7785, 10651, 10651],\n","        [ 7670, 12181,  6029,  ..., 12186,  3635, 12570],\n","        ...,\n","        [ 7372,  3998, 10130,  ...,  8616,  2051,  7983],\n","        [ 9077, 10213,  9280,  ...,  1119,  3461,  7149],\n","        [ 5248,  9280, 12234,  ..., 11901,  6566,  9320]], device='cuda:0')"]},"execution_count":43,"metadata":{},"output_type":"execute_result"}],"source":["dec_orig[3]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1179,"status":"ok","timestamp":1637774415076,"user":{"displayName":"Hammaad Adam","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03287553409115045279"},"user_tz":300},"id":"Dznyd7lnV-vr","outputId":"a0625e71-77da-4aae-c8cd-d34f77b4ee0f"},"outputs":[{"name":"stdout","output_type":"stream","text":["['\u003cs\u003e', 'i', 'just', ',', 'i', 'ca', \"n't\", ',', 'i', 'just', 'ca', \"n't\", 'be', 'lovin', \"'\", 'you', 'no', 'more', ',', 'i', 'love', 'you', 'more', 'than', 'i', 'love', 'myself', '\u003c/s\u003e', '\u003cpad\u003e', '\u003cpad\u003e', '\u003cpad\u003e', '\u003cpad\u003e']\n","['i', 'just', ',', 'i', 'ca', \"n't\", ',', 'i', 'just', 'ca', \"n't\", 'be', 'lovin', \"'\", 'no', 'no', 'more', ',', 'i', 'love', 'more', 'more', 'than', 'i', '\u003c/s\u003e', 'myself', '\u003c/s\u003e', '\u003c/s\u003e', '\u003c/s\u003e', '\u003c/s\u003e', '\u003c/s\u003e']\n","['i', 'just', ',', 'i', 'ca', \"n't\", ',', 'i', 'just', 'ca', \"n't\", 'be', 'lovin', \"'\", 'you', 'more', 'more', ',', 'i', 'love', 'more', 'more', 'than', 'i', 'myself', '\u003c/s\u003e', '\u003c/s\u003e', '\u003c/s\u003e', '\u003c/s\u003e', '\u003c/s\u003e', '\u003c/s\u003e']\n"]}],"source":["def lookup_words(x, vocab):\n","  return [vocab[i] for i in x]\n","\n","idx=17\n","print(lookup_words(lines[idx], vocab))\n","print(lookup_words(rec_orig[3][idx], vocab))\n","print(lookup_words(dec_orig[3][idx], vocab))"]},{"cell_type":"markdown","metadata":{"id":"t2mg2wYk30dv"},"source":["### Encoder-Decoder for Testing"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":413},"executionInfo":{"elapsed":17216,"status":"error","timestamp":1637697712725,"user":{"displayName":"Hammaad Adam","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03287553409115045279"},"user_tz":300},"id":"SlnB42ru9uBI","outputId":"268b3764-e590-45dc-b110-ba39d5953970"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n"]},{"ename":"KeyboardInterrupt","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-9-3a4dd8a3f268\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m   \u001b[0mepoch_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 56\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtaylor_lines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtaylor_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrake_lines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrake_len\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtaylor_lines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrake_lines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mline_lens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtaylor_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrake_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m\u003clistcomp\u003e\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 363\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 363\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m\u003cipython-input-4-293e29ed4a5c\u003e\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mdrake_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdrake_sent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 107\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m                 \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\u003cunk\u003e'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvocab_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m\u003c=\u001b[0m \u001b[0mRARE_WORD_TRESHOLD\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["### Work in progress\n","class EncoderDecoder(nn.Module):\n","  def __init__(self, encoder, decoder, line_embed, generator):\n","    super(EncoderDecoder, self).__init__()\n","\n","    self.encoder = encoder\n","    self.decoder = decoder\n","    self.line_embed = line_embed\n","    self.generator = generator\n","\n","  def forward(self, lines, line_lens):\n","    encoder_hiddens, encoder_finals = self.encode(lines, line_lens)\n","    del encoder_hiddens\n","    return self.reconstruct(encoder_finals, lines[:, :-1]), self.decode(encoder_finals)\n","\n","  def encode(self, lines, line_lens):\n","    return self.encoder(self.line_embed(lines), line_lens)\n","    \n","  def reconstruct(self, h0, lines):\n","    original = self.line_embed(lines)\n","    return self.decoder.forward_teacher(original,h0)\n","\n","  def decode(self, h0):\n","    target = self.line_embed(torch.tensor([SOS_INDEX]).repeat(h0.size()[1],1).to(device))\n","    return self.decoder.forward(target,h0,max_len)\n","\n","epochs = 3\n","lr = 1e-3\n","batch_size = 32\n","print_every = 100\n","max_len = dataset.max_seq_length\n","vocab_size = len(vocab)\n","embed_size = 256\n","hidden_size = 256\n","dropout = 0.2\n","gamma = 0.001\n","\n","train_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","valid_loader = data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n","\n","line_embed = nn.Embedding(vocab_size, embed_size)\n","encoder = Encoder(embed_size,hidden_size)\n","generator = GeneratorTransferredSampled(hidden_size,vocab_size, line_embed, gamma = gamma)\n","decoder = Decoder(embed_size, hidden_size, max_len=vocab_size, generator = generator,dropout=dropout)\n","model = EncoderDecoder(encoder, decoder, line_embed, generator).to(device)\n","optimizer_model = torch.optim.Adam(model.parameters(), lr=lr)\n","rec_loss = nn.NLLLoss(reduction=\"mean\",ignore_index = PAD_INDEX)\n","\n","for epoch in range(epochs):\n","  epoch_rec_loss = 0\n","  epoch_tokens = 0\n","  model.train()\n","  for i,(taylor_lines, taylor_len, drake_lines, drake_len) in enumerate(train_loader):\n","    lines = torch.cat((taylor_lines, drake_lines), 0).to(device)    \n","    line_lens = torch.cat((taylor_len, drake_len), 0).to(device)\n","\n","    # Train model\n","    rec_orig,dec_orig  = model(lines, line_lens)    \n","    loss_rec = rec_loss(input=rec_orig[2].permute(0,2,1), target=lines[:, 1:])\n","\n","    optimizer_model.zero_grad()\n","    loss_rec.backward()\n","    optimizer_model.step()\n","    \n","    epoch_rec_loss += loss_rec.item() * line_lens.sum().item()\n","    epoch_tokens += line_lens.sum().item()\n","\n","  print(\"Finished Training Epoch \", epoch)\n","  print(\"Training PPL\", np.exp(epoch_rec_loss / float(epoch_tokens)))\n","  val_loss = 0\n","  val_tokens = 0\n","\n","  for i,(taylor_lines, taylor_len, drake_lines, drake_len) in enumerate(valid_loader):\n","    lines = torch.cat((taylor_lines, drake_lines), 0).to(device)    \n","    line_lens = torch.cat((taylor_len, drake_len), 0).to(device)\n","\n","    rec_orig,dec_orig  = model(lines, line_lens)    \n","    loss_rec = rec_loss(input=rec_orig[2].permute(0,2,1), target=lines[:, 1:])\n","\n","    val_loss += loss_rec.item() * line_lens.sum().item()\n","    val_tokens += line_lens.sum().item()\n","  \n","  print(\"Valid PPL\", np.exp(val_loss / float(val_tokens)))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1304,"status":"ok","timestamp":1637644597289,"user":{"displayName":"Hammaad Adam","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03287553409115045279"},"user_tz":300},"id":"LO47V3RWEs6J","outputId":"f72bbef8-2c62-4ed8-c885-a61f06b605cd"},"outputs":[{"name":"stdout","output_type":"stream","text":["['\u003cs\u003e', 'i', \"'d\", 'hold', 'you', 'as', 'the', 'water', 'rushes', 'in', ',', 'if', 'i', 'could', 'dance', 'with', 'you', 'again', '\u003c/s\u003e', '\u003cpad\u003e', '\u003cpad\u003e', '\u003cpad\u003e', '\u003cpad\u003e', '\u003cpad\u003e', '\u003cpad\u003e', '\u003cpad\u003e', '\u003cpad\u003e', '\u003cpad\u003e', '\u003cpad\u003e', '\u003cpad\u003e', '\u003cpad\u003e', '\u003cpad\u003e']\n","['i', \"'m\", 'be', 'you', 'the', 'you', 'way', 'i', ',', 'the', 'i', 'you', \"'m\", 'be', ',', 'you', '\u003c/s\u003e', '\u003c/s\u003e', '\u003c/s\u003e', '\u003c/s\u003e', '\u003c/s\u003e', '\u003c/s\u003e', '\u003c/s\u003e', '\u003c/s\u003e', '\u003c/s\u003e', '\u003c/s\u003e', '\u003c/s\u003e', '\u003c/s\u003e', '\u003c/s\u003e', '\u003c/s\u003e', '\u003c/s\u003e']\n","['i', 'party', 'championships', 'championships', 'magnet', 'truest', 'championships', 'case', 'biggie', 'contracts', 'this', 'mission', '11/9/2018', 'remote', 'drop-top', 'worn', 'witches', 'championships', 'avoidin', 'magnet', 'podrug', 'bleach', 'velcro', 'sparkly', 'mission', 'championships', 'championships', 'reclinin', 'vsekh', 'championships', 'forgiveness', 'toughen']\n","31\n","32\n"]}],"source":["def lookup_words(x, vocab):\n","  return [vocab[i] for i in x]\n","\n","idx=5\n","print(lookup_words(lines[idx], vocab))\n","print(lookup_words(rec_orig[3][idx], vocab))\n","print(lookup_words(dec_orig[3][idx], vocab))\n","\n","print(len(lookup_words(rec_orig[3][idx], vocab)))\n","print(len(lookup_words(dec_orig[3][idx], vocab)))"]},{"cell_type":"markdown","metadata":{"id":"wg_FbKl-T9xd"},"source":["### Classifier"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":146,"status":"ok","timestamp":1637908244748,"user":{"displayName":"Katherine Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11324848797432579426"},"user_tz":300},"id":"rjDXvknvT_PM"},"outputs":[],"source":["class LSTMClassifier(nn.Module):\n","  def __init__(self, embed_size, hidden_size, vocab_size, LSTMlayers=1, dropout = 0.2):\n","    super(LSTMClassifier, self).__init__()\n","    self.embed = nn.Linear(vocab_size, embed_size)\n","    self.lstm = nn.LSTM(input_size=embed_size, hidden_size=hidden_size, num_layers=LSTMlayers, batch_first=True)\n","    self.dropout = nn.Dropout(dropout)\n","    self.fc1 =  nn.Linear(in_features=hidden_size, out_features=hidden_size+1)\n","    self.fc2 = nn.Linear(hidden_size+1, 1)\n","\n","  def forward(self, seq):\n","    seq = self.embed(seq)\n","    output, (hidden,cell) = self.lstm(seq)\n","    output = self.dropout(output)\n","    output = self.fc1(output[:,-1])\n","    output = F.relu(output)\n","    output = self.fc2(output)\n","    return output\n","    return output.reshape(output.size(1), -1)\n","    \n","class LSTMDiscriminator(nn.Module):\n","  def __init__(self, input_size, hidden_size, LSTMlayers=1, dropout = 0.2):\n","    super(LSTMDiscriminator, self).__init__()\n","    self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=LSTMlayers, batch_first=True)\n","    self.dropout = nn.Dropout(dropout)\n","    self.fc1 =  nn.Linear(in_features=hidden_size, out_features=hidden_size+1)\n","    self.fc2 = nn.Linear(hidden_size+1, 1)\n","\n","  def forward(self, seq):\n","    output, (hidden,cell) = self.lstm(seq)\n","    output = self.dropout(output)\n","    output = self.fc1(output[:,-1])\n","    output = F.relu(output)\n","    output = self.fc2(output)\n","    return output\n","    return output.reshape(output.size(1), -1)"]},{"cell_type":"markdown","metadata":{"id":"_xuubDG0F5i3"},"source":["### Training"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":166,"status":"ok","timestamp":1637909791120,"user":{"displayName":"Katherine Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11324848797432579426"},"user_tz":300},"id":"yjs1mzrQEyef"},"outputs":[],"source":["class TSTModel(nn.Module):\n","  def __init__(self, max_len, vocab_size, embed_size, hidden_size_z, hidden_size_y, line_embed, encoder, generator, decoder, classifier):\n","    super(TSTModel, self).__init__()\n","\n","    self.hidden_size = hidden_size_y + hidden_size_z\n","\n","    self.encoder = encoder\n","    self.generator = generator\n","    self.decoder = decoder\n","    self.classifier = classifier\n","\n","    self.line_embed = line_embed\n","    self.y_embed_enc = nn.Embedding(2,hidden_size_y)\n","    self.y_embed_gen = nn.Embedding(2,hidden_size_y)\n","\n","    self.max_len = max_len\n","    self.vocab_size = vocab_size\n","    self.embed_size = embed_size\n","    self.hidden_size_z = hidden_size_z\n","    self.hidden_size_y = hidden_size_y\n","\n","  def forward(self, lines, line_lens, labels):\n","    encoded_lines = self.encode(lines, line_lens, labels)\n","    z = encoded_lines[1][-1][:,self.hidden_size_y:]\n","\n","    h0_orig = torch.cat((self.y_embed_gen(labels),z), 1)[None,:]\n","    h0_tsf = torch.cat((self.y_embed_gen(1-labels),z), 1)[None,:]\n","\n","    # Decode back into original form for reconstruction\n","    rec_orig = self.reconstruct(h0_orig, lines[:, :-1])\n","\n","    # Decode into original and transferred forms for classification\n"," \n","    decode_orig = self.decode(h0_orig)\n","    decode_tsf = self.decode(h0_tsf)\n","    \n","    half = int(lines.size(0) / 2)\n","\n","    discrim1_input = torch.cat((rec_orig[4][:half], decode_tsf[4][half:]))\n","    discrim0_input = torch.cat((rec_orig[4][half:], decode_tsf[4][:half]))\n","\n","    classifier_lines = torch.cat((decode_orig[2], decode_tsf[2], F.one_hot(lines[:,1:], self.vocab_size).to(torch.float)), 0)\n","    pred_class = self.classifier(classifier_lines)\n","\n","    # return rec_orig, decode_orig\n","    return rec_orig, pred_class, decode_orig, decode_tsf, discrim0_input, discrim1_input \n","\n","  def forward_beam(self,lines, line_lens, labels):\n","    encoded_lines = self.encode(lines, line_lens, labels)\n","    z = encoded_lines[1][-1][:,self.hidden_size_y:]\n","\n","    h0_orig = torch.cat((self.y_embed_gen(labels),z), 1)[None,:]\n","    h0_tsf = torch.cat((self.y_embed_gen(1-labels),z), 1)[None,:]\n","\n","    decode_orig = self.decode_beam(h0_orig)\n","    decode_tsf = self.decode_beam(h0_tsf)\n","\n","    classifier_lines = torch.cat((decode_orig[2][:,-1,:,:], \n","                                  decode_tsf[2][:,-1,:,:], \n","                                  F.one_hot(lines, self.vocab_size).to(torch.float)), 0)\n","    pred_class = self.classifier(classifier_lines)\n","\n","    return pred_class, decode_orig, decode_tsf\n","\n","  def encode(self, lines, line_lens, labels):\n","    init_state = torch.cat((self.y_embed_enc(labels), torch.zeros((len(lines),self.hidden_size_z), device=device)), 1)[None,:].to(device)\n","    return self.encoder(self.line_embed(lines), line_lens, init_state)\n","\n","  def reconstruct(self, h0, lines):\n","    original = self.line_embed(lines)\n","    return self.decoder.forward_teacher(original,h0)\n","\n","  def decode(self, h0):\n","    target = self.line_embed(torch.tensor([SOS_INDEX]).repeat(h0.size()[1],1).to(device))\n","    return self.decoder.forward(target,h0,self.max_len)"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":123,"status":"ok","timestamp":1637909797265,"user":{"displayName":"Katherine Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11324848797432579426"},"user_tz":300},"id":"5dN3kpsaZpFX"},"outputs":[],"source":["class TSTModelAttention(nn.Module):\n","  def __init__(self, max_len, vocab_size, embed_size, hidden_size_z, hidden_size_y, line_embed, encoder, generator, decoder, classifier):\n","    super(TSTModelAttention, self).__init__()\n","\n","    self.hidden_size = hidden_size_y + hidden_size_z\n","\n","    self.encoder = encoder\n","    self.generator = generator\n","    self.decoder = decoder\n","    self.classifier = classifier\n","\n","    self.line_embed = line_embed\n","    self.y_embed_enc = nn.Embedding(2,hidden_size_y)\n","    self.y_embed_gen = nn.Embedding(2,hidden_size_y)\n","\n","    self.max_len = max_len\n","    self.vocab_size = vocab_size\n","    self.embed_size = embed_size\n","    self.hidden_size_z = hidden_size_z\n","    self.hidden_size_y = hidden_size_y\n","\n","  def forward(self, lines, line_lens, labels):\n","\n","    src_mask = (lines != PAD_INDEX).unsqueeze(-2)\n","    encoder_hidden, encoder_finals = self.encode(lines, line_lens, labels)\n","\n","    z = encoder_finals[-1][:,self.hidden_size_y:]\n","\n","    h0_orig = torch.cat((self.y_embed_gen(labels),z), 1)[None,:]\n","    h0_tsf = torch.cat((self.y_embed_gen(1-labels),z), 1)[None,:]\n","\n","    # Decode back into original form for reconstruction\n","    rec_orig = self.reconstruct(encoder_hidden, h0_orig, lines[:, :-1], src_mask)\n","\n","    # Decode into original and transferred forms for classification\n","    decode_orig = self.decode(encoder_hidden, h0_orig, src_mask)\n","    decode_tsf = self.decode(encoder_hidden, h0_tsf, src_mask)\n","    \n","    half = int(lines.size(0) / 2)\n","\n","    discrim1_input = torch.cat((rec_orig[4][:half], decode_tsf[4][half:]))\n","    discrim0_input = torch.cat((rec_orig[4][half:], decode_tsf[4][:half]))\n","\n","    classifier_lines = torch.cat((decode_orig[2], decode_tsf[2], F.one_hot(lines[:,1:], self.vocab_size).to(torch.float)), 0)\n","    pred_class = self.classifier(classifier_lines)\n","\n","    return rec_orig, pred_class, decode_orig, decode_tsf, discrim0_input, discrim1_input \n","    \n","  def forward_beam(self,lines, line_lens, labels):\n","    encoded_lines = self.encode(lines, line_lens, labels)\n","    z = encoded_lines[1][-1][:,self.hidden_size_y:]\n","\n","    h0_orig = torch.cat((self.y_embed_gen(labels),z), 1)[None,:]\n","    h0_tsf = torch.cat((self.y_embed_gen(1-labels),z), 1)[None,:]\n","\n","    decode_orig = self.decode_beam(h0_orig)\n","    decode_tsf = self.decode_beam(h0_tsf)\n","\n","    classifier_lines = torch.cat((decode_orig[2][:,-1,:,:], \n","                                  decode_tsf[2][:,-1,:,:], \n","                                  F.one_hot(lines, self.vocab_size).to(torch.float)), 0)\n","    pred_class = self.classifier(classifier_lines)\n","\n","    return pred_class, decode_orig, decode_tsf\n","\n","  def encode(self, lines, line_lens, labels):\n","    init_state = torch.cat((self.y_embed_enc(labels), torch.zeros((len(lines),self.hidden_size_z), device=device)), 1)[None,:].to(device)\n","    return self.encoder(self.line_embed(lines), line_lens, init_state)\n","\n","  def reconstruct(self, encoder_hidden, h0, lines, src_mask):\n","    original = self.line_embed(lines)\n","    return self.decoder.forward_teacher(original,encoder_hidden, h0, src_mask)\n","\n","  def decode(self, encoder_hidden, h0, src_mask):\n","    target = self.line_embed(torch.tensor([SOS_INDEX]).repeat(h0.size()[1],1).to(device))\n","    return self.decoder.forward(target, encoder_hidden, h0, src_mask, self.max_len)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"O8jaGcIiNQUi"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch Step: 0 Loss: 3.154443\n","Epoch Step: 100 Loss: 3.584158\n","Epoch Step: 200 Loss: 4.268278\n","Finished Training Epoch  0\n","Training PPL 33.266229609572186\n","Adversarial Loss 281.82122661173344\n","Discriminator Loss 285.62482821941376\n"]},{"ename":"AttributeError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-24-c1ee06973976\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0mrec_orig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_orig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_tsf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_fake0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_fake1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 150\u001b[0;31m     \u001b[0mpred_class_beam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_orig_beam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_tsf_beam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_beam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0mloss_rec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrec_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrec_orig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;31m# loss_class = class_loss(sigmoid(pred_class), classifier_labels.to(torch.float))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m-\u003e 1178\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u003e\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'TSTModel' object has no attribute 'forward_beam'"]}],"source":["# attention = False\n","# classify = False\n","# discriminate = True\n","\n","# epochs = 1\n","# class_epochs = 0\n","# lr = 1e-3\n","# batch_size = 32\n","# print_every = 100\n","\n","# max_len = dataset.max_seq_length\n","# vocab_size = len(vocab)\n","# embed_size = 256\n","# hidden_size_z = 256\n","# hidden_size_y = 128\n","# hidden_size = hidden_size_z + hidden_size_y\n","# dropout = 0.2\n","# gamma = 0.001\n","\n","# TAYLOR_STYLE=1 # for information only, don't change\n","# DRAKE_STYLE=0  # for information only, don't change\n","# train_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","# valid_loader = data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n","# test_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n","\n","# line_embed = nn.Embedding(vocab_size, embed_size)\n","# encoder = Encoder(embed_size,hidden_size)\n","# generator = GeneratorTransferredSampled(hidden_size,vocab_size, line_embed, gamma = gamma)\n","# if attention:\n","#   attention_mech = BahdanauAttention(hidden_size, key_size=hidden_size)\n","#   decoder = AttentionDecoder(embed_size, hidden_size, attention=attention_mech, max_len=vocab_size, generator = generator,dropout=dropout)\n","# else:\n","#   decoder = Decoder(embed_size, hidden_size, max_len=vocab_size, generator = generator, dropout=dropout)\n","\n","# classifier = LSTMClassifier(embed_size, hidden_size_z, vocab_size, dropout =0.2)\n","\n","# discriminator0 = LSTMDiscriminator(hidden_size, hidden_size).to(device)\n","# discriminator1 = LSTMDiscriminator(hidden_size, hidden_size).to(device)\n","\n","# if attention:\n","#   model = TSTModelAttention(dataset.max_seq_length, vocab_size, embed_size, hidden_size_z, hidden_size_y, line_embed, encoder, generator, decoder, classifier).to(device)\n","# else:\n","#   model = TSTModel(dataset.max_seq_length, vocab_size, embed_size, hidden_size_z, hidden_size_y, line_embed, encoder, generator, decoder, classifier).to(device)\n","\n","# optimizer_model = torch.optim.Adam(model.parameters(), lr=lr) \n","# optimizer_discr = torch.optim.Adam(list(discriminator0.parameters()) + list(discriminator1.parameters()), lr=lr) \n","\n","# # rec_loss = nn.CrossEntropyLoss(reduction=\"mean\",ignore_index = PAD_INDEX)\n","# rec_loss = nn.NLLLoss(reduction=\"mean\",ignore_index = PAD_INDEX)\n","# class_loss = nn.BCEWithLogitsLoss(reduction=\"mean\")\n","# discr_loss = nn.BCEWithLogitsLoss(reduction=\"mean\")\n","# sigmoid = nn.Sigmoid()\n","\n","epoch_losses = []\n","for epoch in range(epochs):\n","  epoch_loss = 0\n","  epoch_class_loss = 0\n","  epoch_rec_loss = 0\n","  epoch_adv_loss = 0\n","  epoch_loss_d = 0\n","  epoch_tokens = 0\n","  model.train()\n","  for i,(taylor_lines, taylor_len, drake_lines, drake_len) in enumerate(train_loader):\n","    lines = torch.cat((taylor_lines, drake_lines), 0).to(device)    \n","    line_lens = torch.cat((taylor_len, drake_len), 0).to(device)\n","    labels = torch.cat((torch.ones(size=(len(taylor_lines),), dtype=torch.int32),torch.zeros(size=(len(drake_lines),),dtype=torch.int32))).to(device)\n","    classifier_labels = torch.cat((labels,1-labels, labels)).unsqueeze(1)\n","    \n","    fake_labels = torch.cat((torch.zeros(size=(len(taylor_lines),), dtype=torch.int32),torch.ones(size=(len(taylor_lines),),dtype=torch.int32))).to(device)\n","    fake_labels = fake_labels.unsqueeze(1)\n","\n","    # Train discriminator\n","\n","    if discriminate:\n","      rec_orig, pred_class, decode_orig, decode_tsf, pred_fake0, pred_fake1 = model(lines, line_lens, labels)\n","      pred_fake0 = discriminator0(pred_fake0)\n","      pred_fake1 = discriminator1(pred_fake1)\n","      loss_d0 = discr_loss(pred_fake0, fake_labels.to(torch.float))\n","      loss_d1 = discr_loss(pred_fake1, fake_labels.to(torch.float))\n","      loss_d = loss_d0 + loss_d1\n","\n","      optimizer_discr.zero_grad()\n","      loss_d.backward()\n","      optimizer_discr.step()\n","\n","    # Train model\n","\n","    rec_orig, pred_class, decode_orig, decode_tsf, pred_fake0, pred_fake1 = model(lines, line_lens, labels)\n","\n","    loss_rec = rec_loss(input=rec_orig[2].permute(0,2,1), target=lines[:, 1:])\n","\n","    loss = loss_rec\n","    \n","    if discriminate:\n","      pred_fake0 = discriminator0(pred_fake0)\n","      pred_fake1 = discriminator1(pred_fake1)\n","      loss_adv0 = class_loss(pred_fake0[len(drake_lines):], fake_labels[len(drake_lines):].to(torch.float))\n","      loss_avd1 = class_loss(pred_fake1[len(taylor_lines):], fake_labels[len(taylor_lines):].to(torch.float))\n","      loss -= (loss_adv0 + loss_avd1)\n","    \n","    if classify:\n","      loss_class = class_loss(sigmoid(pred_class), classifier_labels.to(torch.float))\n","      loss += loss_class\n","\n","    optimizer_model.zero_grad()\n","    loss.backward()\n","    optimizer_model.step()\n","    \n","    epoch_loss += loss.item()\n","    epoch_rec_loss += loss_rec.item() * line_lens.sum().item()\n","    epoch_tokens += line_lens.sum().item()\n","\n","    if discriminate:\n","      epoch_loss_d += loss_d.item()\n","      epoch_adv_loss += (loss_adv0.item() + loss_avd1.item())\n","      \n","    if classify:\n","      epoch_class_loss += loss_class.item()\n","\n","    if model.training and i % print_every == 0:\n","      print(\"Epoch Step: %d Loss: %f\" % (i, loss.item()))\n","  \n","  epoch_losses.append(epoch_loss)\n","  print(\"Finished Training Epoch \", epoch)\n","  print(\"Training PPL\", np.exp(epoch_rec_loss / float(epoch_tokens)))\n","\n","  if discriminate:\n","    print(\"Adversarial Loss\", epoch_adv_loss)\n","    print(\"Discriminator Loss\", epoch_loss_d)\n","  \n","  else:\n","    print(\"Classification Loss\", epoch_class_loss)\n","\n","  val_loss = 0\n","  val_tokens = 0\n","  val_class_loss = 0\n","  correct_pred = 0\n","  correct_pred_all = 0\n","\n","  for i,(taylor_lines, taylor_len, drake_lines, drake_len) in enumerate(valid_loader):\n","    lines = torch.cat((taylor_lines, drake_lines), 0).to(device)    \n","    line_lens = torch.cat((taylor_len, drake_len), 0).to(device)\n","    labels = torch.cat((torch.ones(size=(len(taylor_lines),), dtype=torch.int32),torch.zeros(size=(len(drake_lines),),dtype=torch.int32))).to(device)\n","    classifier_labels = torch.cat((labels,1-labels, labels)).unsqueeze(1)\n","    \n","    fake_labels = torch.cat((torch.zeros(size=(len(taylor_lines),), dtype=torch.int32),torch.ones(size=(len(taylor_lines),),dtype=torch.int32))).to(device)\n","    fake_labels = fake_labels.unsqueeze(1)\n","\n","    rec_orig, pred_class, decode_orig, decode_tsf, pred_fake0, pred_fake1 = model(lines, line_lens, labels)\n","    pred_class_beam, decode_orig_beam, decode_tsf_beam = model.forward_beam(lines, line_lens, labels)\n","    loss_rec = rec_loss(input=rec_orig[2].permute(0,2,1), target=lines[:, 1:])\n","    # loss_class = class_loss(sigmoid(pred_class), classifier_labels.to(torch.float))\n","\n","    val_loss += loss_rec.item() * line_lens.sum().item()\n","    val_tokens += line_lens.sum().item()\n","    # val_class_loss += loss_class.item()*classifier_labels.size(0)\n","\n","    correct_pred += torch.sum(1*(1*(torch.sigmoid(pred_class[-len(lines):])\u003e=0.5) == labels.unsqueeze(1)))\n","    correct_pred_all += torch.sum(1*(1*(torch.sigmoid(pred_class)\u003e=0.5) == classifier_labels))\n","  \n","  print(\"Valid PPL\", np.exp(val_loss / float(val_tokens)))\n","  if classify:\n","    print(\"Valid Classification Accuracy on Generated\", correct_pred / (2.*len(valid_dataset)))\n","    print(\"Valid Classification Accuracy on All\", correct_pred_all / (3.*2.*len(valid_dataset)))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":200,"status":"ok","timestamp":1637902409467,"user":{"displayName":"Hammaad Adam","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03287553409115045279"},"user_tz":300},"id":"Y6lRAikKW0EZ","outputId":"712689bd-cb29-475f-99e9-8e873afc4bc6"},"outputs":[{"name":"stdout","output_type":"stream","text":["['\u003cs\u003e', 'ca', \"n't\", 'figure', 'out', 'how', 'i', \"'m\", 'supposed', 'to', 'make', 'it', 'now', ',', 'tell', 'me', 'anything', 'but', 'the', 'truth', '\u003c/s\u003e', '\u003cpad\u003e', '\u003cpad\u003e', '\u003cpad\u003e', '\u003cpad\u003e', '\u003cpad\u003e', '\u003cpad\u003e', '\u003cpad\u003e', '\u003cpad\u003e', '\u003cpad\u003e', '\u003cpad\u003e', '\u003cpad\u003e']\n","['ca', \"n't\", 'figure', 'out', 'how', 'i', \"'m\", 'supposed', 'to', 'make', 'it', 'now', ',', 'tell', 'me', 'anything', 'but', 'the', 'truth', '\u003c/s\u003e', 'the', 'truth', '\u003c/s\u003e', '\u003c/s\u003e', '\u003c/s\u003e', '\u003c/s\u003e', '\u003c/s\u003e', '\u003c/s\u003e', '\u003c/s\u003e', '\u003c/s\u003e', '\u003c/s\u003e']\n","['ca', \"n't\", 'figure', 'out', 'how', 'i', \"'m\", 'supposed', 'to', 'make', 'it', 'now', ',', 'tell', 'me', 'anything', 'but', 'the', 'truth', '\u003c/s\u003e', 'the', 'truth', '\u003c/s\u003e', 'the', 'truth', '\u003c/s\u003e', 'the', 'truth', '\u003c/s\u003e', 'the', 'truth']\n","['ca', \"n't\", 'figure', 'out', 'how', 'i', \"'m\", 'supposed', 'to', 'make', 'it', 'now', ',', 'tell', 'me', 'anything', 'but', 'the', 'truth', '\u003c/s\u003e', 'the', 'truth', '\u003c/s\u003e', 'the', 'truth', '\u003c/s\u003e', '\u003c/s\u003e', '\u003c/s\u003e', 'the', 'truth', '\u003c/s\u003e']\n"]}],"source":["idx=62\n","\n","def lookup_words(x, vocab):\n","  return [vocab[i] for i in x]\n","\n","print(lookup_words(lines[idx], vocab))\n","print(lookup_words(rec_orig[3][idx], vocab))\n","print(lookup_words(decode_orig[3][idx], vocab))\n","print(lookup_words(decode_tsf[3][idx], vocab))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S8YQ7Ephl4Qf"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":441,"status":"ok","timestamp":1637902460006,"user":{"displayName":"Hammaad Adam","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03287553409115045279"},"user_tz":300},"id":"J9ATTK09gakn","outputId":"0f2ccc72-6aee-4255-f525-95a145b6bcd4"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.0\n","0.0\n"]}],"source":["correct_pred = 0\n","correct_pred_all = 0\n","model.eval()\n","with torch.no_grad():\n","  for i,(taylor_lines, taylor_len, drake_lines, drake_len) in enumerate(valid_loader):\n","    lines = torch.cat((taylor_lines, drake_lines), 0).to(device)    \n","    line_lens = torch.cat((taylor_len, drake_len), 0).to(device)\n","    labels = torch.cat((torch.ones(size=(len(taylor_lines),), dtype=torch.int32),torch.zeros(size=(len(drake_lines),),dtype=torch.int32))).to(device)\n","    classifier_labels = torch.cat((labels,1-labels, labels)).unsqueeze(1)\n","\n","    rec_orig,pred_class, decode_orig,decode_tsf,d0,d1 =  model(lines, line_lens, labels)\n","\n","    break\n","    correct_pred += torch.sum(1*(1*(torch.sigmoid(pred_class[-len(lines):])\u003e=0.5) == labels.unsqueeze(1)))\n","    correct_pred_all += torch.sum(1*(1*(torch.sigmoid(pred_class)\u003e=0.5) == classifier_labels))\n","  print(correct_pred / (2.*len(valid_dataset)))\n","  print(correct_pred_all / (3.*2.*len(valid_dataset)))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":198,"status":"ok","timestamp":1637902777158,"user":{"displayName":"Hammaad Adam","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03287553409115045279"},"user_tz":300},"id":"O_sa15FklmJj","outputId":"467aa5ed-b75c-450a-932b-10c44fe97d4d"},"outputs":[{"data":{"text/plain":["tensor([[4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [4.9490e-01],\n","        [9.5989e-01],\n","        [9.4225e-01],\n","        [9.4865e-01],\n","        [9.3976e-01],\n","        [9.0224e-01],\n","        [6.4949e-01],\n","        [9.0019e-01],\n","        [9.5912e-01],\n","        [3.9337e-02],\n","        [9.5871e-01],\n","        [9.3805e-01],\n","        [9.5323e-01],\n","        [9.2548e-01],\n","        [9.5251e-01],\n","        [2.7409e-01],\n","        [9.4960e-01],\n","        [1.0250e-02],\n","        [6.8395e-01],\n","        [7.9567e-01],\n","        [9.5212e-01],\n","        [9.6040e-01],\n","        [9.5598e-01],\n","        [9.4153e-01],\n","        [9.5696e-01],\n","        [9.5065e-01],\n","        [9.5439e-01],\n","        [9.1262e-01],\n","        [9.5792e-01],\n","        [9.4087e-01],\n","        [2.0821e-03],\n","        [9.3857e-01],\n","        [6.4039e-01],\n","        [1.0340e-03],\n","        [9.5957e-01],\n","        [8.2407e-04],\n","        [7.9862e-04],\n","        [9.8823e-02],\n","        [4.6175e-04],\n","        [1.9985e-03],\n","        [5.7675e-03],\n","        [1.2901e-03],\n","        [7.8583e-04],\n","        [5.4201e-04],\n","        [2.7479e-03],\n","        [4.6740e-03],\n","        [1.4192e-03],\n","        [5.8466e-03],\n","        [8.3855e-01],\n","        [1.2381e-03],\n","        [8.7277e-01],\n","        [1.0851e-03],\n","        [1.8811e-03],\n","        [9.5208e-01],\n","        [5.5568e-04],\n","        [5.3402e-04],\n","        [1.2718e-01],\n","        [4.2276e-02],\n","        [9.1930e-04],\n","        [5.7926e-04],\n","        [8.8280e-04],\n","        [1.1600e-03],\n","        [3.0053e-03],\n","        [4.5253e-02],\n","        [8.0722e-04]], device='cuda:0')"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["torch.sigmoid(pred_class)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A6LmSVOGr4Ha"},"outputs":[],"source":["def lookup_words(x, vocab):\n","  return [vocab[i] for i in x]\n","\n","with torch.no_grad():\n","  for i,(taylor_lines, taylor_len, drake_lines, drake_len) in enumerate(train_loader):\n","    lines = torch.cat((taylor_lines, drake_lines), 0).to(device)    \n","    line_lens = torch.cat((taylor_len, drake_len), 0).to(device)\n","    labels = torch.cat((torch.ones(size=(len(taylor_lines),), dtype=torch.int32),torch.zeros(size=(len(drake_lines),),dtype=torch.int32))).to(device)\n","    # classifier_labels = torch.cat((labels,1-labels, labels)).unsqueeze(1)\n","\n","    # rec_orig,pred_class,decode_orig,decode_tsf,_,_ =  model(lines, line_lens, labels)\n","\n","    rec_orig,pred_class, dec_orig,dec_tsf,d0,d1 =  model(lines, line_lens, labels)\n","    \n","    break\n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["m7FQaWQbojAb","S3i2hvtYoo30","t2mg2wYk30dv"],"machine_shape":"hm","name":"Computing Training Loop and Beam Search.ipynb","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}