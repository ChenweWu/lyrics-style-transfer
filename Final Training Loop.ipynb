{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Final Training Loop.ipynb","provenance":[],"collapsed_sections":["m7FQaWQbojAb","S3i2hvtYoo30","ynBTHa5grUr7","6-6cHH91VbdK","wg_FbKl-T9xd","ALXZL0AibQxr"],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"m7FQaWQbojAb"},"source":["### Import and Setup"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wyz5JwHDopli","executionInfo":{"status":"ok","timestamp":1638379756355,"user_tz":300,"elapsed":4863,"user":{"displayName":"Katherine Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11324848797432579426"}},"outputId":"3bc1941c-986f-4072-bbc3-a75c72e4466d"},"source":["!pip install unidecode"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting unidecode\n","  Downloading Unidecode-1.3.2-py3-none-any.whl (235 kB)\n","\u001b[K     |████████████████████████████████| 235 kB 5.3 MB/s \n","\u001b[?25hInstalling collected packages: unidecode\n","Successfully installed unidecode-1.3.2\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T8q-CET4os1p","executionInfo":{"status":"ok","timestamp":1638379775087,"user_tz":300,"elapsed":18743,"user":{"displayName":"Katherine Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11324848797432579426"}},"outputId":"6ac9369e-bf13-40a8-d7f4-f84a4d7b55e9"},"source":["import json\n","import re\n","from unidecode import unidecode\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","import nltk\n","nltk.download('punkt')\n","from collections import Counter\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"0ELBqq8kpAZS"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","assert device == \"cuda\"  \n","\n","seed = 42\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S3i2hvtYoo30"},"source":["### Data"]},{"cell_type":"code","metadata":{"id":"OGzVRiwZowGv"},"source":["f = open('/content/drive/Shareddrives/MIT NLP 8.864/Data/drake.json')\n","drake = json.load(f)\n","f.close()\n","\n","f = open('/content/drive/Shareddrives/MIT NLP 8.864/Data/tswift.json')\n","taylor = json.load(f)\n","f.close()\n","\n","drake = [drake['songs'][i]['lyrics'] for i in range(len(drake['songs']))]\n","taylor = [taylor['songs'][i]['lyrics'] for i in range(len(taylor['songs']))]\n","\n","taylor_lyrics = [re.sub('\\u2005', ' ', re.sub(r'[\\(\\[].*?[\\)\\]]', '', taylor[i])).split('\\n') for i in range(len(taylor))]\n","taylor_lyrics = [[unidecode(i) for i in taylor_lyrics[j]] for j in range(len(taylor_lyrics))]\n","taylor_lyrics = [[re.sub('\\d+EmbedShare URLCopyEmbedCopy', '', i) for i in taylor_lyrics[j]] for j in range(len(taylor_lyrics))]\n","taylor_lyrics = [[re.sub('\\d+.EmbedShare URLCopyEmbedCopy', '', i) for i in taylor_lyrics[j]] for j in range(len(taylor_lyrics))]\n","taylor_lyrics = [[re.sub('EmbedShare URLCopyEmbedCopy', '', i) for i in taylor_lyrics[j]] for j in range(len(taylor_lyrics))]\n","taylor_lyrics = [[i for i in taylor_lyrics[j] if i != ''] for j in range(len(taylor_lyrics))]\n","\n","drake_lyrics = [re.sub('\\u2005', ' ', re.sub(r'[\\(\\[].*?[\\)\\]]', '', drake[i])).split('\\n') for i in range(len(drake))]\n","drake_lyrics = [[unidecode(i) for i in drake_lyrics[j]] for j in range(len(drake_lyrics))]\n","drake_lyrics = [[re.sub('\\d+EmbedShare URLCopyEmbedCopy', '', i) for i in drake_lyrics[j]] for j in range(len(drake_lyrics))]\n","drake_lyrics = [[re.sub('\\d+.EmbedShare URLCopyEmbedCopy', '', i) for i in drake_lyrics[j]] for j in range(len(drake_lyrics))]\n","drake_lyrics = [[re.sub('EmbedShare URLCopyEmbedCopy', '', i) for i in drake_lyrics[j]] for j in range(len(drake_lyrics))]\n","drake_lyrics = [[i for i in drake_lyrics[j] if i != ''] for j in range(len(drake_lyrics))]\n","\n","taylor_lyrics = [[line1 + ', ' + line2 for line1,line2 in zip(song[0::2], song[1::2])] for song in taylor_lyrics]\n","drake_lyrics = [[line1 + ', ' + line2 for line1,line2 in zip(song[0::2], song[1::2])] for song in drake_lyrics]\n","\n","drake_tokenized = [[word_tokenize(drake_lyrics[i][j]) for j in range(len(drake_lyrics[i]))] for i in range(len(drake_lyrics))]\n","taylor_tokenized = [[word_tokenize(taylor_lyrics[i][j]) for j in range(len(taylor_lyrics[i]))] for i in range(len(taylor_lyrics))]\n","\n","drake_tokenized = [[[word.lower() for word in line] for line in song] for song in drake_tokenized]\n","taylor_tokenized = [[[word.lower() for word in line] for line in song] for song in taylor_tokenized]\n","\n","drake_length = sum([[len(sent) for sent in song] for song in drake_tokenized], [])\n","taylor_length = sum([[len(sent) for sent in song] for song in taylor_tokenized], [])\n","\n","drake_lyrics = sum([[sent for sent in song if (len(sent) >= 10 and len (sent) <= 30)] for song in drake_tokenized], [])\n","taylor_lyrics = sum([[sent for sent in song if (len(sent) >= 10 and len (sent) <= 30)] for song in taylor_tokenized], [])\n","\n","taylor_vocab = sum(taylor_lyrics,[])\n","drake_vocab = sum(drake_lyrics,[])\n","\n","def unique(list1):\n","     \n","    # insert the list to the set\n","    list_set = set(list1)\n","    # convert the set to the list\n","    unique_list = (list(list_set))\n","    return unique_list\n","\n","vocab = taylor_vocab + drake_vocab\n","vocab_counts = Counter(vocab)\n","vocab = unique(vocab)\n","vocab = ['<pad>','<unk>','<s>', '</s>'] + vocab\n","\n","from torch.utils import data\n","import torch\n","\n","# These IDs are reserved.\n","MAX_SENT_LENGTH = 30\n","MAX_SENT_LENGTH_PLUS_SOS_EOS = 32\n","PAD_INDEX = 0\n","UNK_INDEX = 1\n","SOS_INDEX = 2\n","EOS_INDEX = 3\n","RARE_WORD_TRESHOLD = 0\n","\n","vocab_counts['<pad>'] = RARE_WORD_TRESHOLD + 1\n","vocab_counts['<unk>'] = RARE_WORD_TRESHOLD + 1\n","vocab_counts['<s>'] = RARE_WORD_TRESHOLD + 1\n","vocab_counts['</s>'] = RARE_WORD_TRESHOLD + 1\n","\n","class TSTDataset(data.Dataset):\n","    def __init__(self, taylor_sentences, drake_sentences, vocab, vocab_counts, sampling=1.):\n","        self.taylor_sentences = taylor_sentences[:int(len(taylor_sentences) * sampling)]\n","        self.drake_sentences = drake_sentences[:int(len(drake_sentences) * sampling)]\n","\n","        self.max_seq_length = MAX_SENT_LENGTH_PLUS_SOS_EOS\n","        self.vocab = vocab\n","        self.vocab_counts = vocab_counts\n","\n","        self.v2id = {v : i for i, v in enumerate(self.vocab)}\n","        self.id2v = {val : key for key, val in self.v2id.items()}\n","    \n","    def __len__(self):\n","        return min(len(self.taylor_sentences), len(self.drake_sentences))\n","    \n","    def __getitem__(self, index):\n","        taylor_sent = self.taylor_sentences[index]\n","        taylor_len = len(taylor_sent) + 2   # add <s> and </s> to each sentence\n","        taylor_id = []\n","        for w in taylor_sent:\n","            if w not in self.vocab:\n","                w = '<unk>'\n","            if vocab_counts[w] <= RARE_WORD_TRESHOLD:\n","                w = '<unk>'\n","            taylor_id.append(self.v2id[w])\n","\n","        taylor_id = ([SOS_INDEX] + taylor_id + [EOS_INDEX] + [PAD_INDEX] *\n","                  (self.max_seq_length - taylor_len))\n","\n","        drake_sent = self.drake_sentences[index]\n","        drake_len = len(drake_sent) + 2   # add <s> and </s> to each sentence\n","        drake_id = []\n","        for w in drake_sent:\n","            if w not in self.vocab:\n","                w = '<unk>'\n","            if vocab_counts[w] <= RARE_WORD_TRESHOLD:\n","                w = '<unk>'\n","            drake_id.append(self.v2id[w])\n","\n","        drake_id = ([SOS_INDEX] + drake_id + [EOS_INDEX] + [PAD_INDEX] *\n","                  (self.max_seq_length - drake_len))\n","\n","        return torch.tensor(taylor_id), taylor_len, torch.tensor(drake_id), drake_len\n","\n","dataset = TSTDataset(taylor_lyrics, drake_lyrics, vocab, vocab_counts)\n","\n","test_pct = 0.2\n","valid_pct = 0.1\n","\n","train_dataset, test_dataset = torch.utils.data.random_split(dataset, [int(len(dataset)*(1-test_pct)),len(dataset)-int(len(dataset)*(1-test_pct))])\n","valid_dataset, train_dataset = torch.utils.data.random_split(train_dataset, [int(len(dataset)*valid_pct),len(train_dataset)-int(len(dataset)*valid_pct)])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ynBTHa5grUr7"},"source":["### Encoder"]},{"cell_type":"code","metadata":{"id":"FieXza5erYUH"},"source":["from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","\n","class Encoder(nn.Module):\n","  def __init__(self, input_size, hidden_size, dropout=0.):\n","    \"\"\"\n","    Inputs: \n","      - `input_size`: an int representing the RNN input size.\n","      - `hidden_size`: an int representing the RNN hidden size.\n","      - `dropout`: a float representing the dropout rate during training. Note\n","          that for 1-layer RNN this has no effect since dropout only applies to\n","          outputs of intermediate layers.\n","    \"\"\"\n","    super(Encoder, self).__init__()\n","    self.rnn = nn.GRU(input_size, hidden_size, num_layers=1, batch_first=True,\n","                      dropout=dropout, bidirectional=False)\n","\n","  def forward(self, inputs, lengths, init_state=None):\n","    \"\"\"\n","    Inputs:\n","      - `inputs`: a 3d-tensor of shape (batch_size, max_seq_length, embed_size)\n","          representing a batch of padded embedded word vectors of source\n","          sentences.\n","      - `lengths`: a 1d-tensor of shape (batch_size,) representing the sequence\n","          lengths of `inputs`.\n","\n","    Returns:\n","      - `outputs`: a 3d-tensor of shape\n","        (batch_size, max_seq_length, hidden_size).\n","      - `finals`: a 3d-tensor of shape (num_layers, batch_size, hidden_size).\n","      Hint: `outputs` and `finals` are both standard GRU outputs. Check:\n","      https://pytorch.org/docs/stable/nn.html#gru\n","    \"\"\"\n","    # Our variable-length inputs are padded to the same length for batching\n","    # Here we \"pack\" them for computational efficiency (see note below)\n","    packed = pack_padded_sequence(inputs, lengths.cpu(), batch_first=True,\n","                                  enforce_sorted=False)\n","    outputs, finals = self.rnn(packed, init_state)\n","    outputs, _ = pad_packed_sequence(outputs, batch_first=True,\n","                                     total_length=MAX_SENT_LENGTH_PLUS_SOS_EOS)\n","    return outputs, finals"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rKiJAkvZGMWl"},"source":["### Decoder"]},{"cell_type":"markdown","metadata":{"id":"6-6cHH91VbdK"},"source":["#### Generator"]},{"cell_type":"code","metadata":{"id":"E1Tdf6_283WY"},"source":["class GeneratorTransferredSampled(nn.Module):\n","  \"\"\"Define standard linear + softmax generation step.\"\"\"\n","  def __init__(self, hidden_size, vocab_size, src_embed, gamma=0.001):\n","    \"\"\"\n","    Inputs:\n","      - `src_embed`: a 2d-tensor of shape (vocab_size, embed_size )\n","    \"\"\"\n","    super(GeneratorTransferredSampled, self).__init__()\n","    self.proj = nn.Linear(hidden_size, vocab_size, bias=True)\n","    self.gamma = gamma\n","    self.logsoftmax = nn.LogSoftmax(dim = 2)\n","    self.softmax = nn.Softmax(dim = 2)\n","    self.src_embed = src_embed\n","\n","  def embedding(self,x):\n","    return torch.matmul(x,self.src_embed.weight)\n","    \n","  def gumbel_softmax(self,logits, eps=1e-20):\n","    U = torch.rand(logits.shape).to(device)\n","    G = -torch.log(-torch.log(U + eps) + eps).to(device)\n","    return self.logsoftmax((logits + G) / self.gamma)\n","\n","  def gumbel(self,logits, eps=1e-20):\n","    U = torch.rand(logits.shape).to(device)\n","    G = -torch.log(-torch.log(U + eps) + eps).to(device)\n","    return (logits + G) / self.gamma\n","\n","  def forward(self, x):\n","    logits = self.proj(x)\n","    logprob = self.logsoftmax(logits)\n","    prob = self.softmax(logits)\n","    output = self.embedding(prob)\n","    word  = logits.argmax(dim = 2, keepdim = False)\n","\n","    return output, logprob, word\n","\n","  def forward_gumbel(self, x):\n","    logits = self.proj(x)\n","    prob = self.softmax(self.gumbel(logits))\n","    logprob = self.logsoftmax(self.gumbel(logits))\n","    output = self.embedding(prob)\n","    word  = logits.argmax(dim = 2, keepdim = False)\n","\n","    return output, logprob, word"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lauSKRx7Vdi8"},"source":["#### BeamSearch"]},{"cell_type":"code","metadata":{"id":"0bJLILziMf2s"},"source":["from queue import PriorityQueue\n","class BeamSearchNode:\n","  def __init__(self, hiddenstate, previousNode, cur_embed, wordId, \n","               logProb,  length ):\n","    self.h = hiddenstate\n","    self.prevNode = previousNode\n","    self.cur_embed = cur_embed\n","    self.wordid = wordId\n","    self.logp = logProb\n","    self.leng = length\n","    \n","  def __lt__(self,other):\n","    return self.logp < other.logp\n","\n","  def eval(self, alpha=1.0):\n","    return self.logp \n","    # Add here a function for shaping a reward\n","    # reward = 0\n","    # return self.logp / float(self.leng - 1 + 1e-6) + alpha * reward'\n","\n","class BeamSearch:\n","  def __init__(self,decoder, beam_width, topk, line_embed,max_len,\n","               max_iter=2000):\n","    \"\"\"Use Beam Search to generate a full sentence with the given decoder model\n","  Inputs:\n","      - `decoder`: decoder module with forward_step_beam function\n","      - `beam_width` : the length of the beam \n","      - `max_len`: an int representing the maximum decoding length.\n","      - `max_iter`: The maximum decoding iteration\n","    \"\"\"\n","    self.decoder = decoder\n","    self.beam_width = beam_width\n","    self.topk = topk\n","    self.line_embed = line_embed\n","    self.max_len = max_len\n","    self.max_iter = max_iter\n","\n","  def beam_decode(self, inputs, encoder_hidden, encoder_finals, src_mask, max_len,\n","                  hidden = None):\n","                  # inputs, encoder_finals,src_mask, proj_key, hidden):\n","    \"\"\"Use Beam Search to generate a full sentence with the given decoder model\n","    Inputs:\n","        - `inputs`: a 3d-tensor of shape (batch_size, 1, embed_size)\n","            representing a batch of padded embedded word vectors of SOS . \n","        - `encoder_finals`: a 3d-tensor of shape\n","            (num_enc_layers, batch_size, hidden_size) representing the final\n","            encoder hidden states used to initialize the initial decoder hidden\n","            states.\n","\n","    Returns:\n","        - `final_logp_batch`: a 2d-tensor of shape\n","            (batch_size, sentences_num) representing the probability of generating \n","            the sentence.\n","        - `final_hidden_batch`: a 4d-tensor of shape\n","            (sentences_num, num_layers, batch_size, hidden_size) representing \n","            the final hidden layer\n","        - `decoded_batch`: a 3d-tensor of shape\n","            (batch_size,sentences_num,  max_len) representing output sentence and\n","            the corresponding word index (can be used for embedding)  \n","    \n","    \"\"\"\n","    \n","    decoded_batch = []\n","    final_hidden_batch, final_logp_batch = [],[]\n","    # print(\"shape of encoder_finals:\",encoder_finals.shape)\n","    for i in range(inputs.shape[0]):\n","      if hidden is None:\n","        hidden = self.decoder.init_hidden(encoder_finals[:,i:i+1,:])\n","      decoder_input = inputs[i:i+1,:,:]\n","      print(\"shape of inputs:\",inputs.shape)\n","      print(\"shape of decoder_input:\",decoder_input.shape)\n","      # Number of sentence to generate\n","      endnodes = []\n","      number_required = self.topk\n","      print(\"shape of encoder_hidden\",encoder_hidden[i:i+1,:,:].shape)\n","      print(\"shape of hidden:\",hidden.shape)\n","      proj_key = self.decoder.attention.key_layer(encoder_hidden[i:i+1,:,:])\n","\n","      # starting node -  hidden vector, previous node, cur_embed, word id , logp, length\n","      node = BeamSearchNode(self.decoder.init_hidden(encoder_finals[:,i:i+1,:]), \n","                            None, decoder_input, [[SOS_INDEX]], 0, 1)\n","      nodes = PriorityQueue()\n","\n","      nodes.put((-node.eval(), node))\n","      qsize = 1\n","\n","      while qsize<=self.max_iter:\n","        tocheck = min(nodes.qsize(), self.beam_width)\n","        new_nodes = PriorityQueue()\n","        while tocheck>0:\n","          score, n = nodes.get()\n","          decoder_input = n.cur_embed\n","          decoder_hidden = n.h\n","          if n.leng > self.max_len:\n","            endnodes.append((score, n))\n","            # if we reached maximum # of sentences required\n","            if len(endnodes) >= number_required:\n","                break\n","\n","          # decode for one step using decoder\n","          print(\"encoder_hidden.shape 2:\",encoder_hidden[i:i+1,:,:].shape)\n","          hidden, _, logsoftmax_logits, wordId = decoder.forward_step_beam(decoder_input, \n","                                                                           encoder_hidden[i:i+1,:,:],\n","                                                                          src_mask[i:i+1,:,:], \n","                                                                          proj_key, \n","                                                                          decoder_hidden)\n","          tocheck -= 1\n","          # PUT HERE REAL BEAM SEARCH OF TOP\n","          log_prob, indexes = torch.topk(logsoftmax_logits, self.beam_width)\n","          print(\"indexes.shape\",indexes.shape)\n","          for new_k in range(self.beam_width):\n","            \n","            decoded_t = indexes[0][0][new_k].view(1, -1)\n","            log_p = log_prob[0][0][new_k]\n","            prev_embed = self.line_embed(decoded_t)\n","            print('decoded_t.shape:',decoded_t.shape)\n","            print('prev_embed.shape:',prev_embed.shape)\n","\n","            node = BeamSearchNode(decoder_hidden, n,prev_embed, decoded_t, \n","                                  n.logp + log_p,n.leng + 1)\n","            score = -node.eval()\n","            new_nodes.put((score, node))\n","          qsize += self.beam_width - 1\n","        nodes = new_nodes\n","\n","        if len(endnodes) >= number_required:\n","            break\n","        \n","\n","      # choose nbest paths, back trace them\n","      if len(endnodes) == 0:\n","          endnodes = [nodes.get() for _ in range(self.topk)]\n","\n","      utterances = []\n","      final_logps = []\n","      final_hiddens = []\n","      # final_gumbel_logits = []\n","      for score, n in sorted(endnodes, key=operator.itemgetter(0)):\n","          end_node = n\n","          utterance = []\n","          # gumbel_logits = []\n","          utterance.append(n.wordid[0][0])\n","          # gumbel_logits =gumbel_logits + [n.gumbel_logits]\n","          # back trace\n","          while n.prevNode != None:\n","              n = n.prevNode\n","              utterance.append(n.wordid[0][0])\n","              # if n.gumbel_logits is not None:\n","              #   gumbel_logits =gumbel_logits + [n.gumbel_logits]\n","\n","          utterance = torch.unsqueeze(torch.tensor(utterance[::-1][:self.max_len]), axis = 0)\n","          utterances.append(utterance)\n","          # gumbel_logits = torch.cat(gumbel_logits, dim = 1)\n","          final_logp = end_node.logp \n","          final_hidden = end_node.h\n","          final_logps.append(final_logp)\n","          final_hiddens.append(torch.unsqueeze(torch.tensor(final_hidden), axis = 0))\n","          # final_gumbel_logits.append(gumbel_logits)\n","        \n","      utterances = torch.cat(utterances, axis = 0 )\n","      decoded_batch.append(torch.unsqueeze(utterances, axis = 0))\n","      final_logp_batch.append(torch.unsqueeze(torch.tensor(final_logps), axis = 0))\n","      final_hiddens = torch.cat(final_hiddens, axis = 0)\n","      final_hidden_batch.append(final_hiddens)\n","      # final_gumbel_logits = torch.cat(final_gumbel_logits, axis = 0)\n","      # final_gumbel_logits_batch.append(torch.unsqueeze(final_gumbel_logits, axis = 0))\n","      # print(\"after decoded_batch:\")\n","      # print(decoded_batch)\n","\n","    # decoded_batch size = (batch, topk, sentence_len, 1)\n","    final_logp_batch = torch.cat(final_logp_batch, axis = 0)\n","    final_hidden_batch = torch.cat(final_hidden_batch, axis = 2)\n","    # final_gumbel_logits_batch = torch.cat(final_gumbel_logits_batch, axis = 0)\n","    decoded_batch = torch.cat(decoded_batch, axis = 0)\n","    print(\"final_logp_batch.shape:\",final_logp_batch.shape)\n","    print(\"final_hidden_batch.shape:\",final_hidden_batch.shape)\n","    # print(\"final_gumbel_logits_batch.shape:\",final_gumbel_logits_batch.shape)\n","    print(\"decoded_batch.shape:\",decoded_batch.shape)\n","    print(\"max_len:\", self.max_len)\n","    print(\"topk:\", self.topk)\n","    print(\"batch size:\", inputs.shape[0])\n","    return final_hidden_batch, final_logp_batch,  decoded_batch\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GOjwClznVgrm"},"source":["#### Decoder"]},{"cell_type":"code","metadata":{"id":"plJpAyPu86yw"},"source":["class Decoder(nn.Module):\n","  \"\"\"An RNN decoder + generator with GRU\"\"\"\n","\n","  def __init__(self, input_size, hidden_size, max_len,generator, num_layers = 1, dropout=0.):\n","    \"\"\"\n","      Inputs:\n","        - `input_size` , `hidden_size`, and `dropout` the same as in Encoder.\n","    \"\"\"\n","    super(Decoder, self).__init__()\n","    self.input_size = input_size\n","    self.hidden_size = hidden_size\n","    self.rnn = nn.GRU(input_size, hidden_size, num_layers=num_layers, batch_first=True,\n","                      dropout=dropout, bidirectional=False)\n","    self.generator = generator\n","    self.max_len = max_len\n","    self.dropout_layer = nn.Dropout(p=dropout)\n","    self.rnn_to_pre = nn.Linear(input_size + hidden_size,\n","                                hidden_size, bias=False)\n","\n","  def forward_step(self, prev_embed, hidden):\n","    \"\"\"Helper function for forward below:\n","       Perform a single decoder step (1 word).\n","\n","       Inputs:\n","      - `prev_embed`: a 3d-tensor of shape (batch_size, 1, embed_size = vocab_size)\n","          representing the padded embedded word vectors at this step in training\n","      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size) representing\n","          the current hidden state.\n","\n","      Returns:\n","      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size)\n","          representing the current decoder hidden state.\n","      - `output`: a 3d-tensor of shape (batch_size, max_len, vocab_size)\n","          representing the total generated outputs.\n","    \"\"\"\n","    pre_output, hidden = self.rnn(prev_embed, hidden)\n","    pre_output = torch.cat([prev_embed, pre_output], dim=2)\n","    pre_output = self.dropout_layer(pre_output)\n","    pre_output = self.rnn_to_pre(pre_output)\n","    pre_output = torch.tanh(pre_output)\n","\n","    return hidden, pre_output\n","\n","    ### Your code here!\n","    pre_output, hidden = self.rnn(prev_embed, hidden)\n","    pre_output = torch.cat([prev_embed, pre_output], dim=2)\n","    pre_output = self.dropout_layer(pre_output)\n","    pre_output = self.rnn_to_pre(pre_output)\n","    pre_output = self.pre_activation(pre_output)\n","    \n","  def forward_step_beam(self, prev_embed, hidden):\n","    \"\"\"Beam Search only: Unroll the decoder one step at a time.\n","    Inputs:\n","      - `input`: a 3d-tensor of shape (batch_size, 1, embed_size)\n","          representing the padded embedded word vectors at this step in training\n","      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size) representing\n","          the current hidden state.\n","    Returns:\n","      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size)\n","          representing the current decoder hidden state.\n","      - `output`: a 3d-tensor of shape (batch_size, 1, vocab_size)\n","          representing the total generated outputs.    \n","      - `gumbel_logits`: a 3d-tensor of shape\n","          (batch_size, 1, trg_vocab_size) representing the mapped decoder\n","          outputs from gumbel softmax.\n","      - `output_word`: a 2d-tensor of shape\n","          (batch_size, 1) representing output sentence and\n","          the corresponding word index (can be used for embedding)    \n","      - `logits`: a 2d-tensor of shape\n","          (batch_size, 1, trg_vocab_size) representing the mapped decoder\n","          outputs from log softmax \n","          \"\"\"\n","    hidden, pre_output = self.forward_step(prev_embed,hidden)\n","    output, gumbel_logits, output_word, logits = self.generator(pre_output)\n","    return  hidden, output, gumbel_logits, output_word, logits\n","\n","  def forward(self, input, encoder_finals,max_len, hidden=None):\n","    \"\"\"Unroll the decoder one step at a time.\n","\n","    Inputs:\n","      - `inputs`: a 3d-tensor of shape (batch_size, 1, embed_size)\n","          representing a batch of padded embedded word vectors of SOS . \n","          If size is (batch_size,max_len, embed_size), then it is teacher forcing.\n","      - `encoder_finals`: a 3d-tensor of shape\n","          (num_enc_layers, batch_size, hidden_size) representing the final\n","          encoder hidden states used to initialize the initial decoder hidden\n","          states.\n","      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size) representing\n","          the value to be used to initialize the initial decoder hidden states.\n","          If None, then use `encoder_finals`.\n","      - `max_len`: an int representing the maximum decoding length.\n","      - `style`: TAYLOR_STYLE or DRAKE_STYLE\n","\n","    Returns:\n","      - `hidden`: a 3d-tensor of shape\n","          (num_layers, batch_size, hidden_size) representing the final hidden\n","          state for each element in the batch.\n","      - `outputs`: a 3d-tensor of shape\n","          (batch_size, max_len, hidden_size) representing the raw decoder\n","          outputs (before mapping to a `trg_vocab_size`-dim vector).\n","      - `logits_vectors`: a 3d-tensor of shape\n","          (batch_size, max_len, trg_vocab_size) representing the mapped decoder\n","          outputs.\n","      - `words`: a 3d-tensor of shape\n","          (batch_size, max_len, 1) representing output sentence and\n","          the corresponding word index (can be used for embedding)  \n","    \"\"\"\n","\n","    # Initialize decoder hidden state.\n","    if hidden is None:\n","      hidden = self.init_hidden(encoder_finals)\n","    output_vectors = []\n","    logits_vectors = []\n","    words = []\n","    hidden_states = []\n","    hidden_states.append(hidden[-1][:,None,:])\n","    for i in range(max_len-1) :\n","      hidden, prev_output = self.forward_step(input,hidden)\n","      input, logits, output_word = self.generator.forward_gumbel(prev_output)\n","      # input, logits, output_word = self.generator(prev_output)\n","\n","      # input = torch.concat([input,torch.full(input.shape,style)], axis = -1)\n","      logits_vectors.append(logits)\n","      output_vectors.append(input)\n","      words.append(output_word)\n","      hidden_states.append(prev_output)\n","\n","    outputs = torch.cat(output_vectors, dim =1)\n","    logits_vectors = torch.cat(logits_vectors,dim = 1)\n","    words = torch.cat(words, axis = -1)\n","    hidden_states = torch.cat(hidden_states, axis = 1)\n","    return hidden, outputs , logits_vectors, words, hidden_states\n","\n","  def forward_teacher(self, input, encoder_finals, max_len=None, hidden=None):\n","    \"\"\"Unroll the decoder one step at a time.\n","\n","    Inputs:\n","      - `inputs`: a 3d-tensor of shape (batch_size,max_len, embed_size)\n","          representing a batch of padded embedded word vectors of original \n","          sentence and acts as  teacher forcing.\n","\n","    Returns:\n","      - `hidden`: a 3d-tensor of shape\n","          (num_layers, batch_size, hidden_size) representing the final hidden\n","          state for each element in the batch.\n","      - `outputs`: a 3d-tensor of shape\n","          (batch_size, max_len, hidden_size) representing the raw decoder\n","          outputs (before mapping to a `trg_vocab_size`-dim vector).\n","      - `logits_vectors`: a 3d-tensor of shape\n","          (batch_size, max_len, trg_vocab_size) representing the mapped decoder\n","          outputs each represents the probability? \n","      - `words`: a 3d-tensor of shape\n","          (batch_size, max_len, 1) representing output sentence and\n","          the corresponding word index (can be used for embedding)      \n","    \"\"\"\n","\n","    # Initialize decoder hidden state.\n","    if max_len is None:\n","      max_len = input.shape[1]\n","    if hidden is None:\n","      hidden = self.init_hidden(encoder_finals)\n","    output_vectors = []\n","    logits_vectors = []\n","    words = []\n","    hidden_states = []\n","    hidden_states.append(hidden[-1][:,None,:])\n","    for i in range(max_len):\n","      hidden, prev_output = self.forward_step(input[:,i:i+1,:],hidden)\n","      output, logits, output_word = self.generator(prev_output)\n","      \n","      logits_vectors.append(logits)\n","      output_vectors.append(output)\n","      words.append(output_word)\n","      hidden_states.append(prev_output)\n","\n","    outputs = torch.cat(output_vectors, dim =1)\n","    logits_vectors = torch.cat(logits_vectors,dim = 1)\n","    words = torch.cat(words, axis = -1)\n","    hidden_states = torch.cat(hidden_states, axis = 1)\n","    return hidden, outputs , logits_vectors, words, hidden_states\n","\n","  def init_hidden(self, encoder_finals):\n","    \"\"\"Use encoder final hidden state to initialize decoder's first hidden\n","       state.\n","\n","       Input: `encoder_finals` is same as in forward()\n","\n","       Returns: \n","         - `decoder_init_hiddens`: a 3d-tensor of shape \n","              (num_layers, batch_size, hidden_size) representing the initial\n","              hidden state of the decoder for each element in the batch \n","    \"\"\"\n","    decoder_init_hiddens = torch.tanh(encoder_finals)\n","    return decoder_init_hiddens"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NX25tl4G5vWa"},"source":["### Attention Decoder"]},{"cell_type":"code","metadata":{"id":"VrWt0I5N6ijP"},"source":["class BahdanauAttention(nn.Module):\n","    \"\"\"Implements Bahdanau (MLP) attention\"\"\"\n","    \n","    def __init__(self, hidden_size, key_size=None, query_size=None):\n","        super(BahdanauAttention, self).__init__()\n","        \n","        # We assume a bi-directional encoder so key_size is 2*hidden_size\n","        key_size = 2 * hidden_size if key_size is None else key_size\n","        query_size = hidden_size if query_size is None else query_size\n","\n","        self.key_layer = nn.Linear(key_size, hidden_size, bias=False)\n","        self.query_layer = nn.Linear(query_size, hidden_size, bias=False)\n","        self.energy_layer = nn.Linear(hidden_size, 1, bias=False)\n","        \n","        # to store attention scores\n","        self.alphas = None\n","        \n","    def forward(self, query=None, proj_key=None, value=None, mask=None):\n","        assert mask is not None, \"mask is required\"\n","\n","        # We first project the query (the decoder state).\n","        # The projected keys (the encoder states) were already pre-computated.\n","        query = self.query_layer(query)\n","        \n","        # Calculate scores.\n","        scores = self.energy_layer(torch.tanh(query + proj_key))\n","        scores = scores.squeeze(2).unsqueeze(1)\n","        \n","        # Mask out invalid positions.\n","        # The mask marks valid positions so we invert it using `mask & 0`.\n","        scores.data.masked_fill_(mask == 0, -float('inf'))\n","        \n","        # Turn scores to probabilities.\n","        alphas = F.softmax(scores, dim=-1)\n","        self.alphas = alphas        \n","        \n","        # The context vector is the weighted sum of the values.\n","        context = torch.bmm(alphas, value)\n","        \n","        # context shape: [B, 1, 2D], alphas shape: [B, 1, M]\n","        return context, alphas"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KKIgeUK75xSm"},"source":["class AttentionDecoder(nn.Module):\n","  \"\"\"An RNN decoder + generator with GRU\"\"\"\n","\n","  def __init__(self, input_size, hidden_size, attention, \n","               max_len,generator,\n","               num_layers = 1, dropout=0.):\n","    \"\"\"\n","      Inputs:\n","        - `input_size` , `hidden_size`, and `dropout` the same as in Encoder.\n","    \"\"\"\n","    super(AttentionDecoder, self).__init__()\n","    self.input_size = input_size\n","    self.hidden_size = hidden_size\n","    # self.rnn = nn.GRU(input_size, hidden_size, num_layers=num_layers, batch_first=True,\n","    #                   dropout=dropout, bidirectional=False)\n","    self.rnn = nn.GRU(input_size + hidden_size, hidden_size, num_layers,\n","                          batch_first=True, dropout=dropout)\n","    \n","    self.generator = generator\n","    self.max_len = max_len\n","    self.dropout_layer = nn.Dropout(p=dropout)\n","    # self.rnn_to_pre = nn.Linear(input_size + hidden_size,\n","    #                             hidden_size, bias=False)\n","    self.rnn_to_pre = nn.Linear(hidden_size + hidden_size + input_size,\n","                                hidden_size, bias=False)\n","    self.attention = attention\n","\n","  def forward_step(self, prev_embed, encoder_hidden, \n","                   src_mask, proj_key, hidden):\n","    \"\"\"Helper function for forward below:\n","       Perform a single decoder step (1 word).\n","\n","       Inputs:\n","      - `prev_embed`: a 3d-tensor of shape (batch_size, 1, embed_size = vocab_size)\n","          representing the padded embedded word vectors at this step in training\n","      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size) representing\n","          the current hidden state.\n","\n","      Returns:\n","      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size)\n","          representing the current decoder hidden state.\n","      - `output`: a 3d-tensor of shape (batch_size, max_len, vocab_size)\n","          representing the total generated outputs.\n","    \"\"\"\n","\n","    # compute context vector using attention mechanism\n","    query = hidden[-1].unsqueeze(1)  # [#layers, B, D] -> [B, 1, D]\n","    print('hidden.shape:',hidden.shape)\n","    print('query.shape:',query.shape)\n","    print('proj_key.shape:',proj_key.shape)\n","    print('encoder_hidden.shape:',encoder_hidden.shape)\n","    print('src_mask.shape:',src_mask.shape)\n","    context, attn_probs = self.attention(\n","        query=query, proj_key=proj_key,\n","        value=encoder_hidden, mask=src_mask)\n","    \n","    # RNN\n","    print(\"prev_embed.shape:\",prev_embed.shape)\n","    print(\"context.shape:\",context.shape)\n","    rnn_input = torch.cat([prev_embed, context], dim=2)\n","\n","    output, hidden = self.rnn(rnn_input, hidden)\n","    \n","    pre_output = torch.cat([prev_embed, output, context], dim=2)\n","    pre_output = self.dropout_layer(pre_output)\n","    pre_output = self.rnn_to_pre(pre_output)\n","    \n","    return hidden, pre_output\n","    \n","  def forward_step_beam(self, prev_embed, encoder_hidden, \n","                   src_mask, proj_key, hidden):\n","    \"\"\"Beam Search only: Unroll the decoder one step at a time.\"\"\"\n","    temp_hidden, pre_output = self.forward_step(prev_embed,encoder_hidden, \n","                   src_mask, proj_key, hidden)\n","    output, logits, output_word = self.generator.forward_gumbel(pre_output)\n","    return  temp_hidden, output, logits, output_word\n","\n","  def forward(self, input, encoder_hidden, encoder_finals, src_mask, max_len, hidden=None):\n","    \"\"\"Unroll the decoder one step at a time.\n","\n","    Inputs:\n","      - `inputs`: a 3d-tensor of shape (batch_size, 1, embed_size)\n","          representing a batch of padded embedded word vectors of SOS . \n","          If size is (batch_size,max_len, embed_size), then it is teacher forcing.\n","      - `encoder_finals`: a 3d-tensor of shape\n","          (num_enc_layers, batch_size, hidden_size) representing the final\n","          encoder hidden states used to initialize the initial decoder hidden\n","          states.\n","      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size) representing\n","          the value to be used to initialize the initial decoder hidden states.\n","          If None, then use `encoder_finals`.\n","      - `max_len`: an int representing the maximum decoding length.\n","      - `style`: TAYLOR_STYLE or DRAKE_STYLE\n","\n","    Returns:\n","      - `hidden`: a 3d-tensor of shape\n","          (num_layers, batch_size, hidden_size) representing the final hidden\n","          state for each element in the batch.\n","      - `outputs`: a 3d-tensor of shape\n","          (batch_size, max_len, hidden_size) representing the raw decoder\n","          outputs (before mapping to a `trg_vocab_size`-dim vector).\n","      - `logits_vectors`: a 3d-tensor of shape\n","          (batch_size, max_len, trg_vocab_size) representing the mapped decoder\n","          outputs.\n","      - `words`: a 3d-tensor of shape\n","          (batch_size, max_len, 1) representing output sentence and\n","          the corresponding word index (can be used for embedding)  \n","    \"\"\"\n","\n","    # Initialize decoder hidden state.\n","    if hidden is None:\n","      hidden = self.init_hidden(encoder_finals)\n","\n","    proj_key = self.attention.key_layer(encoder_hidden)\n","\n","    output_vectors = []\n","    logits_vectors = []\n","    words = []\n","    hidden_states = []\n","    hidden_states.append(hidden[-1][:,None,:])\n","    \n","    for i in range(max_len-1) :\n","      \n","      hidden, prev_output = self.forward_step(input,encoder_hidden, src_mask, proj_key, hidden)\n","      input, logits, output_word = self.generator.forward_gumbel(prev_output)\n","\n","      logits_vectors.append(logits)\n","      output_vectors.append(input)\n","      words.append(output_word)\n","      hidden_states.append(prev_output)\n","\n","    outputs = torch.cat(output_vectors, dim =1)\n","    logits_vectors = torch.cat(logits_vectors,dim = 1)\n","    words = torch.cat(words, axis = -1)\n","    hidden_states = torch.cat(hidden_states, axis = 1)\n","\n","    return hidden, outputs , logits_vectors, words, hidden_states\n","\n","  def forward_teacher(self, input, encoder_hidden, encoder_finals, \n","                      src_mask, max_len=None, hidden=None):\n","    \"\"\"Unroll the decoder one step at a time.\n","\n","    Inputs:\n","      - `inputs`: a 3d-tensor of shape (batch_size,max_len, embed_size)\n","          representing a batch of padded embedded word vectors of original \n","          sentence and acts as  teacher forcing.\n","\n","    Returns:\n","      - `hidden`: a 3d-tensor of shape\n","          (num_layers, batch_size, hidden_size) representing the final hidden\n","          state for each element in the batch.\n","      - `outputs`: a 3d-tensor of shape\n","          (batch_size, max_len, hidden_size) representing the raw decoder\n","          outputs (before mapping to a `trg_vocab_size`-dim vector).\n","      - `logits_vectors`: a 3d-tensor of shape\n","          (batch_size, max_len, trg_vocab_size) representing the mapped decoder\n","          outputs each represents the probability? \n","      - `words`: a 3d-tensor of shape\n","          (batch_size, max_len, 1) representing output sentence and\n","          the corresponding word index (can be used for embedding)      \n","    \"\"\"\n","\n","    # Initialize decoder hidden state.\n","    if max_len is None:\n","      max_len = input.shape[1]\n","    if hidden is None:\n","      hidden = self.init_hidden(encoder_finals)\n","    \n","    proj_key = self.attention.key_layer(encoder_hidden)\n","\n","    output_vectors = []\n","    logits_vectors = []\n","    words = []\n","    hidden_states = []\n","    hidden_states.append(hidden[-1][:,None,:])\n","\n","    for i in range(max_len):\n","      hidden, prev_output = self.forward_step(input[:,i:i+1,:], encoder_hidden, src_mask, proj_key, hidden)\n","      \n","      output, logits, output_word = self.generator(prev_output)\n","      logits_vectors.append(logits)\n","      output_vectors.append(output)\n","      words.append(output_word)\n","      hidden_states.append(prev_output)\n","\n","    outputs = torch.cat(output_vectors, dim =1)\n","    logits_vectors = torch.cat(logits_vectors,dim = 1)\n","    words = torch.cat(words, axis = -1)\n","    hidden_states = torch.cat(hidden_states, axis = 1)\n","    return hidden, outputs , logits_vectors, words, hidden_states\n","\n","  def init_hidden(self, encoder_finals):\n","    \"\"\"Use encoder final hidden state to initialize decoder's first hidden\n","       state.\n","\n","       Input: `encoder_finals` is same as in forward()\n","\n","       Returns: \n","         - `decoder_init_hiddens`: a 3d-tensor of shape \n","              (num_layers, batch_size, hidden_size) representing the initial\n","              hidden state of the decoder for each element in the batch \n","    \"\"\"\n","    decoder_init_hiddens = torch.tanh(encoder_finals)\n","    return decoder_init_hiddens"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wg_FbKl-T9xd"},"source":["### Classifier"]},{"cell_type":"code","metadata":{"id":"SeDjRElB65UC"},"source":["class LSTMDiscriminator(nn.Module):\n","  def __init__(self, input_size, hidden_size, LSTMlayers=1, dropout = 0.5):\n","    super(LSTMDiscriminator, self).__init__()\n","\n","    self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=LSTMlayers, \n","                        batch_first=True, bidirectional=True)\n","    self.drop = nn.Dropout(p=dropout)\n","    self.fc = nn.Linear(2*hidden_size, 1)\n","    self.hidden_size = hidden_size\n","\n","  def forward(self, text_emb, text_len):\n","\n","    packed_input = pack_padded_sequence(text_emb, text_len.cpu(), batch_first=True, enforce_sorted=False)\n","    packed_output, _ = self.lstm(packed_input)\n","    output, _ = pad_packed_sequence(packed_output, batch_first=True)\n","\n","    out_forward = output[range(len(output)), text_len - 1, :self.hidden_size]\n","    out_reverse = output[:, 0, self.hidden_size:]\n","    out_reduced = torch.cat((out_forward, out_reverse), 1)\n","    text_fea = self.drop(out_reduced)\n","\n","    text_fea = self.fc(text_fea)\n","    text_fea = torch.squeeze(text_fea, 1)\n","    text_out = torch.sigmoid(text_fea)\n","\n","    return text_out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PylylJgLahxw"},"source":["class LSTMClassifier(nn.Module):\n","\n","    def __init__(self, dimension=128):\n","        super(LSTMClassifier, self).__init__()\n","\n","        self.embedding = nn.Linear(len(vocab), 300)\n","        self.dimension = dimension\n","        self.lstm = nn.LSTM(input_size=300,\n","                            hidden_size=dimension,\n","                            num_layers=1,\n","                            batch_first=True,\n","                            bidirectional=True)\n","        self.drop = nn.Dropout(p=0.5)\n","\n","        self.fc = nn.Linear(2*dimension, 1)\n","\n","    def forward(self, text, text_len):\n","\n","        text_emb = self.embedding(text)\n","\n","        packed_input = pack_padded_sequence(text_emb, text_len.cpu(), batch_first=True, enforce_sorted=False)\n","        packed_output, _ = self.lstm(packed_input)\n","        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n","\n","        out_forward = output[range(len(output)), text_len - 1, :self.dimension]\n","        out_reverse = output[:, 0, self.dimension:]\n","        out_reduced = torch.cat((out_forward, out_reverse), 1)\n","        text_fea = self.drop(out_reduced)\n","\n","        text_fea = self.fc(text_fea)\n","        text_fea = torch.squeeze(text_fea, 1)\n","        text_out = torch.sigmoid(text_fea)\n","\n","        return text_out"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_xuubDG0F5i3"},"source":["### Training"]},{"cell_type":"markdown","metadata":{"id":"ALXZL0AibQxr"},"source":["#### TSTModel"]},{"cell_type":"code","metadata":{"id":"yjs1mzrQEyef"},"source":["class TSTModel(nn.Module):\n","  def __init__(self, max_len, vocab_size, embed_size, hidden_size_z, hidden_size_y, \n","               line_embed, encoder, generator, decoder, classifier,beamSeasrch):\n","    super(TSTModel, self).__init__()\n","\n","    self.hidden_size = hidden_size_y + hidden_size_z\n","\n","    self.encoder = encoder\n","    self.generator = generator\n","    self.decoder = decoder\n","    self.classifier = classifier\n","    self.beamSeasrch = beamSeasrch\n","\n","    self.line_embed = line_embed\n","    self.y_embed_enc = nn.Embedding(2,hidden_size_y)\n","    self.y_embed_gen = nn.Embedding(2,hidden_size_y)\n","\n","    self.max_len = max_len\n","    self.vocab_size = vocab_size\n","    self.embed_size = embed_size\n","    self.hidden_size_z = hidden_size_z\n","    self.hidden_size_y = hidden_size_y\n","\n","  def forward(self, lines, line_lens, labels):\n","    encoded_lines = self.encode(lines, line_lens, labels)\n","    z = encoded_lines[1][-1][:,self.hidden_size_y:]\n","\n","    h0_orig = torch.cat((self.y_embed_gen(labels),z), 1)[None,:]\n","    h0_tsf = torch.cat((self.y_embed_gen(1-labels),z), 1)[None,:]\n","\n","    # Decode back into original form for reconstruction\n","    rec_orig = self.reconstruct(h0_orig, lines[:, :-1])\n","\n","    # Decode into original and transferred forms for classification\n"," \n","    decode_orig = self.decode(h0_orig)\n","    decode_tsf = self.decode(h0_tsf)\n","    \n","    half = int(lines.size(0) / 2)\n","\n","    discrim1_input = torch.cat((rec_orig[4][:half], decode_tsf[4][half:]))\n","    discrim0_input = torch.cat((rec_orig[4][half:], decode_tsf[4][:half]))\n","\n","    classifier_lines = torch.cat((decode_orig[2], decode_tsf[2], F.one_hot(lines[:,1:], self.vocab_size).to(torch.float)), 0)\n","    classifier_line_lens = torch.cat((line_lens, line_lens, line_lens),0)\n","    pred_class = self.classifier(classifier_lines, classifier_line_lens)\n","\n","    # return rec_orig, decode_orig\n","    return rec_orig, pred_class, decode_orig, decode_tsf, (discrim0_input, line_lens), (discrim1_input, line_lens) \n","\n","  def forward_beam(self,lines, line_lens, labels):\n","    encoded_lines = self.encode(lines, line_lens, labels)\n","    z = encoded_lines[1][-1][:,self.hidden_size_y:]\n","\n","    h0_orig = torch.cat((self.y_embed_gen(labels),z), 1)[None,:]\n","    h0_tsf = torch.cat((self.y_embed_gen(1-labels),z), 1)[None,:]\n","\n","    decode_orig = self.decode_beam(h0_orig)\n","    decode_tsf = self.decode_beam(h0_tsf)\n","\n","    classifier_lines = torch.cat((decode_orig[2][:,-1,:,:], \n","                                  decode_tsf[2][:,-1,:,:], \n","                                  F.one_hot(lines, self.vocab_size).to(torch.float)), 0)\n","    pred_class = self.classifier(classifier_lines)\n","\n","    return pred_class, decode_orig, decode_tsf\n","    \n","  def decode_beam(self,h0):\n","    target = self.line_embed(torch.tensor([SOS_INDEX]).repeat(h0.size()[1],1).to(device))\n","    return self.beamSeasrch.beam_decode(target, h0)\n","\n","  def encode(self, lines, line_lens, labels):\n","    init_state = torch.cat((self.y_embed_enc(labels), torch.zeros((len(lines),self.hidden_size_z), device=device)), 1)[None,:].to(device)\n","    return self.encoder(self.line_embed(lines), line_lens, init_state)\n","\n","  def reconstruct(self, h0, lines):\n","    original = self.line_embed(lines)\n","    return self.decoder.forward_teacher(original,h0)\n","\n","  def decode(self, h0):\n","    target = self.line_embed(torch.tensor([SOS_INDEX]).repeat(h0.size()[1],1).to(device))\n","    return self.decoder.forward(target,h0,self.max_len)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5gZWJ18WbN9Y"},"source":["#### TSTModelAttention"]},{"cell_type":"code","metadata":{"id":"5dN3kpsaZpFX"},"source":["class TSTModelAttention(nn.Module):\n","  def __init__(self, max_len, vocab_size, embed_size, hidden_size_z, hidden_size_y, \n","               line_embed, encoder, generator, decoder, classifier,beamSeasrch):\n","    super(TSTModelAttention, self).__init__()\n","\n","    self.hidden_size = hidden_size_y + hidden_size_z\n","\n","    self.encoder = encoder\n","    self.generator = generator\n","    self.decoder = decoder\n","    self.classifier = classifier\n","    self.beamSeasrch = beamSeasrch\n","\n","    self.line_embed = line_embed\n","    self.y_embed_enc = nn.Embedding(2,hidden_size_y)\n","    self.y_embed_gen = nn.Embedding(2,hidden_size_y)\n","\n","    self.max_len = max_len\n","    self.vocab_size = vocab_size\n","    self.embed_size = embed_size\n","    self.hidden_size_z = hidden_size_z\n","    self.hidden_size_y = hidden_size_y\n","\n","  def forward(self, lines, line_lens, labels):\n","\n","    src_mask = (lines != PAD_INDEX).unsqueeze(-2)\n","    encoder_hidden, encoder_finals = self.encode(lines, line_lens, labels)\n","\n","    z = encoder_finals[-1][:,self.hidden_size_y:]\n","\n","    h0_orig = torch.cat((self.y_embed_gen(labels),z), 1)[None,:]\n","    h0_tsf = torch.cat((self.y_embed_gen(1-labels),z), 1)[None,:]\n","\n","    # Decode back into original form for reconstruction\n","    rec_orig = self.reconstruct(encoder_hidden, h0_orig, lines[:, :-1], src_mask)\n","\n","    # Decode into original and transferred forms for classification\n","    decode_orig = self.decode(encoder_hidden, h0_orig, src_mask)\n","    decode_tsf = self.decode(encoder_hidden, h0_tsf, src_mask)\n","    \n","    half = int(lines.size(0) / 2)\n","\n","    discrim1_input = torch.cat((rec_orig[4][:half], decode_tsf[4][half:]))\n","    discrim0_input = torch.cat((rec_orig[4][half:], decode_tsf[4][:half]))\n","\n","    classifier_lines = torch.cat((decode_orig[2], decode_tsf[2], F.one_hot(lines[:,1:], self.vocab_size).to(torch.float)), 0)\n","    classifier_line_lens = torch.cat((line_lens, line_lens, line_lens),0)\n","    pred_class = self.classifier(classifier_lines, classifier_line_lens-1)\n","\n","    return rec_orig, pred_class, decode_orig, decode_tsf, (discrim0_input, line_lens), (discrim1_input, line_lens)\n","  \n","  def forward_beam(self,lines, line_lens, labels):\n","    src_mask = (lines != PAD_INDEX).unsqueeze(-2)\n","    encoder_hidden, encoder_finals = self.encode(lines, line_lens, labels)\n","    z = encoder_finals[-1][:,self.hidden_size_y:]\n","\n","    h0_orig = torch.cat((self.y_embed_gen(labels),z), 1)[None,:]\n","    h0_tsf = torch.cat((self.y_embed_gen(1-labels),z), 1)[None,:]\n","\n","    decode_orig = self.decode_beam(encoder_hidden, h0_orig, src_mask)\n","    decode_tsf = self.decode_beam(encoder_hidden, h0_tsf, src_mask)\n","\n","    # half = int(lines.size(0) / 2)\n","\n","    # discrim1_input = torch.cat((rec_orig[4][:half], decode_tsf[4][half:]))\n","    # discrim0_input = torch.cat((rec_orig[4][half:], decode_tsf[4][:half]))\n","\n","    # classifier_lines = torch.cat((decode_orig[2], decode_tsf[2], F.one_hot(lines[:,1:], self.vocab_size).to(torch.float)), 0)\n","    # classifier_line_lens = torch.cat((line_lens, line_lens, line_lens),0)\n","    # pred_class = self.classifier(classifier_lines, classifier_line_lens-1)\n","\n","    return decode_orig, decode_tsf #rec_orig, ,pred_class,  (discrim0_input, line_lens), (discrim1_input, line_lens)\n","    \n","  def decode_beam(self,encoder_hidden,h0,src_mask):\n","    target = self.line_embed(torch.tensor([SOS_INDEX]).repeat(h0.size()[1],1).to(device))\n","    return self.beamSeasrch.beam_decode(target, encoder_hidden,h0,src_mask, self.max_len)\n","\n","  def encode(self, lines, line_lens, labels):\n","    init_state = torch.cat((self.y_embed_enc(labels), torch.zeros((len(lines),self.hidden_size_z), device=device)), 1)[None,:].to(device)\n","    return self.encoder(self.line_embed(lines), line_lens, init_state)\n","\n","  def reconstruct(self, encoder_hidden, h0, lines, src_mask):\n","    original = self.line_embed(lines)\n","    return self.decoder.forward_teacher(original,encoder_hidden, h0, src_mask)\n","\n","  def decode(self, encoder_hidden, h0, src_mask):\n","    target = self.line_embed(torch.tensor([SOS_INDEX]).repeat(h0.size()[1],1).to(device))\n","    return self.decoder.forward(target, encoder_hidden, h0, src_mask, self.max_len)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EhCxBGg_1575"},"source":["import tensorflow as tf"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u8rrgiRCvIzv"},"source":["#### Training"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":436},"id":"O8jaGcIiNQUi","executionInfo":{"status":"error","timestamp":1638382959445,"user_tz":300,"elapsed":959,"user":{"displayName":"Katherine Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11324848797432579426"}},"outputId":"9dcefbcd-f4e2-40a5-eee0-a6a094b6a78b"},"source":["attention = True\n","classify = True\n","discriminate = False\n","\n","epochs = 1\n","class_epochs = 0\n","lr = 1e-3\n","batch_size = 32\n","print_every = 100\n","\n","max_len = dataset.max_seq_length\n","vocab_size = len(vocab)\n","embed_size = 256\n","hidden_size_z = 256\n","hidden_size_y = 128\n","hidden_size = hidden_size_z + hidden_size_y\n","dropout = 0.2\n","gamma = 0.001\n","\n","TAYLOR_STYLE=1 # for information only, don't change\n","DRAKE_STYLE=0  # for information only, don't change\n","train_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","valid_loader = data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n","\n","\n","line_embed = nn.Embedding(vocab_size, embed_size)\n","encoder = Encoder(embed_size,hidden_size)\n","generator = GeneratorTransferredSampled(hidden_size,vocab_size, line_embed, gamma = gamma)\n","if attention:\n","  attention_mech = BahdanauAttention(hidden_size, key_size=hidden_size)\n","  decoder = AttentionDecoder(embed_size, hidden_size, attention=attention_mech, \n","                             max_len=vocab_size, generator = generator,dropout=dropout)\n","else:\n","  decoder = Decoder(embed_size, hidden_size, max_len=vocab_size, generator = generator, dropout=dropout)\n","\n","# classifier = LSTMClassifier(embed_size, hidden_size_z, vocab_size, dropout =0.2)\n","classifier = LSTMClassifier()\n","\n","discriminator0 = LSTMDiscriminator(hidden_size, hidden_size).to(device)\n","discriminator1 = LSTMDiscriminator(hidden_size, hidden_size).to(device)\n","\n","# BEAM SEARCH\n","beamSeasrch = BeamSearch(decoder, 3,3,line_embed,max_len)\n","\n","if attention:\n","  model = TSTModelAttention(dataset.max_seq_length, vocab_size, embed_size, \n","                            hidden_size_z, hidden_size_y, line_embed, encoder, \n","                            generator, decoder, classifier,\n","                            beamSeasrch).to(device)\n","else:\n","  model = TSTModel(dataset.max_seq_length, vocab_size, embed_size, hidden_size_z, \n","                   hidden_size_y, line_embed, encoder, generator, decoder, classifier,\n","                   beamSeasrch).to(device)\n","\n","optimizer_model = torch.optim.Adam(model.parameters(), lr=lr) \n","optimizer_discr = torch.optim.Adam(list(discriminator0.parameters()) + list(discriminator1.parameters()), lr=lr) \n","\n","# rec_loss = nn.CrossEntropyLoss(reduction=\"mean\",ignore_index = PAD_INDEX)\n","rec_loss = nn.NLLLoss(reduction=\"mean\",ignore_index = PAD_INDEX)\n","# class_loss = nn.BCEWithLogitsLoss(reduction=\"mean\")\n","class_loss = nn.BCELoss()\n","discr_loss = nn.BCELoss()\n","\n","epoch_losses = []\n","for epoch in range(epochs):\n","  epoch_loss = 0\n","  epoch_class_loss = 0\n","  epoch_rec_loss = 0\n","  epoch_adv_loss = 0\n","  epoch_loss_d = 0\n","  epoch_tokens = 0\n","  model.train()\n","  for i,(taylor_lines, taylor_len, drake_lines, drake_len) in enumerate(train_loader):\n","    lines = torch.cat((taylor_lines, drake_lines), 0).to(device)    \n","    line_lens = torch.cat((taylor_len, drake_len), 0).to(device)\n","    labels = torch.cat((torch.ones(size=(len(taylor_lines),), dtype=torch.int32),torch.zeros(size=(len(drake_lines),),dtype=torch.int32))).to(device)\n","    classifier_labels = torch.cat((labels,1-labels, labels))\n","    \n","    fake_labels = torch.cat((torch.zeros(size=(len(taylor_lines),), dtype=torch.int32),torch.ones(size=(len(taylor_lines),),dtype=torch.int32))).to(device)\n","    fake_labels = fake_labels\n","\n","    # Train discriminator\n","\n","    if discriminate:\n","      rec_orig, pred_class, decode_orig, decode_tsf, pred_fake0, pred_fake1 = model(lines, line_lens, labels)\n","      \n","      pred_fake0 = discriminator0(pred_fake0[0], pred_fake0[1])\n","      pred_fake1 = discriminator1(pred_fake1[0], pred_fake1[1])\n","\n","      loss_d0 = discr_loss(pred_fake0, fake_labels.to(torch.float))\n","      loss_d1 = discr_loss(pred_fake1, fake_labels.to(torch.float))\n","      loss_d = loss_d0 + loss_d1\n","\n","      optimizer_discr.zero_grad()\n","      loss_d.backward()\n","      optimizer_discr.step()\n","\n","    # Train model\n","\n","    rec_orig, pred_class, decode_orig, decode_tsf, pred_fake0, pred_fake1 = model(lines, line_lens, labels)\n","\n","    loss_rec = rec_loss(input=rec_orig[2].permute(0,2,1), target=lines[:, 1:])\n","\n","    loss = loss_rec\n","    \n","    if discriminate:\n","      pred_fake0 = discriminator0(pred_fake0[0], pred_fake0[1])\n","      pred_fake1 = discriminator1(pred_fake1[0], pred_fake1[1])\n","      loss_adv0 = class_loss(pred_fake0[len(drake_lines):], fake_labels[len(drake_lines):].to(torch.float))\n","      loss_adv1 = class_loss(pred_fake1[len(taylor_lines):], fake_labels[len(taylor_lines):].to(torch.float))\n","      if loss_adv0 < 1.2 and loss_adv1 < 1.2:\n","        loss -= 0.5*(loss_adv0 + loss_adv1)\n","    \n","    if classify:\n","      loss_class = class_loss(pred_class, classifier_labels.to(torch.float))\n","      loss += loss_class\n","\n","    optimizer_model.zero_grad()\n","    loss.backward()\n","    optimizer_model.step()\n","    \n","    epoch_loss += loss.item()\n","    epoch_rec_loss += loss_rec.item() * line_lens.sum().item()\n","    epoch_tokens += line_lens.sum().item()\n","\n","    if discriminate:\n","      epoch_loss_d += loss_d.item()\n","      epoch_adv_loss += (loss_adv0.item() + loss_adv1.item())\n","      \n","    if classify:\n","      epoch_class_loss += loss_class.item()\n","\n","    if model.training and i % print_every == 0:\n","      print(\"Epoch Step: %d Loss: %f\" % (i, loss.item()))\n","  \n","  epoch_losses.append(epoch_loss)\n","  print(\"Finished Training Epoch \", epoch)\n","  print(\"Training PPL\", np.exp(epoch_rec_loss / float(epoch_tokens)))\n","\n","  if discriminate:\n","    print(\"Adversarial Loss\", epoch_adv_loss)\n","    print(\"Discriminator Loss\", epoch_loss_d)\n","  \n","  if classify:\n","    print(\"Classification Loss\", epoch_class_loss)\n","\n","  val_loss = 0\n","  val_tokens = 0\n","  val_class_loss = 0\n","  correct_pred = 0\n","  correct_pred_all = 0\n","  correct_pred_drake = 0\n","  correct_pred_tay = 0\n","\n","  for i,(taylor_lines, taylor_len, drake_lines, drake_len) in enumerate(valid_loader):\n","    lines = torch.cat((taylor_lines, drake_lines), 0).to(device)    \n","    line_lens = torch.cat((taylor_len, drake_len), 0).to(device)\n","    labels = torch.cat((torch.ones(size=(len(taylor_lines),), dtype=torch.int32),torch.zeros(size=(len(drake_lines),),dtype=torch.int32))).to(device)\n","    classifier_labels = torch.cat((labels,1-labels, labels))\n","    \n","    fake_labels = torch.cat((torch.zeros(size=(len(taylor_lines),), dtype=torch.int32),torch.ones(size=(len(taylor_lines),),dtype=torch.int32))).to(device)\n","\n","    rec_orig, pred_class, decode_orig, decode_tsf, pred_fake0, pred_fake1 = model(lines, line_lens, labels)\n","    pred_class_beam, decode_orig_beam, decode_tsf_beam = model.forward_beam(lines, line_lens, labels)\n","    loss_rec = rec_loss(input=rec_orig[2].permute(0,2,1), target=lines[:, 1:])\n","    # loss_class = class_loss(pred_class, classifier_labels.to(torch.float))\n","\n","    val_loss += loss_rec.item() * line_lens.sum().item()\n","    val_tokens += line_lens.sum().item()\n","    # val_class_loss += loss_class.item()*classifier_labels.size(0)\n","\n","    if classify:\n","      correct_pred += torch.sum((pred_class[-len(lines):] >= 0.5) == classifier_labels[-len(lines):])\n","      correct_pred_all += torch.sum((pred_class >= 0.5) == classifier_labels)\n","\n","    if discriminate:\n","      pred_fake0 = discriminator0(pred_fake0[0], pred_fake0[1])\n","      pred_fake1 = discriminator1(pred_fake1[0], pred_fake1[1])\n","\n","      correct_pred_drake += torch.sum((pred_fake0 >= 0.5) == fake_labels) \n","      correct_pred_tay += torch.sum((pred_fake1 >= 0.5) == fake_labels)\n","    break\n","  \n","  print(\"Valid PPL\", np.exp(val_loss / float(val_tokens)))\n","  if classify:\n","    print(\"Valid Classification Accuracy on True\", correct_pred / (2.*len(valid_dataset)))\n","    print(\"Valid Classification Accuracy on All\", correct_pred_all / (3.*2.*len(valid_dataset)))\n","\n","  if discriminate:\n","    print(\"Valid Classification Accuracy on Drake\", correct_pred_drake / (2.*len(valid_dataset)))\n","    print(\"Valid Classification Accuracy on Taylor\", correct_pred_tay / (2.*len(valid_dataset)))\n","  break"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-90-ceb5c4a44cab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m   \u001b[0mepoch_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtaylor_lines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtaylor_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrake_lines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrake_len\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtaylor_lines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrake_lines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mline_lens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtaylor_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrake_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-83a3331b9ae1>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mtaylor_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtaylor_sent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m                 \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'<unk>'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvocab_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mRARE_WORD_TRESHOLD\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"8GKdhbvFvL9_"},"source":["#### Validation"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vm_7rn0_NsZ_","executionInfo":{"status":"ok","timestamp":1638382971241,"user_tz":300,"elapsed":4795,"user":{"displayName":"Katherine Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11324848797432579426"}},"outputId":"58a91ebf-d021-4428-a145-f9d77cdca9d3"},"source":["lines = lines[0:1,:]\n","line_lens = line_lens[0:1]\n","labels = labels[0:1]\n","decode_orig_beam, decode_tsf_beam = model.forward_beam(lines, line_lens, labels)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["shape of inputs: torch.Size([1, 1, 256])\n","shape of decoder_input: torch.Size([1, 1, 256])\n","shape of encoder_hidden torch.Size([1, 32, 384])\n","shape of hidden: torch.Size([1, 1, 384])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","final_logp_batch.shape: torch.Size([1, 3])\n","final_hidden_batch.shape: torch.Size([3, 1, 1, 384])\n","decoded_batch.shape: torch.Size([1, 3, 32])\n","max_len: 32\n","topk: 3\n","batch size: 1\n","shape of inputs: torch.Size([1, 1, 256])\n","shape of decoder_input: torch.Size([1, 1, 256])\n","shape of encoder_hidden torch.Size([1, 32, 384])\n","shape of hidden: torch.Size([1, 1, 384])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:157: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"]},{"output_type":"stream","name":"stdout","text":["prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","encoder_hidden.shape 2: torch.Size([1, 32, 384])\n","hidden.shape: torch.Size([1, 1, 384])\n","query.shape: torch.Size([1, 1, 384])\n","proj_key.shape: torch.Size([1, 32, 384])\n","encoder_hidden.shape: torch.Size([1, 32, 384])\n","src_mask.shape: torch.Size([1, 1, 32])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","context.shape: torch.Size([1, 1, 384])\n","indexes.shape torch.Size([1, 1, 3])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","decoded_t.shape: torch.Size([1, 1])\n","prev_embed.shape: torch.Size([1, 1, 256])\n","final_logp_batch.shape: torch.Size([1, 3])\n","final_hidden_batch.shape: torch.Size([3, 1, 1, 384])\n","decoded_batch.shape: torch.Size([1, 3, 32])\n","max_len: 32\n","topk: 3\n","batch size: 1\n"]}]},{"cell_type":"code","metadata":{"id":"kbCiQ75EVHTZ"},"source":["import operator\n","val_loss = 0\n","val_tokens = 0\n","val_class_loss = 0\n","correct_pred = 0\n","correct_pred_all = 0\n","correct_pred_drake = 0\n","correct_pred_tay = 0\n","\n","for i,(taylor_lines, taylor_len, drake_lines, drake_len) in enumerate(valid_loader):\n","  lines = torch.cat((taylor_lines, drake_lines), 0).to(device)    \n","  line_lens = torch.cat((taylor_len, drake_len), 0).to(device)\n","  \n","  labels = torch.cat((torch.ones(size=(len(taylor_lines),), dtype=torch.int32),torch.zeros(size=(len(drake_lines),),dtype=torch.int32))).to(device)\n","  classifier_labels = torch.cat((labels,1-labels, labels))\n","  \n","  fake_labels = torch.cat((torch.zeros(size=(len(taylor_lines),), dtype=torch.int32),torch.ones(size=(len(taylor_lines),),dtype=torch.int32))).to(device)\n","  print(\"lines.shape\",lines.shape)\n","  print(\"line_lens:\",line_lens)\n","  print(\"labels:\",labels)\n","  \n","  rec_orig, pred_class, decode_orig, decode_tsf, pred_fake0, pred_fake1 = model(lines, line_lens, labels)\n","  pred_class_beam, decode_orig_beam, decode_tsf_beam = model.forward_beam(lines, line_lens, labels)\n","  loss_rec = rec_loss(input=rec_orig[2].permute(0,2,1), target=lines[:, 1:])\n","  # loss_class = class_loss(pred_class, classifier_labels.to(torch.float))\n","\n","  val_loss += loss_rec.item() * line_lens.sum().item()\n","  val_tokens += line_lens.sum().item()\n","  # val_class_loss += loss_class.item()*classifier_labels.size(0)\n","\n","  if classify:\n","    correct_pred += torch.sum((pred_class[-len(lines):] >= 0.5) == classifier_labels[-len(lines):])\n","    correct_pred_all += torch.sum((pred_class >= 0.5) == classifier_labels)\n","\n","  if discriminate:\n","    pred_fake0 = discriminator0(pred_fake0[0], pred_fake0[1])\n","    pred_fake1 = discriminator1(pred_fake1[0], pred_fake1[1])\n","\n","    correct_pred_drake += torch.sum((pred_fake0 >= 0.5) == fake_labels) \n","    correct_pred_tay += torch.sum((pred_fake1 >= 0.5) == fake_labels)\n","  break\n","\n","print(\"Valid PPL\", np.exp(val_loss / float(val_tokens)))\n","if classify:\n","  print(\"Valid Classification Accuracy on True\", correct_pred / (2.*len(valid_dataset)))\n","  print(\"Valid Classification Accuracy on All\", correct_pred_all / (3.*2.*len(valid_dataset)))\n","\n","if discriminate:\n","  print(\"Valid Classification Accuracy on Drake\", correct_pred_drake / (2.*len(valid_dataset)))\n","  print(\"Valid Classification Accuracy on Taylor\", correct_pred_tay / (2.*len(valid_dataset)))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y6lRAikKW0EZ","executionInfo":{"status":"ok","timestamp":1638383113085,"user_tz":300,"elapsed":111,"user":{"displayName":"Katherine Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11324848797432579426"}},"outputId":"9d105d13-8cfb-41ef-8e40-e6983c7fa621"},"source":["idx=15\n","\n","def lookup_words(x, vocab):\n","  return [vocab[i] for i in x]\n","\n","# print(lookup_words(lines[idx], vocab))\n","# print(lookup_words(rec_orig[3][idx], vocab))\n","print(lookup_words(decode_orig[3][idx], vocab))\n","print(lookup_words(decode_tsf[3][idx], vocab))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['houstatlantavegas', 'huh', 'achilles', 'scorching', 'post', 'wham', 'jazeera', 'meditation', 'shutting', 'chanel', 'talking', 'aggressive', 'absent-mindedly', 'inclusive', '550', 'shoots', 'strap', 'trickin', '2011/2012', 'stations', '$', 'asleep', 'ah-oh', 'autopilot', 'outta', 'doa', 'alone', 'found', 'field', 'prank', 'dignified']\n","['try', 'commitment', 'myspace', 'rockout', 'quizno', 'entertaining', 'realized', 'naps', 'soften', 'scarin', 'killed', 'knives', 'b.b', 'greet', 'coachella', 'pussyholes', 'unforgivin', 'called', 'napkins', 'gangsta', 'j.u.s.t.i.c.e', 'cosmetic', 'glue', 'mind', 'bold', 'backyards', 'adventures', 'just', 'tussle', 'unleveled', 'code']\n"]}]},{"cell_type":"code","metadata":{"id":"JvQZ9bkbPlKB"},"source":[""],"execution_count":null,"outputs":[]}]}