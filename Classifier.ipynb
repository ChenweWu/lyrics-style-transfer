{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Classifier.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN/FAmwvKEqfwcMSU1e1nHx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"N4ZOvaczksb9"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from tqdm import tqdm\n","import os\n","import random\n","\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","assert device == \"cuda\"  \n","\n","seed = 42\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","\n","\n","# input [Batch (sentence), L (max line length), V (corpus size)] with probabilities \n","# output : [B,2]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X8SteiVnplL9"},"source":["embedding_size = 256\n","seq_length = 32\n","print(vocab_size)\n","\n","embedding = nn.Embedding(vocab_size, embedding_size, padding_idx=0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8TX3d2DvkmPh"},"source":["# LSTM (RNN) Classifier"]},{"cell_type":"code","metadata":{"id":"ReZNlEcei3G7"},"source":["#LSTM Classifier\n","\n","batch_size = 4\n","LSTMlayers = 3\n","vocab_size = len(unique(vocab_counts))\n","\n","\n","lstm = nn.LSTM(input_size=256, hidden_size=256, num_layers=LSTMlayers, batch_first=True)\n","dropout = nn.Dropout(0.5)\n","fc1 =  nn.Linear(in_features=256, out_features=257)\n","fc2 = nn.Linear(257, 1)\n","\n","# def initialize_states():\n","#   initial_hidden_state:\n","#   initial_memory_state: torch.zeros()\n","\n","def LSTM_forward(seq):\n","  # hidden_state = \n","  seq = embedding(seq)\n","  output, (hidden,cell) = lstm(seq)\n","  output = dropout(output)\n","  print(output.shape)\n","  output = fc1(output[:,-1])\n","  print(output.shape)\n","  output = F.relu(output)\n","  output = fc2(output)\n","  output = F.sigmoid(output)\n","\n","  return output.reshape(output.size(1), -1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IDaTRllukmjI"},"source":["# CNN Classifier"]},{"cell_type":"code","metadata":{"id":"056zpXdVkvHV"},"source":["filter_size = [2,3,4]\n","n_filters = [100,100,100]\n","n_class = 2\n","fc = nn.Linear(np.sum(n_filters), 1)\n","dropout = nn.Dropout(0.5)\n","\n","def CNN_forward(seq):\n","      seq = embedding(seq)\n","      seq_reshaped = seq.permute(0,2,1)\n","      conv_list = []\n","      pool_list = []\n","      for convolution1d in nn.ModuleList([nn.Conv1d(256, n_filters[i],filter_size[i]) for i in range(3)]):\n","        conv_list.append(F.relu(convolution1d(seq_reshaped)))\n","      \n","      for conv in conv_list:\n","        pool_list.append(F.max_pool1d(conv, conv.shape[2]))\n","      \n","      concat = []\n","      for pool in pool_list:\n","        concat.append(pool.squeeze(dim=2))      \n","      concat = torch.cat(concat, dim=1)\n","\n","      output = fc(concat)\n","      output = dropout(output)\n","      output = torch.sigmoid(output)\n","\n","      # return logits\n","      return output.reshape(output.size(1), -1)\n","      # output looks like : tensor([[0.9315, 0.5000]], grad_fn=<ViewBackward>)\n","\n","loss = 0 \n","def loss_fcn(taylor_batch, drake_batch): \n","  target_taylor =torch.tensor([[1,1]])\n","  target_drake =torch.tensor([[0,0]])   \n","  target_taylor=target_taylor.type(torch.FloatTensor)\n","  target_drake=target_drake.type(torch.FloatTensor)\n","\n","  #define and add optimizer\n","  taylor_loss = F.binary_cross_entropy(taylor_batch,target_taylor)\n","  drake_loss = F.binary_cross_entropy(drake_batch,target_drake)\n","  return taylor_loss, drake_loss\n","\n","# loss += loss_fcn[0] + loss_fcn[1]"],"execution_count":null,"outputs":[]}]}