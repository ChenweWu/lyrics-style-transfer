{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"model saving","provenance":[],"collapsed_sections":["S3i2hvtYoo30","ynBTHa5grUr7","q96M0-eW9OBl","HeRBfORu9P49","iaAnYmyv9gbu","kxvEOOu_9Zkt","t2mg2wYk30dv","wg_FbKl-T9xd","9XEGiaaa93DR","5WANFTxxpPjW","vfpA5Nvd-oHa"],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"m7FQaWQbojAb"},"source":["### Import and Setup"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wyz5JwHDopli","executionInfo":{"status":"ok","timestamp":1638560406780,"user_tz":300,"elapsed":6994,"user":{"displayName":"Sirui Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09569618597630381386"}},"outputId":"3b9a3a08-ce0d-4d6b-af1a-031d98a99658"},"source":["!pip install unidecode"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting unidecode\n","  Downloading Unidecode-1.3.2-py3-none-any.whl (235 kB)\n","\u001b[?25l\r\u001b[K     |█▍                              | 10 kB 17.8 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 20 kB 16.9 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 30 kB 10.7 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 40 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |███████                         | 51 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 61 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 71 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 81 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 92 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 102 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 112 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 122 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 133 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 143 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 153 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 163 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 174 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 184 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 194 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 204 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 215 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 225 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 235 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 235 kB 5.5 MB/s \n","\u001b[?25hInstalling collected packages: unidecode\n","Successfully installed unidecode-1.3.2\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T8q-CET4os1p","executionInfo":{"status":"ok","timestamp":1638560465981,"user_tz":300,"elapsed":59213,"user":{"displayName":"Sirui Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09569618597630381386"}},"outputId":"988942c8-8113-47cf-b040-626a04c8afc2"},"source":["import json\n","import re\n","from unidecode import unidecode\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","import nltk\n","nltk.download('punkt')\n","from collections import Counter\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"0ELBqq8kpAZS"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","assert device == \"cuda\"  \n","\n","seed = 42\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S3i2hvtYoo30"},"source":["### Data"]},{"cell_type":"code","metadata":{"id":"OGzVRiwZowGv"},"source":["f = open('/content/drive/Shareddrives/MIT NLP 8.864/Data/drake.json')\n","drake = json.load(f)\n","f.close()\n","\n","f = open('/content/drive/Shareddrives/MIT NLP 8.864/Data/tswift.json')\n","taylor = json.load(f)\n","f.close()\n","\n","drake = [drake['songs'][i]['lyrics'] for i in range(len(drake['songs']))]\n","taylor = [taylor['songs'][i]['lyrics'] for i in range(len(taylor['songs']))]\n","\n","taylor_lyrics = [re.sub('\\u2005', ' ', re.sub(r'[\\(\\[].*?[\\)\\]]', '', taylor[i])).split('\\n') for i in range(len(taylor))]\n","taylor_lyrics = [[unidecode(i) for i in taylor_lyrics[j]] for j in range(len(taylor_lyrics))]\n","taylor_lyrics = [[re.sub('\\d+EmbedShare URLCopyEmbedCopy', '', i) for i in taylor_lyrics[j]] for j in range(len(taylor_lyrics))]\n","taylor_lyrics = [[re.sub('\\d+.EmbedShare URLCopyEmbedCopy', '', i) for i in taylor_lyrics[j]] for j in range(len(taylor_lyrics))]\n","taylor_lyrics = [[re.sub('EmbedShare URLCopyEmbedCopy', '', i) for i in taylor_lyrics[j]] for j in range(len(taylor_lyrics))]\n","taylor_lyrics = [[i for i in taylor_lyrics[j] if i != ''] for j in range(len(taylor_lyrics))]\n","\n","drake_lyrics = [re.sub('\\u2005', ' ', re.sub(r'[\\(\\[].*?[\\)\\]]', '', drake[i])).split('\\n') for i in range(len(drake))]\n","drake_lyrics = [[unidecode(i) for i in drake_lyrics[j]] for j in range(len(drake_lyrics))]\n","drake_lyrics = [[re.sub('\\d+EmbedShare URLCopyEmbedCopy', '', i) for i in drake_lyrics[j]] for j in range(len(drake_lyrics))]\n","drake_lyrics = [[re.sub('\\d+.EmbedShare URLCopyEmbedCopy', '', i) for i in drake_lyrics[j]] for j in range(len(drake_lyrics))]\n","drake_lyrics = [[re.sub('EmbedShare URLCopyEmbedCopy', '', i) for i in drake_lyrics[j]] for j in range(len(drake_lyrics))]\n","drake_lyrics = [[i for i in drake_lyrics[j] if i != ''] for j in range(len(drake_lyrics))]\n","\n","# taylor_lyrics = [[line1 + ', ' + line2 for line1,line2 in zip(song[0::2], song[1::2])] for song in taylor_lyrics]\n","# drake_lyrics = [[line1 + ', ' + line2 for line1,line2 in zip(song[0::2], song[1::2])] for song in drake_lyrics]\n","\n","drake_tokenized = [[word_tokenize(drake_lyrics[i][j]) for j in range(len(drake_lyrics[i]))] for i in range(len(drake_lyrics))]\n","taylor_tokenized = [[word_tokenize(taylor_lyrics[i][j]) for j in range(len(taylor_lyrics[i]))] for i in range(len(taylor_lyrics))]\n","\n","drake_tokenized = [[[word.lower() for word in line] for line in song] for song in drake_tokenized]\n","taylor_tokenized = [[[word.lower() for word in line] for line in song] for song in taylor_tokenized]\n","\n","drake_length = sum([[len(sent) for sent in song] for song in drake_tokenized], [])\n","taylor_length = sum([[len(sent) for sent in song] for song in taylor_tokenized], [])\n","\n","# drake_lyrics = sum([[sent for sent in song if (len(sent) >= 10 and len (sent) <= 30)] for song in drake_tokenized], [])\n","# taylor_lyrics = sum([[sent for sent in song if (len(sent) >= 10 and len (sent) <= 30)] for song in taylor_tokenized], [])\n","\n","drake_lyrics = sum([[sent for sent in song if (len(sent) >= 5 and len (sent) <= 15)] for song in drake_tokenized], [])\n","taylor_lyrics = sum([[sent for sent in song if (len(sent) >= 5 and len (sent) <= 15)] for song in taylor_tokenized], [])\n","\n","taylor_vocab = sum(taylor_lyrics,[])\n","drake_vocab = sum(drake_lyrics,[])\n","\n","def unique(list1):\n","     \n","    # insert the list to the set\n","    list_set = set(list1)\n","    # convert the set to the list\n","    unique_list = (list(list_set))\n","    return unique_list\n","\n","vocab = taylor_vocab + drake_vocab\n","vocab_counts = Counter(vocab)\n","vocab = unique(vocab)\n","vocab = ['<pad>','<unk>','<s>', '</s>'] + vocab\n","\n","from torch.utils import data\n","import torch\n","\n","# These IDs are reserved.\n","MAX_SENT_LENGTH = 15\n","MAX_SENT_LENGTH_PLUS_SOS_EOS = MAX_SENT_LENGTH + 2\n","PAD_INDEX = 0\n","UNK_INDEX = 1\n","SOS_INDEX = 2\n","EOS_INDEX = 3\n","RARE_WORD_TRESHOLD = 0\n","\n","vocab_counts['<pad>'] = RARE_WORD_TRESHOLD + 1\n","vocab_counts['<unk>'] = RARE_WORD_TRESHOLD + 1\n","vocab_counts['<s>'] = RARE_WORD_TRESHOLD + 1\n","vocab_counts['</s>'] = RARE_WORD_TRESHOLD + 1\n","\n","class TSTDataset(data.Dataset):\n","    def __init__(self, taylor_sentences, drake_sentences, vocab, vocab_counts, sampling=1.):\n","        self.taylor_sentences = taylor_sentences[:int(len(taylor_sentences) * sampling)]\n","        self.drake_sentences = drake_sentences[:int(len(drake_sentences) * sampling)]\n","\n","        self.max_seq_length = MAX_SENT_LENGTH_PLUS_SOS_EOS\n","        self.vocab = vocab\n","        self.vocab_counts = vocab_counts\n","\n","        self.v2id = {v : i for i, v in enumerate(self.vocab)}\n","        self.id2v = {val : key for key, val in self.v2id.items()}\n","    \n","    def __len__(self):\n","        return min(len(self.taylor_sentences), len(self.drake_sentences))\n","    \n","    def __getitem__(self, index):\n","        taylor_sent = self.taylor_sentences[index]\n","        taylor_len = len(taylor_sent) + 2   # add <s> and </s> to each sentence\n","        taylor_id = []\n","        for w in taylor_sent:\n","            if w not in self.vocab:\n","                w = '<unk>'\n","            if vocab_counts[w] <= RARE_WORD_TRESHOLD:\n","                w = '<unk>'\n","            taylor_id.append(self.v2id[w])\n","\n","        taylor_id = ([SOS_INDEX] + taylor_id + [EOS_INDEX] + [PAD_INDEX] *\n","                  (self.max_seq_length - taylor_len))\n","\n","        drake_sent = self.drake_sentences[index]\n","        drake_len = len(drake_sent) + 2   # add <s> and </s> to each sentence\n","        drake_id = []\n","        for w in drake_sent:\n","            if w not in self.vocab:\n","                w = '<unk>'\n","            if vocab_counts[w] <= RARE_WORD_TRESHOLD:\n","                w = '<unk>'\n","            drake_id.append(self.v2id[w])\n","\n","        drake_id = ([SOS_INDEX] + drake_id + [EOS_INDEX] + [PAD_INDEX] *\n","                  (self.max_seq_length - drake_len))\n","\n","        return torch.tensor(taylor_id), taylor_len, torch.tensor(drake_id), drake_len\n","\n","dataset = TSTDataset(taylor_lyrics, drake_lyrics, vocab, vocab_counts)\n","\n","test_pct = 0.2\n","valid_pct = 0.1\n","\n","train_dataset, test_dataset = torch.utils.data.random_split(dataset, [int(len(dataset)*(1-test_pct)),len(dataset)-int(len(dataset)*(1-test_pct))])\n","valid_dataset, train_dataset = torch.utils.data.random_split(train_dataset, [int(len(dataset)*valid_pct),len(train_dataset)-int(len(dataset)*valid_pct)])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ynBTHa5grUr7"},"source":["### Encoder"]},{"cell_type":"code","metadata":{"id":"FieXza5erYUH"},"source":["from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","\n","class Encoder(nn.Module):\n","  def __init__(self, input_size, hidden_size, dropout=0.):\n","    \"\"\"\n","    Inputs: \n","      - `input_size`: an int representing the RNN input size.\n","      - `hidden_size`: an int representing the RNN hidden size.\n","      - `dropout`: a float representing the dropout rate during training. Note\n","          that for 1-layer RNN this has no effect since dropout only applies to\n","          outputs of intermediate layers.\n","    \"\"\"\n","    super(Encoder, self).__init__()\n","    self.rnn = nn.GRU(input_size, hidden_size, num_layers=1, batch_first=True,\n","                      dropout=dropout, bidirectional=False)\n","\n","  def forward(self, inputs, lengths, init_state=None):\n","    \"\"\"\n","    Inputs:\n","      - `inputs`: a 3d-tensor of shape (batch_size, max_seq_length, embed_size)\n","          representing a batch of padded embedded word vectors of source\n","          sentences.\n","      - `lengths`: a 1d-tensor of shape (batch_size,) representing the sequence\n","          lengths of `inputs`.\n","\n","    Returns:\n","      - `outputs`: a 3d-tensor of shape\n","        (batch_size, max_seq_length, hidden_size).\n","      - `finals`: a 3d-tensor of shape (num_layers, batch_size, hidden_size).\n","      Hint: `outputs` and `finals` are both standard GRU outputs. Check:\n","      https://pytorch.org/docs/stable/nn.html#gru\n","    \"\"\"\n","    # Our variable-length inputs are padded to the same length for batching\n","    # Here we \"pack\" them for computational efficiency (see note below)\n","    packed = pack_padded_sequence(inputs, lengths.cpu(), batch_first=True,\n","                                  enforce_sorted=False)\n","    outputs, finals = self.rnn(packed, init_state)\n","    outputs, _ = pad_packed_sequence(outputs, batch_first=True,\n","                                     total_length=MAX_SENT_LENGTH_PLUS_SOS_EOS)\n","    return outputs, finals"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rKiJAkvZGMWl"},"source":["### Decoder"]},{"cell_type":"markdown","metadata":{"id":"q96M0-eW9OBl"},"source":["#### Generator"]},{"cell_type":"code","metadata":{"id":"E1Tdf6_283WY"},"source":["class GeneratorTransferredSampled(nn.Module):\n","  \"\"\"Define standard linear + softmax generation step.\"\"\"\n","  def __init__(self, hidden_size, vocab_size, src_embed, gamma=0.001):\n","    \"\"\"\n","    Inputs:\n","      - `src_embed`: a 2d-tensor of shape (vocab_size, embed_size )\n","    \"\"\"\n","    super(GeneratorTransferredSampled, self).__init__()\n","    self.proj = nn.Linear(hidden_size, vocab_size, bias=True)\n","    self.gamma = gamma\n","    self.logsoftmax = nn.LogSoftmax(dim = 2)\n","    self.softmax = nn.Softmax(dim = 2)\n","    self.src_embed = src_embed\n","\n","  def embedding(self,x):\n","    return torch.matmul(x,self.src_embed.weight)\n","    \n","  def gumbel_softmax(self,logits, eps=1e-20):\n","    U = torch.rand(logits.shape).to(device)\n","    G = -torch.log(-torch.log(U + eps) + eps).to(device)\n","    return self.logsoftmax((logits + G) / self.gamma)\n","\n","  def gumbel(self,logits, eps=1e-20):\n","    U = torch.rand(logits.shape).to(device)\n","    G = -torch.log(-torch.log(U + eps) + eps).to(device)\n","    return (logits + G) / self.gamma\n","\n","  def forward(self, x):\n","    logits = self.proj(x)\n","    logprob = self.logsoftmax(logits)\n","    prob = self.softmax(logits)\n","    output = self.embedding(prob)\n","    word  = logits.argmax(dim = 2, keepdim = False)\n","\n","    return output, logprob, word\n","\n","  def forward_gumbel(self, x):\n","    logits = self.proj(x)\n","    prob = self.softmax(self.gumbel(logits))\n","    logprob = self.logsoftmax(self.gumbel(logits))\n","    output = self.embedding(prob)\n","    word  = logits.argmax(dim = 2, keepdim = False)\n","\n","    return output, logprob, word"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HeRBfORu9P49"},"source":["#### Basic Decoder"]},{"cell_type":"code","metadata":{"id":"plJpAyPu86yw"},"source":["class Decoder(nn.Module):\n","  \"\"\"An RNN decoder + generator with GRU\"\"\"\n","\n","  def __init__(self, input_size, hidden_size, max_len,generator, num_layers = 1, dropout=0.):\n","    \"\"\"\n","      Inputs:\n","        - `input_size` , `hidden_size`, and `dropout` the same as in Encoder.\n","    \"\"\"\n","    super(Decoder, self).__init__()\n","    self.input_size = input_size\n","    self.hidden_size = hidden_size\n","    self.rnn = nn.GRU(input_size, hidden_size, num_layers=num_layers, batch_first=True,\n","                      dropout=dropout, bidirectional=False)\n","    self.generator = generator\n","    self.max_len = max_len\n","    self.dropout_layer = nn.Dropout(p=dropout)\n","    self.rnn_to_pre = nn.Linear(input_size + hidden_size,\n","                                hidden_size, bias=False)\n","\n","  def forward_step(self, prev_embed, hidden):\n","    \"\"\"Helper function for forward below:\n","       Perform a single decoder step (1 word).\n","\n","       Inputs:\n","      - `prev_embed`: a 3d-tensor of shape (batch_size, 1, embed_size = vocab_size)\n","          representing the padded embedded word vectors at this step in training\n","      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size) representing\n","          the current hidden state.\n","\n","      Returns:\n","      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size)\n","          representing the current decoder hidden state.\n","      - `output`: a 3d-tensor of shape (batch_size, max_len, vocab_size)\n","          representing the total generated outputs.\n","    \"\"\"\n","    pre_output, hidden = self.rnn(prev_embed, hidden)\n","    pre_output = torch.cat([prev_embed, pre_output], dim=2)\n","    pre_output = self.dropout_layer(pre_output)\n","    pre_output = self.rnn_to_pre(pre_output)\n","    pre_output = torch.tanh(pre_output)\n","\n","    return hidden, pre_output\n","\n","    ### Your code here!\n","    pre_output, hidden = self.rnn(prev_embed, hidden)\n","    pre_output = torch.cat([prev_embed, pre_output], dim=2)\n","    pre_output = self.dropout_layer(pre_output)\n","    pre_output = self.rnn_to_pre(pre_output)\n","    pre_output = self.pre_activation(pre_output)\n","    \n","  def forward_step_beam(self, prev_embed, encoder_hidden, \n","                   src_mask, proj_key, hidden):\n","    \"\"\"Beam Search only: Unroll the decoder one step at a time.\n","    Inputs:\n","      - `input`: a 3d-tensor of shape (batch_size, 1, embed_size)\n","          representing the padded embedded word vectors at this step in training\n","      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size) representing\n","          the current hidden state.\n","    Returns:\n","      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size)\n","          representing the current decoder hidden state.\n","      - `output`: a 3d-tensor of shape (batch_size, 1, vocab_size)\n","          representing the total generated outputs.    \n","      - `gumbel_logits`: a 3d-tensor of shape\n","          (batch_size, 1, trg_vocab_size) representing the mapped decoder\n","          outputs from gumbel softmax.\n","      - `output_word`: a 2d-tensor of shape\n","          (batch_size, 1) representing output sentence and\n","          the corresponding word index (can be used for embedding)    \n","      - `logits`: a 2d-tensor of shape\n","          (batch_size, 1, trg_vocab_size) representing the mapped decoder\n","          outputs from log softmax \n","          \"\"\"\n","    temp_hidden, pre_output = self.forward_step(prev_embed,encoder_hidden, \n","                   src_mask, proj_key, hidden)\n","    output, logits, output_word = self.generator.forward_gumbel(pre_output)\n","    return  temp_hidden, output, logits, output_word\n","\n","  def forward(self, input, encoder_finals,max_len, hidden=None):\n","    \"\"\"Unroll the decoder one step at a time.\n","\n","    Inputs:\n","      - `inputs`: a 3d-tensor of shape (batch_size, 1, embed_size)\n","          representing a batch of padded embedded word vectors of SOS . \n","          If size is (batch_size,max_len, embed_size), then it is teacher forcing.\n","      - `encoder_finals`: a 3d-tensor of shape\n","          (num_enc_layers, batch_size, hidden_size) representing the final\n","          encoder hidden states used to initialize the initial decoder hidden\n","          states.\n","      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size) representing\n","          the value to be used to initialize the initial decoder hidden states.\n","          If None, then use `encoder_finals`.\n","      - `max_len`: an int representing the maximum decoding length.\n","      - `style`: TAYLOR_STYLE or DRAKE_STYLE\n","\n","    Returns:\n","      - `hidden`: a 3d-tensor of shape\n","          (num_layers, batch_size, hidden_size) representing the final hidden\n","          state for each element in the batch.\n","      - `outputs`: a 3d-tensor of shape\n","          (batch_size, max_len, hidden_size) representing the raw decoder\n","          outputs (before mapping to a `trg_vocab_size`-dim vector).\n","      - `logits_vectors`: a 3d-tensor of shape\n","          (batch_size, max_len, trg_vocab_size) representing the mapped decoder\n","          outputs.\n","      - `words`: a 3d-tensor of shape\n","          (batch_size, max_len, 1) representing output sentence and\n","          the corresponding word index (can be used for embedding)  \n","    \"\"\"\n","\n","    # Initialize decoder hidden state.\n","    if hidden is None:\n","      hidden = self.init_hidden(encoder_finals)\n","    output_vectors = []\n","    logits_vectors = []\n","    words = []\n","    hidden_states = []\n","    hidden_states.append(hidden[-1][:,None,:])\n","    for i in range(max_len-1) :\n","      hidden, prev_output = self.forward_step(input,hidden)\n","      input, logits, output_word = self.generator.forward_gumbel(prev_output)\n","      # input, logits, output_word = self.generator(prev_output)\n","\n","      # input = torch.concat([input,torch.full(input.shape,style)], axis = -1)\n","      logits_vectors.append(logits)\n","      output_vectors.append(input)\n","      words.append(output_word)\n","      hidden_states.append(prev_output)\n","\n","    outputs = torch.cat(output_vectors, dim =1)\n","    logits_vectors = torch.cat(logits_vectors,dim = 1)\n","    words = torch.cat(words, axis = -1)\n","    hidden_states = torch.cat(hidden_states, axis = 1)\n","    return hidden, outputs , logits_vectors, words, hidden_states\n","\n","  def forward_teacher(self, input, encoder_finals, max_len=None, hidden=None):\n","    \"\"\"Unroll the decoder one step at a time.\n","\n","    Inputs:\n","      - `inputs`: a 3d-tensor of shape (batch_size,max_len, embed_size)\n","          representing a batch of padded embedded word vectors of original \n","          sentence and acts as  teacher forcing.\n","\n","    Returns:\n","      - `hidden`: a 3d-tensor of shape\n","          (num_layers, batch_size, hidden_size) representing the final hidden\n","          state for each element in the batch.\n","      - `outputs`: a 3d-tensor of shape\n","          (batch_size, max_len, hidden_size) representing the raw decoder\n","          outputs (before mapping to a `trg_vocab_size`-dim vector).\n","      - `logits_vectors`: a 3d-tensor of shape\n","          (batch_size, max_len, trg_vocab_size) representing the mapped decoder\n","          outputs each represents the probability? \n","      - `words`: a 3d-tensor of shape\n","          (batch_size, max_len, 1) representing output sentence and\n","          the corresponding word index (can be used for embedding)      \n","    \"\"\"\n","\n","    # Initialize decoder hidden state.\n","    if max_len is None:\n","      max_len = input.shape[1]\n","    if hidden is None:\n","      hidden = self.init_hidden(encoder_finals)\n","    output_vectors = []\n","    logits_vectors = []\n","    words = []\n","    hidden_states = []\n","    hidden_states.append(hidden[-1][:,None,:])\n","    for i in range(max_len):\n","      hidden, prev_output = self.forward_step(input[:,i:i+1,:],hidden)\n","      # output, logits, output_word = self.generator(prev_output)\n","      output, logits, output_word = self.generator.forward_gumbel(prev_output)\n","      \n","      logits_vectors.append(logits)\n","      output_vectors.append(output)\n","      words.append(output_word)\n","      hidden_states.append(prev_output)\n","\n","    outputs = torch.cat(output_vectors, dim =1)\n","    logits_vectors = torch.cat(logits_vectors,dim = 1)\n","    words = torch.cat(words, axis = -1)\n","    hidden_states = torch.cat(hidden_states, axis = 1)\n","    return hidden, outputs , logits_vectors, words, hidden_states\n","\n","  def init_hidden(self, encoder_finals):\n","    \"\"\"Use encoder final hidden state to initialize decoder's first hidden\n","       state.\n","\n","       Input: `encoder_finals` is same as in forward()\n","\n","       Returns: \n","         - `decoder_init_hiddens`: a 3d-tensor of shape \n","              (num_layers, batch_size, hidden_size) representing the initial\n","              hidden state of the decoder for each element in the batch \n","    \"\"\"\n","    decoder_init_hiddens = torch.tanh(encoder_finals)\n","    return decoder_init_hiddens"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iaAnYmyv9gbu"},"source":["#### Beam Search"]},{"cell_type":"code","metadata":{"id":"OA_J4dhd9ibD"},"source":["from queue import PriorityQueue\n","\n","class BeamSearchNode:\n","  def __init__(self, hiddenstate, previousNode, cur_embed, wordId, \n","               logProb,  length ):\n","    self.h = hiddenstate\n","    self.prevNode = previousNode\n","    self.cur_embed = cur_embed\n","    self.wordid = wordId\n","    self.logp = logProb\n","    self.leng = length\n","    \n","  def __lt__(self,other):\n","    return self.logp < other.logp\n","\n","  def eval(self, alpha=1.0):\n","    return self.logp \n","    # Add here a function for shaping a reward\n","    # reward = 0\n","    # return self.logp / float(self.leng - 1 + 1e-6) + alpha * reward'\n","\n","class BeamSearch:\n","  def __init__(self,decoder, beam_width, topk, line_embed,max_len,\n","               max_iter=2000):\n","    \"\"\"Use Beam Search to generate a full sentence with the given decoder model\n","  Inputs:\n","      - `decoder`: decoder module with forward_step_beam function\n","      - `beam_width` : the length of the beam \n","      - `max_len`: an int representing the maximum decoding length.\n","      - `max_iter`: The maximum decoding iteration\n","    \"\"\"\n","    self.decoder = decoder\n","    self.beam_width = beam_width\n","    self.topk = topk\n","    self.line_embed = line_embed\n","    self.max_len = max_len\n","    self.max_iter = max_iter\n","\n","  def beam_decode(self, inputs, encoder_hidden, encoder_finals, src_mask, max_len,\n","                  hidden = None):\n","                  # inputs, encoder_finals,src_mask, proj_key, hidden):\n","    \"\"\"Use Beam Search to generate a full sentence with the given decoder model\n","    Inputs:\n","        - `inputs`: a 3d-tensor of shape (batch_size, 1, embed_size)\n","            representing a batch of padded embedded word vectors of SOS . \n","        - `encoder_finals`: a 3d-tensor of shape\n","            (num_enc_layers, batch_size, hidden_size) representing the final\n","            encoder hidden states used to initialize the initial decoder hidden\n","            states.\n","\n","    Returns:\n","        - `final_logp_batch`: a 2d-tensor of shape\n","            (batch_size, sentences_num) representing the probability of generating \n","            the sentence.\n","        - `final_hidden_batch`: a 4d-tensor of shape\n","            (sentences_num, num_layers, batch_size, hidden_size) representing \n","            the final hidden layer\n","        - `decoded_batch`: a 3d-tensor of shape\n","            (batch_size,sentences_num,  max_len) representing output sentence and\n","            the corresponding word index (can be used for embedding)  \n","    \n","    \"\"\"\n","    \n","    decoded_batch = []\n","    final_hidden_batch, final_logp_batch = [],[]\n","    # print(\"shape of encoder_finals:\",encoder_finals.shape)\n","    for i in range(inputs.shape[0]):\n","      if hidden is None:\n","        hidden = self.decoder.init_hidden(encoder_finals[:,i:i+1,:])\n","      decoder_input = inputs[i:i+1,:,:]\n","      # Number of sentence to generate\n","      endnodes = []\n","      number_required = self.topk\n","      proj_key = self.decoder.attention.key_layer(encoder_hidden[i:i+1,:,:])\n","\n","      # starting node -  hidden vector, previous node, cur_embed, word id , logp, length\n","      node = BeamSearchNode(self.decoder.init_hidden(encoder_finals[:,i:i+1,:]), \n","                            None, decoder_input, [[SOS_INDEX]], 0, 1)\n","      nodes = PriorityQueue()\n","\n","      nodes.put((-node.eval(), node))\n","      qsize = 1\n","\n","      while qsize<=self.max_iter:\n","        tocheck = min(nodes.qsize(), self.beam_width)\n","        new_nodes = PriorityQueue()\n","        while tocheck>0:\n","          score, n = nodes.get()\n","          decoder_input = n.cur_embed\n","          decoder_hidden = n.h\n","          if n.leng > self.max_len:\n","            endnodes.append((score, n))\n","            # if we reached maximum # of sentences required\n","            if len(endnodes) >= number_required:\n","                break\n","\n","          # decode for one step using decoder\n","          hidden, _, logsoftmax_logits, wordId = decoder.forward_step_beam(decoder_input, \n","                                                                           encoder_hidden[i:i+1,:,:],\n","                                                                          src_mask[i:i+1,:,:], \n","                                                                          proj_key, \n","                                                                          decoder_hidden)\n","          tocheck -= 1\n","          # PUT HERE REAL BEAM SEARCH OF TOP\n","          log_prob, indexes = torch.topk(logsoftmax_logits, self.beam_width)\n","          for new_k in range(self.beam_width):\n","            \n","            decoded_t = indexes[0][0][new_k].view(1, -1)\n","            log_p = log_prob[0][0][new_k]\n","            prev_embed = self.line_embed(decoded_t)\n","\n","            node = BeamSearchNode(decoder_hidden, n,prev_embed, decoded_t, \n","                                  n.logp + log_p,n.leng + 1)\n","            score = -node.eval()\n","            new_nodes.put((score, node))\n","          qsize += self.beam_width - 1\n","        nodes = new_nodes\n","\n","        if len(endnodes) >= number_required:\n","            break\n","        \n","\n","      # choose nbest paths, back trace them\n","      if len(endnodes) == 0:\n","          endnodes = [nodes.get() for _ in range(self.topk)]\n","\n","      utterances = []\n","      final_logps = []\n","      final_hiddens = []\n","      # final_gumbel_logits = []\n","      for score, n in sorted(endnodes, key=operator.itemgetter(0)):\n","          end_node = n\n","          utterance = []\n","          # gumbel_logits = []\n","          utterance.append(n.wordid[0][0])\n","          # gumbel_logits =gumbel_logits + [n.gumbel_logits]\n","          # back trace\n","          while n.prevNode != None:\n","              n = n.prevNode\n","              utterance.append(n.wordid[0][0])\n","              # if n.gumbel_logits is not None:\n","              #   gumbel_logits =gumbel_logits + [n.gumbel_logits]\n","\n","          utterance = torch.unsqueeze(torch.tensor(utterance[::-1][:self.max_len]), axis = 0)\n","          utterances.append(utterance)\n","          # gumbel_logits = torch.cat(gumbel_logits, dim = 1)\n","          final_logp = end_node.logp \n","          final_hidden = end_node.h\n","          final_logps.append(final_logp)\n","          final_hiddens.append(torch.unsqueeze(torch.tensor(final_hidden), axis = 0))\n","          # final_gumbel_logits.append(gumbel_logits)\n","        \n","      utterances = torch.cat(utterances, axis = 0 )\n","      decoded_batch.append(torch.unsqueeze(utterances, axis = 0))\n","      final_logp_batch.append(torch.unsqueeze(torch.tensor(final_logps), axis = 0))\n","      final_hiddens = torch.cat(final_hiddens, axis = 0)\n","      final_hidden_batch.append(final_hiddens)\n","      # final_gumbel_logits = torch.cat(final_gumbel_logits, axis = 0)\n","      # final_gumbel_logits_batch.append(torch.unsqueeze(final_gumbel_logits, axis = 0))\n","\n","    # decoded_batch size = (batch, topk, sentence_len, 1)\n","    final_logp_batch = torch.cat(final_logp_batch, axis = 0)\n","    final_hidden_batch = torch.cat(final_hidden_batch, axis = 2)\n","    # final_gumbel_logits_batch = torch.cat(final_gumbel_logits_batch, axis = 0)\n","    decoded_batch = torch.cat(decoded_batch, axis = 0)\n","    print(\"final_logp_batch.shape:\",final_logp_batch.shape)\n","    print(\"final_hidden_batch.shape:\",final_hidden_batch.shape)\n","    # print(\"final_gumbel_logits_batch.shape:\",final_gumbel_logits_batch.shape)\n","    print(\"decoded_batch.shape:\",decoded_batch.shape)\n","    print(\"max_len:\", self.max_len)\n","    print(\"topk:\", self.topk)\n","    print(\"batch size:\", inputs.shape[0])\n","    return final_hidden_batch, final_logp_batch,  decoded_batch\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NX25tl4G5vWa"},"source":["### Attention Decoder"]},{"cell_type":"code","metadata":{"id":"VrWt0I5N6ijP"},"source":["class BahdanauAttention(nn.Module):\n","    \"\"\"Implements Bahdanau (MLP) attention\"\"\"\n","    \n","    def __init__(self, hidden_size, key_size=None, query_size=None):\n","        super(BahdanauAttention, self).__init__()\n","        \n","        # We assume a bi-directional encoder so key_size is 2*hidden_size\n","        key_size = 2 * hidden_size if key_size is None else key_size\n","        query_size = hidden_size if query_size is None else query_size\n","\n","        self.key_layer = nn.Linear(key_size, hidden_size, bias=False)\n","        self.query_layer = nn.Linear(query_size, hidden_size, bias=False)\n","        self.energy_layer = nn.Linear(hidden_size, 1, bias=False)\n","        \n","        # to store attention scores\n","        self.alphas = None\n","        \n","    def forward(self, query=None, proj_key=None, value=None, mask=None):\n","        assert mask is not None, \"mask is required\"\n","\n","        # We first project the query (the decoder state).\n","        # The projected keys (the encoder states) were already pre-computated.\n","        query = self.query_layer(query)\n","        \n","        # Calculate scores.\n","        scores = self.energy_layer(torch.tanh(query + proj_key))\n","        scores = scores.squeeze(2).unsqueeze(1)\n","        \n","        # Mask out invalid positions.\n","        # The mask marks valid positions so we invert it using `mask & 0`.\n","        scores.data.masked_fill_(mask == 0, -float('inf'))\n","        \n","        # Turn scores to probabilities.\n","        alphas = F.softmax(scores, dim=-1)\n","        self.alphas = alphas        \n","        \n","        # The context vector is the weighted sum of the values.\n","        context = torch.bmm(alphas, value)\n","        \n","        # context shape: [B, 1, 2D], alphas shape: [B, 1, M]\n","        return context, alphas"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KKIgeUK75xSm"},"source":["class AttentionDecoder(nn.Module):\n","  \"\"\"An RNN decoder + generator with GRU\"\"\"\n","\n","  def __init__(self, input_size, hidden_size, attention, max_len,generator, num_layers = 1, dropout=0.):\n","    \"\"\"\n","      Inputs:\n","        - `input_size` , `hidden_size`, and `dropout` the same as in Encoder.\n","    \"\"\"\n","    super(AttentionDecoder, self).__init__()\n","    self.input_size = input_size\n","    self.hidden_size = hidden_size\n","    # self.rnn = nn.GRU(input_size, hidden_size, num_layers=num_layers, batch_first=True,\n","    #                   dropout=dropout, bidirectional=False)\n","    self.rnn = nn.GRU(input_size + hidden_size, hidden_size, num_layers,\n","                          batch_first=True, dropout=dropout)\n","    \n","    self.generator = generator\n","    self.max_len = max_len\n","    self.dropout_layer = nn.Dropout(p=dropout)\n","    # self.rnn_to_pre = nn.Linear(input_size + hidden_size,\n","    #                             hidden_size, bias=False)\n","    self.rnn_to_pre = nn.Linear(hidden_size + hidden_size + input_size,\n","                                hidden_size, bias=False)\n","    self.attention = attention\n","\n","  def forward_step(self, prev_embed, encoder_hidden, src_mask, proj_key, hidden):\n","    \"\"\"Helper function for forward below:\n","       Perform a single decoder step (1 word).\n","\n","       Inputs:\n","      - `prev_embed`: a 3d-tensor of shape (batch_size, 1, embed_size = vocab_size)\n","          representing the padded embedded word vectors at this step in training\n","      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size) representing\n","          the current hidden state.\n","\n","      Returns:\n","      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size)\n","          representing the current decoder hidden state.\n","      - `output`: a 3d-tensor of shape (batch_size, max_len, vocab_size)\n","          representing the total generated outputs.\n","    \"\"\"\n","\n","    # compute context vector using attention mechanism\n","    query = hidden[-1].unsqueeze(1)  # [#layers, B, D] -> [B, 1, D]\n","    context, attn_probs = self.attention(\n","        query=query, proj_key=proj_key,\n","        value=encoder_hidden, mask=src_mask)\n","    \n","    # RNN\n","    rnn_input = torch.cat([prev_embed, context], dim=2)\n","\n","    output, hidden = self.rnn(rnn_input, hidden)\n","    \n","    pre_output = torch.cat([prev_embed, output, context], dim=2)\n","    pre_output = self.dropout_layer(pre_output)\n","    pre_output = self.rnn_to_pre(pre_output)\n","    \n","    return hidden, pre_output\n","  def forward_step_beam(self, prev_embed, encoder_hidden, \n","                   src_mask, proj_key, hidden):\n","    \"\"\"Beam Search only: Unroll the decoder one step at a time.\"\"\"\n","    temp_hidden, pre_output = self.forward_step(prev_embed,encoder_hidden, \n","                   src_mask, proj_key, hidden)\n","    output, logits, output_word = self.generator.forward_gumbel(pre_output)\n","    return  temp_hidden, output, logits, output_word\n","\n","  def forward(self, input, encoder_hidden, encoder_finals, src_mask, max_len, hidden=None):\n","    \"\"\"Unroll the decoder one step at a time.\n","\n","    Inputs:\n","      - `inputs`: a 3d-tensor of shape (batch_size, 1, embed_size)\n","          representing a batch of padded embedded word vectors of SOS . \n","          If size is (batch_size,max_len, embed_size), then it is teacher forcing.\n","      - `encoder_finals`: a 3d-tensor of shape\n","          (num_enc_layers, batch_size, hidden_size) representing the final\n","          encoder hidden states used to initialize the initial decoder hidden\n","          states.\n","      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size) representing\n","          the value to be used to initialize the initial decoder hidden states.\n","          If None, then use `encoder_finals`.\n","      - `max_len`: an int representing the maximum decoding length.\n","      - `style`: TAYLOR_STYLE or DRAKE_STYLE\n","\n","    Returns:\n","      - `hidden`: a 3d-tensor of shape\n","          (num_layers, batch_size, hidden_size) representing the final hidden\n","          state for each element in the batch.\n","      - `outputs`: a 3d-tensor of shape\n","          (batch_size, max_len, hidden_size) representing the raw decoder\n","          outputs (before mapping to a `trg_vocab_size`-dim vector).\n","      - `logits_vectors`: a 3d-tensor of shape\n","          (batch_size, max_len, trg_vocab_size) representing the mapped decoder\n","          outputs.\n","      - `words`: a 3d-tensor of shape\n","          (batch_size, max_len, 1) representing output sentence and\n","          the corresponding word index (can be used for embedding)  \n","    \"\"\"\n","\n","    # Initialize decoder hidden state.\n","    if hidden is None:\n","      hidden = self.init_hidden(encoder_finals)\n","\n","    proj_key = self.attention.key_layer(encoder_hidden)\n","\n","    output_vectors = []\n","    logits_vectors = []\n","    words = []\n","    hidden_states = []\n","    hidden_states.append(hidden[-1][:,None,:])\n","    \n","    for i in range(max_len-1) :\n","      \n","      hidden, prev_output = self.forward_step(input,encoder_hidden, src_mask, proj_key, hidden)\n","      input, logits, output_word = self.generator.forward_gumbel(prev_output)\n","\n","      logits_vectors.append(logits)\n","      output_vectors.append(input)\n","      words.append(output_word)\n","      hidden_states.append(prev_output)\n","\n","    outputs = torch.cat(output_vectors, dim =1)\n","    logits_vectors = torch.cat(logits_vectors,dim = 1)\n","    words = torch.cat(words, axis = -1)\n","    hidden_states = torch.cat(hidden_states, axis = 1)\n","\n","    return hidden, outputs , logits_vectors, words, hidden_states\n","\n","  def forward_teacher(self, input, encoder_hidden, encoder_finals, src_mask, max_len=None, hidden=None):\n","    \"\"\"Unroll the decoder one step at a time.\n","\n","    Inputs:\n","      - `inputs`: a 3d-tensor of shape (batch_size,max_len, embed_size)\n","          representing a batch of padded embedded word vectors of original \n","          sentence and acts as  teacher forcing.\n","\n","    Returns:\n","      - `hidden`: a 3d-tensor of shape\n","          (num_layers, batch_size, hidden_size) representing the final hidden\n","          state for each element in the batch.\n","      - `outputs`: a 3d-tensor of shape\n","          (batch_size, max_len, hidden_size) representing the raw decoder\n","          outputs (before mapping to a `trg_vocab_size`-dim vector).\n","      - `logits_vectors`: a 3d-tensor of shape\n","          (batch_size, max_len, trg_vocab_size) representing the mapped decoder\n","          outputs each represents the probability? \n","      - `words`: a 3d-tensor of shape\n","          (batch_size, max_len, 1) representing output sentence and\n","          the corresponding word index (can be used for embedding)      \n","    \"\"\"\n","\n","    # Initialize decoder hidden state.\n","    if max_len is None:\n","      max_len = input.shape[1]\n","    if hidden is None:\n","      hidden = self.init_hidden(encoder_finals)\n","    \n","    proj_key = self.attention.key_layer(encoder_hidden)\n","\n","    output_vectors = []\n","    logits_vectors = []\n","    words = []\n","    hidden_states = []\n","    hidden_states.append(hidden[-1][:,None,:])\n","\n","    for i in range(max_len):\n","      hidden, prev_output = self.forward_step(input[:,i:i+1,:], encoder_hidden, src_mask, proj_key, hidden)\n","      \n","      output, logits, output_word = self.generator(prev_output)\n","      logits_vectors.append(logits)\n","      output_vectors.append(output)\n","      words.append(output_word)\n","      hidden_states.append(prev_output)\n","\n","    outputs = torch.cat(output_vectors, dim =1)\n","    logits_vectors = torch.cat(logits_vectors,dim = 1)\n","    words = torch.cat(words, axis = -1)\n","    hidden_states = torch.cat(hidden_states, axis = 1)\n","    return hidden, outputs , logits_vectors, words, hidden_states\n","\n","  def init_hidden(self, encoder_finals):\n","    \"\"\"Use encoder final hidden state to initialize decoder's first hidden\n","       state.\n","\n","       Input: `encoder_finals` is same as in forward()\n","\n","       Returns: \n","         - `decoder_init_hiddens`: a 3d-tensor of shape \n","              (num_layers, batch_size, hidden_size) representing the initial\n","              hidden state of the decoder for each element in the batch \n","    \"\"\"\n","    decoder_init_hiddens = torch.tanh(encoder_finals)\n","    return decoder_init_hiddens"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kxvEOOu_9Zkt"},"source":["#### EncoderDecoderAttention (in progress)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":290},"id":"bJgy5Rqt7efj","executionInfo":{"status":"error","timestamp":1638545706293,"user_tz":300,"elapsed":10226,"user":{"displayName":"Sirui Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09569618597630381386"}},"outputId":"0a224632-f760-416d-e26c-14b5abcb7e6f"},"source":["# ### Work in progress\n","# class EncoderDecoderAttention(nn.Module):\n","#   def __init__(self, encoder, decoder, line_embed, generator):\n","#     super(EncoderDecoderAttention, self).__init__()\n","\n","#     self.encoder = encoder\n","#     self.decoder = decoder\n","#     self.line_embed = line_embed\n","#     self.generator = generator\n","\n","#   def forward(self, lines, line_lens):\n","#     encoder_hidden, encoder_finals = self.encode(lines, line_lens)\n","#     src_mask = (lines != PAD_INDEX).unsqueeze(-2)\n","#     # return self.reconstruct(encoder_hidden, encoder_finals, lines[:, :-1], src_mask)\n","#     return self.reconstruct(encoder_hidden, encoder_finals, lines[:, :-1], src_mask), self.decode(encoder_hidden, encoder_finals, src_mask)\n","\n","#   def encode(self, lines, line_lens):\n","#     return self.encoder(self.line_embed(lines), line_lens)\n","    \n","#   def reconstruct(self, encoder_hidden, h0, lines, src_mask):\n","#     original = self.line_embed(lines)\n","#     return self.decoder.forward_teacher(original,encoder_hidden, h0, src_mask)\n","\n","#   def decode(self, encoder_hidden, h0, src_mask):\n","#     target = self.line_embed(torch.tensor([SOS_INDEX]).repeat(h0.size()[1],1).to(device))\n","#     return self.decoder.forward(target, encoder_hidden, h0, src_mask, max_len)\n","\n","# epochs = 3\n","# lr = 1e-3\n","# batch_size = 32\n","# print_every = 100\n","# max_len = dataset.max_seq_length\n","# vocab_size = len(vocab)\n","# embed_size = 256\n","# hidden_size = 256\n","# dropout = 0.2\n","# gamma = 0.001\n","\n","# train_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","# valid_loader = data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n","# test_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n","\n","# line_embed = nn.Embedding(vocab_size, embed_size)\n","# encoder = Encoder(embed_size,hidden_size)\n","# generator = GeneratorTransferredSampled(hidden_size,vocab_size, line_embed, gamma = gamma)\n","# attention = BahdanauAttention(hidden_size, key_size=hidden_size)\n","# decoder = AttentionDecoder(embed_size, hidden_size, attention=attention, max_len=vocab_size, generator = generator,dropout=dropout)\n","# model = EncoderDecoderAttention(encoder, decoder, line_embed, generator).to(device)\n","# optimizer_model = torch.optim.Adam(model.parameters(), lr=lr)\n","# rec_loss = nn.NLLLoss(reduction=\"mean\",ignore_index = PAD_INDEX)\n","\n","# for epoch in range(epochs):\n","#   epoch_rec_loss = 0\n","#   epoch_tokens = 0\n","#   model.train()\n","#   for i,(taylor_lines, taylor_len, drake_lines, drake_len) in enumerate(train_loader):\n","#     lines = torch.cat((taylor_lines, drake_lines), 0).to(device)    \n","#     line_lens = torch.cat((taylor_len, drake_len), 0).to(device)\n","\n","#     # Train model\n","#     rec_orig,dec_orig  = model(lines, line_lens)   \n","#     # rec_orig  = model(lines, line_lens)    \n","#     loss_rec = rec_loss(input=dec_orig[2].permute(0,2,1), target=lines[:, 1:])\n","\n","#     optimizer_model.zero_grad()\n","#     loss_rec.backward()\n","#     optimizer_model.step()\n","    \n","#     epoch_rec_loss += loss_rec.item() * line_lens.sum().item()\n","#     epoch_tokens += line_lens.sum().item()\n","\n","#   print(\"Finished Training Epoch \", epoch)\n","#   print(\"Training PPL\", np.exp(epoch_rec_loss / float(epoch_tokens)))\n","#   val_loss = 0\n","#   val_tokens = 0\n","\n","#   for i,(taylor_lines, taylor_len, drake_lines, drake_len) in enumerate(valid_loader):\n","#     lines = torch.cat((taylor_lines, drake_lines), 0).to(device)    \n","#     line_lens = torch.cat((taylor_len, drake_len), 0).to(device)\n","\n","#     rec_orig,dec_orig  = model(lines, line_lens)  \n","#     # rec_orig  = model(lines, line_lens)    \n","#     loss_rec = rec_loss(input=dec_orig[2].permute(0,2,1), target=lines[:, 1:])\n","\n","#     val_loss += loss_rec.item() * line_lens.sum().item()\n","#     val_tokens += line_lens.sum().item()\n","  \n","#   print(\"Valid PPL\", np.exp(val_loss / float(val_tokens)))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-761a9dfa3bc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0moptimizer_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mepoch_rec_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_rec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mline_lens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0mepoch_tokens\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mline_lens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"n-8K-9QC4W25"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UO_EaE4pb-wm"},"source":["# dec_orig[3]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dznyd7lnV-vr"},"source":["# def lookup_words(x, vocab):\n","#   return [vocab[i] for i in x]\n","\n","# idx=17\n","# print(lookup_words(lines[idx], vocab))\n","# print(lookup_words(rec_orig[3][idx], vocab))\n","# print(lookup_words(dec_orig[3][idx], vocab))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t2mg2wYk30dv"},"source":["### Encoder-Decoder for Testing"]},{"cell_type":"code","metadata":{"id":"SlnB42ru9uBI"},"source":["# ### Work in progress\n","# class EncoderDecoder(nn.Module):\n","#   def __init__(self, encoder, decoder, line_embed, generator):\n","#     super(EncoderDecoder, self).__init__()\n","\n","#     self.encoder = encoder\n","#     self.decoder = decoder\n","#     self.line_embed = line_embed\n","#     self.generator = generator\n","\n","#   def forward(self, lines, line_lens):\n","#     encoder_hiddens, encoder_finals = self.encode(lines, line_lens)\n","#     del encoder_hiddens\n","#     return self.reconstruct(encoder_finals, lines[:, :-1]), self.decode(encoder_finals)\n","\n","#   def encode(self, lines, line_lens):\n","#     return self.encoder(self.line_embed(lines), line_lens)\n","    \n","#   def reconstruct(self, h0, lines):\n","#     original = self.line_embed(lines)\n","#     return self.decoder.forward_teacher(original,h0)\n","\n","#   def decode(self, h0):\n","#     target = self.line_embed(torch.tensor([SOS_INDEX]).repeat(h0.size()[1],1).to(device))\n","#     return self.decoder.forward(target,h0,max_len)\n","\n","# epochs = 3\n","# lr = 1e-3\n","# batch_size = 32\n","# print_every = 100\n","# max_len = dataset.max_seq_length\n","# vocab_size = len(vocab)\n","# embed_size = 256\n","# hidden_size = 256\n","# dropout = 0.2\n","# gamma = 0.001\n","\n","# train_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","# valid_loader = data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n","# test_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n","\n","# line_embed = nn.Embedding(vocab_size, embed_size)\n","# encoder = Encoder(embed_size,hidden_size)\n","# generator = GeneratorTransferredSampled(hidden_size,vocab_size, line_embed, gamma = gamma)\n","# decoder = Decoder(embed_size, hidden_size, max_len=vocab_size, generator = generator,dropout=dropout)\n","# model = EncoderDecoder(encoder, decoder, line_embed, generator).to(device)\n","# optimizer_model = torch.optim.Adam(model.parameters(), lr=lr)\n","# rec_loss = nn.NLLLoss(reduction=\"mean\",ignore_index = PAD_INDEX)\n","\n","# for epoch in range(epochs):\n","#   epoch_rec_loss = 0\n","#   epoch_tokens = 0\n","#   model.train()\n","#   for i,(taylor_lines, taylor_len, drake_lines, drake_len) in enumerate(train_loader):\n","#     lines = torch.cat((taylor_lines, drake_lines), 0).to(device)    \n","#     line_lens = torch.cat((taylor_len, drake_len), 0).to(device)\n","\n","#     # Train model\n","#     rec_orig,dec_orig  = model(lines, line_lens)    \n","#     loss_rec = rec_loss(input=rec_orig[2].permute(0,2,1), target=lines[:, 1:])\n","\n","#     optimizer_model.zero_grad()\n","#     loss_rec.backward()\n","#     optimizer_model.step()\n","    \n","#     epoch_rec_loss += loss_rec.item() * line_lens.sum().item()\n","#     epoch_tokens += line_lens.sum().item()\n","\n","#   print(\"Finished Training Epoch \", epoch)\n","#   print(\"Training PPL\", np.exp(epoch_rec_loss / float(epoch_tokens)))\n","#   val_loss = 0\n","#   val_tokens = 0\n","\n","#   for i,(taylor_lines, taylor_len, drake_lines, drake_len) in enumerate(valid_loader):\n","#     lines = torch.cat((taylor_lines, drake_lines), 0).to(device)    \n","#     line_lens = torch.cat((taylor_len, drake_len), 0).to(device)\n","\n","#     rec_orig,dec_orig  = model(lines, line_lens)    \n","#     loss_rec = rec_loss(input=rec_orig[2].permute(0,2,1), target=lines[:, 1:])\n","\n","#     val_loss += loss_rec.item() * line_lens.sum().item()\n","#     val_tokens += line_lens.sum().item()\n","  \n","#   print(\"Valid PPL\", np.exp(val_loss / float(val_tokens)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wg_FbKl-T9xd"},"source":["### Classifier"]},{"cell_type":"code","metadata":{"id":"SeDjRElB65UC"},"source":["class LSTMDiscriminator(nn.Module):\n","  def __init__(self, input_size, hidden_size, LSTMlayers=1, dropout = 0.5):\n","    super(LSTMDiscriminator, self).__init__()\n","\n","    self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=LSTMlayers, \n","                        batch_first=True, bidirectional=True)\n","    self.drop = nn.Dropout(p=dropout)\n","    self.fc = nn.Linear(2*hidden_size, 1)\n","    self.hidden_size = hidden_size\n","\n","  def forward(self, text_emb, text_len):\n","    text_len[text_len==0] += 1\n","\n","    packed_input = pack_padded_sequence(text_emb, text_len.cpu(), batch_first=True, enforce_sorted=False)\n","    packed_output, _ = self.lstm(packed_input)\n","    output, _ = pad_packed_sequence(packed_output, batch_first=True)\n","\n","    out_forward = output[range(len(output)), text_len - 1, :self.hidden_size]\n","    out_reverse = output[:, 0, self.hidden_size:]\n","    out_reduced = torch.cat((out_forward, out_reverse), 1)\n","    text_fea = self.drop(out_reduced)\n","\n","    text_fea = self.fc(text_fea)\n","    text_fea = torch.squeeze(text_fea, 1)\n","    text_out = torch.sigmoid(text_fea)\n","\n","    return text_out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PylylJgLahxw"},"source":["class LSTMClassifier(nn.Module):\n","\n","    def __init__(self, dimension=128):\n","        super(LSTMClassifier, self).__init__()\n","\n","        self.embedding = nn.Linear(len(vocab), 300)\n","        self.dimension = dimension\n","        self.lstm = nn.LSTM(input_size=300,\n","                            hidden_size=dimension,\n","                            num_layers=1,\n","                            batch_first=True,\n","                            bidirectional=True)\n","        self.drop = nn.Dropout(p=0.5)\n","\n","        self.fc = nn.Linear(2*dimension, 1)\n","\n","    def forward(self, text, text_len):\n","\n","        text_emb = self.embedding(text)\n","        text_len[text_len==0] += 1\n","\n","        packed_input = pack_padded_sequence(text_emb, text_len.cpu(), batch_first=True, enforce_sorted=False)\n","        packed_output, _ = self.lstm(packed_input)\n","        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n","\n","        out_forward = output[range(len(output)), text_len - 1, :self.dimension]\n","        out_reverse = output[:, 0, self.dimension:]\n","        out_reduced = torch.cat((out_forward, out_reverse), 1)\n","        text_fea = self.drop(out_reduced)\n","\n","        text_fea = self.fc(text_fea)\n","        text_fea = torch.squeeze(text_fea, 1)\n","        text_out = torch.sigmoid(text_fea)\n","\n","        return text_out"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_xuubDG0F5i3"},"source":["### Training"]},{"cell_type":"markdown","metadata":{"id":"9XEGiaaa93DR"},"source":["#### TSTModel"]},{"cell_type":"code","metadata":{"id":"yjs1mzrQEyef"},"source":["class TSTModel(nn.Module):\n","  def __init__(self, max_len, vocab_size, embed_size, hidden_size_z, hidden_size_y, line_embed, encoder, generator, decoder, classifier):\n","    super(TSTModel, self).__init__()\n","\n","    self.hidden_size = hidden_size_y + hidden_size_z\n","\n","    self.encoder = encoder\n","    self.generator = generator\n","    self.decoder = decoder\n","    self.classifier = classifier\n","\n","    self.line_embed = line_embed\n","    self.y_embed_enc = nn.Embedding(2,hidden_size_y)\n","    self.y_embed_gen = nn.Embedding(2,hidden_size_y)\n","\n","    self.max_len = max_len\n","    self.vocab_size = vocab_size\n","    self.embed_size = embed_size\n","    self.hidden_size_z = hidden_size_z\n","    self.hidden_size_y = hidden_size_y\n","\n","  def forward(self, lines, line_lens, labels):\n","\n","    encoder_hidden, encoded_lines = self.encode(lines, line_lens, labels)\n","    z = encoded_lines[-1][:,self.hidden_size_y:]\n","\n","    h0_orig = torch.cat((self.y_embed_gen(labels),z), 1)[None,:]\n","    h0_tsf = torch.cat((self.y_embed_gen(1-labels),z), 1)[None,:]\n","\n","    # Decode back into original form for reconstruction\n","    rec_orig = self.reconstruct(h0_orig, lines[:, :-1])\n","\n","    # Decode into original and transferred forms for classification\n"," \n","    decode_orig = self.decode(h0_orig)\n","    decode_tsf = self.decode(h0_tsf)\n","    \n","    half = int(lines.size(0) / 2)\n","\n","    discrim1_input = torch.cat((rec_orig[4][:half], decode_tsf[4][half:]))\n","    discrim0_input = torch.cat((rec_orig[4][half:], decode_tsf[4][:half]))\n","\n","    classifier_lines = torch.cat((decode_orig[2], decode_tsf[2], F.one_hot(lines[:,1:], self.vocab_size).to(torch.float)), 0)\n","    \n","    rec_orig_len = first_eos(rec_orig[3]) + 1\n","    decode_orig_len = first_eos(decode_orig[3]) + 1\n","    decode_tsf_len = first_eos(decode_tsf[3]) + 1\n","\n","    classifier_line_lens = torch.cat((decode_orig_len, decode_tsf_len, line_lens),0)\n","    # classifier_line_lens = torch.cat((line_lens, line_lens, line_lens),0)\n","    discrim0_lens = torch.cat((rec_orig_len[half:], decode_tsf_len[:half]))\n","    discrim1_lens = torch.cat((rec_orig_len[:half], decode_tsf_len[half:]))\n","\n","    pred_class = self.classifier(classifier_lines, classifier_line_lens-1)\n","\n","    # return rec_orig, decode_orig\n","    return rec_orig, pred_class, decode_orig, decode_tsf, (discrim0_input, discrim0_lens), (discrim1_input, discrim1_lens)\n","\n","  def encode(self, lines, line_lens, labels):\n","    init_state = torch.cat((self.y_embed_enc(labels), torch.zeros((len(lines),self.hidden_size_z), device=device)), 1)[None,:].to(device)\n","    return self.encoder(self.line_embed(lines), line_lens, init_state)\n","\n","  def reconstruct(self, h0, lines):\n","    original = self.line_embed(lines)\n","    return self.decoder.forward_teacher(original,h0)\n","\n","  def decode(self, h0):\n","    target = self.line_embed(torch.tensor([SOS_INDEX]).repeat(h0.size()[1],1).to(device))\n","    return self.decoder.forward(target,h0,self.max_len)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C1-4ipH195Oe"},"source":["#### TSTModelAttention"]},{"cell_type":"code","metadata":{"id":"5dN3kpsaZpFX"},"source":["class TSTModelAttention(nn.Module):\n","  def __init__(self, max_len, vocab_size, embed_size, hidden_size_z, hidden_size_y, \n","               line_embed, encoder, generator, decoder, classifier,beamSeasrch):\n","    super(TSTModelAttention, self).__init__()\n","\n","    self.hidden_size = hidden_size_y + hidden_size_z\n","\n","    self.encoder = encoder\n","    self.generator = generator\n","    self.decoder = decoder\n","    self.classifier = classifier\n","    self.beamSeasrch = beamSeasrch\n","\n","    self.line_embed = line_embed\n","    self.y_embed_enc = nn.Embedding(2,hidden_size_y)\n","    self.y_embed_gen = nn.Embedding(2,hidden_size_y)\n","\n","    self.max_len = max_len\n","    self.vocab_size = vocab_size\n","    self.embed_size = embed_size\n","    self.hidden_size_z = hidden_size_z\n","    self.hidden_size_y = hidden_size_y\n","\n","  def forward(self, lines, line_lens, labels):\n","\n","    src_mask = (lines != PAD_INDEX).unsqueeze(-2)\n","    encoder_hidden, encoder_finals = self.encode(lines, line_lens, labels)\n","\n","    z = encoder_finals[-1][:,self.hidden_size_y:]\n","\n","    h0_orig = torch.cat((self.y_embed_gen(labels),z), 1)[None,:]\n","    h0_tsf = torch.cat((self.y_embed_gen(1-labels),z), 1)[None,:]\n","\n","    # Decode back into original form for reconstruction\n","    rec_orig = self.reconstruct(encoder_hidden, h0_orig, lines[:, :-1], src_mask)\n","\n","    # Decode into original and transferred forms for classification\n","    decode_orig = self.decode(encoder_hidden, h0_orig, src_mask)\n","    decode_tsf = self.decode(encoder_hidden, h0_tsf, src_mask)\n","    \n","    half = int(lines.size(0) / 2)\n","\n","    discrim1_input = torch.cat((rec_orig[4][:half], decode_tsf[4][half:]))\n","    discrim0_input = torch.cat((rec_orig[4][half:], decode_tsf[4][:half]))\n","\n","    classifier_lines = torch.cat((decode_orig[2], decode_tsf[2], F.one_hot(lines[:,1:], self.vocab_size).to(torch.float)), 0)\n","\n","    rec_orig_len = first_eos(rec_orig[3]) + 1\n","    decode_orig_len = first_eos(decode_orig[3]) + 1\n","    decode_tsf_len = first_eos(decode_tsf[3]) + 1\n","\n","    classifier_line_lens = torch.cat((decode_orig_len, decode_tsf_len, line_lens),0)\n","    # classifier_line_lens = torch.cat((line_lens, line_lens, line_lens),0)\n","    discrim0_lens = torch.cat((rec_orig_len[half:], decode_tsf_len[:half]))\n","    discrim1_lens = torch.cat((rec_orig_len[:half], decode_tsf_len[half:]))\n","\n","    pred_class = self.classifier(classifier_lines, classifier_line_lens-1)\n","    \n","    # return rec_orig, pred_class, decode_orig, decode_tsf, (discrim0_input, line_lens), (discrim1_input, line_lens)\n","    return rec_orig, pred_class, decode_orig, decode_tsf, (discrim0_input, discrim0_lens), (discrim1_input, discrim1_lens)\n","\n","  def forward_beam(self,lines, line_lens, labels):\n","    src_mask = (lines != PAD_INDEX).unsqueeze(-2)\n","    encoder_hidden, encoder_finals = self.encode(lines, line_lens, labels)\n","    z = encoder_finals[-1][:,self.hidden_size_y:]\n","\n","    h0_orig = torch.cat((self.y_embed_gen(labels),z), 1)[None,:]\n","    h0_tsf = torch.cat((self.y_embed_gen(1-labels),z), 1)[None,:]\n","\n","    decode_orig = self.decode_beam(encoder_hidden, h0_orig, src_mask)\n","    decode_tsf = self.decode_beam(encoder_hidden, h0_tsf, src_mask)\n","\n","    # half = int(lines.size(0) / 2)\n","\n","    # discrim1_input = torch.cat((rec_orig[4][:half], decode_tsf[4][half:]))\n","    # discrim0_input = torch.cat((rec_orig[4][half:], decode_tsf[4][:half]))\n","\n","    # classifier_lines = torch.cat((decode_orig[2], decode_tsf[2], F.one_hot(lines[:,1:], self.vocab_size).to(torch.float)), 0)\n","    # classifier_line_lens = torch.cat((line_lens, line_lens, line_lens),0)\n","    # pred_class = self.classifier(classifier_lines, classifier_line_lens-1)\n","\n","    return decode_orig, decode_tsf #rec_orig, ,pred_class,  (discrim0_input, line_lens), (discrim1_input, line_lens)\n","\n","  \n","  def decode_beam(self,encoder_hidden,h0,src_mask):\n","    target = self.line_embed(torch.tensor([SOS_INDEX]).repeat(h0.size()[1],1).to(device))\n","    return self.beamSeasrch.beam_decode(target, encoder_hidden,h0,src_mask, self.max_len)\n","\n","  def encode(self, lines, line_lens, labels):\n","    init_state = torch.cat((self.y_embed_enc(labels), torch.zeros((len(lines),self.hidden_size_z), device=device)), 1)[None,:].to(device)\n","    return self.encoder(self.line_embed(lines), line_lens, init_state)\n","\n","  def reconstruct(self, encoder_hidden, h0, lines, src_mask):\n","    original = self.line_embed(lines)\n","    return self.decoder.forward_teacher(original,encoder_hidden, h0, src_mask)\n","\n","  def decode(self, encoder_hidden, h0, src_mask):\n","    target = self.line_embed(torch.tensor([SOS_INDEX]).repeat(h0.size()[1],1).to(device))\n","    return self.decoder.forward(target, encoder_hidden, h0, src_mask, self.max_len)\n","\n","def first_eos(x):\n","  eos_pos = (x == EOS_INDEX)\n","  found, indices = ((eos_pos.cumsum(1) == 1) & eos_pos).max(1)\n","  indices = indices + (~found*x.size(1))\n","  return indices"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BoCYL-kD-Q4o"},"source":["#### Training"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":488},"id":"O8jaGcIiNQUi","executionInfo":{"status":"error","timestamp":1638560821461,"user_tz":300,"elapsed":13277,"user":{"displayName":"Sirui Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09569618597630381386"}},"outputId":"05cae5bc-64e7-472b-d349-a9619c68cbbe"},"source":["# Changing learning rate can affect things\n","# Focusing on reconstruction first, then later building in discriminator or classifier seems to help\n","# Without attention, autoencoder needs at least 7-8 epochs to get reasonable reconstructions\n","\n","attention = True\n","classify = True\n","discriminate = True\n","\n","pre_train_classifier = False\n","\n","epochs = 11\n","class_epochs = 2\n","lr = 1e-3\n","batch_size = 32\n","print_every = 100\n","\n","max_len = dataset.max_seq_length\n","vocab_size = len(vocab)\n","embed_size = 100\n","hidden_size_z = 500\n","hidden_size_y = 200\n","hidden_size = hidden_size_z + hidden_size_y\n","dropout = 0.2\n","gamma = 0.1\n","\n","TAYLOR_STYLE=1 # for information only, don't change\n","DRAKE_STYLE=0  # for information only, don't change\n","train_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","valid_loader = data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n","\n","line_embed = nn.Embedding(vocab_size, embed_size)\n","encoder = Encoder(embed_size,hidden_size)\n","generator = GeneratorTransferredSampled(hidden_size,vocab_size, line_embed, gamma = gamma)\n","classifier = LSTMClassifier()\n","discriminator0 = LSTMDiscriminator(hidden_size, hidden_size).to(device)\n","discriminator1 = LSTMDiscriminator(hidden_size, hidden_size).to(device)\n","\n","\n","if pre_train_classifier:\n","  classifier = classifier.to(device)\n","  optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-3) \n","  class_loss = nn.BCELoss()\n","\n","  for epoch in range(class_epochs):\n","    correct = 0\n","    classifier.train()\n","    for i,(taylor_lines, taylor_len, drake_lines, drake_len) in enumerate(train_loader):\n","      lines = torch.cat((taylor_lines, drake_lines), 0).to(device)  \n","      classifier_lines = F.one_hot(lines[:,1:], len(vocab)).to(torch.float).to(device)\n","\n","      line_lens = torch.cat((taylor_len, drake_len), 0).to(device)\n","      labels = torch.cat((torch.ones(size=(len(taylor_lines),), dtype=torch.int32),torch.zeros(size=(len(drake_lines),),dtype=torch.int32))).to(device)\n","\n","      pred_class = classifier(classifier_lines, line_lens-1)\n","      loss_class = class_loss(input=pred_class, target=labels.to(torch.float))\n","\n","      optimizer.zero_grad()\n","      loss_class.backward()\n","      optimizer.step()\n","\n","      correct += torch.sum((pred_class >= 0.5) == labels)\n","    print(\"Pre-Training Accuracy: \", correct / float(2*len(train_dataset)))\n","    classifier.eval()\n","    correct = 0\n","    for i,(taylor_lines, taylor_len, drake_lines, drake_len) in enumerate(valid_loader):\n","      lines = torch.cat((taylor_lines, drake_lines), 0).to(device)  \n","      classifier_lines = F.one_hot(lines[:,1:], len(vocab)).to(torch.float).to(device)\n","\n","      line_lens = torch.cat((taylor_len, drake_len), 0).to(device)\n","      labels = torch.cat((torch.ones(size=(len(taylor_lines),), dtype=torch.int32),torch.zeros(size=(len(drake_lines),),dtype=torch.int32))).to(device)\n","\n","      pred_class = classifier(classifier_lines, line_lens-1)\n","      correct += torch.sum((pred_class >= 0.5) == labels)\n","    print(\"Pre-Valid Accuracy: \", correct / float(2*len(valid_dataset)))\n","\n","if attention:\n","  attention_mech = BahdanauAttention(hidden_size, key_size=hidden_size)\n","  decoder = AttentionDecoder(embed_size, hidden_size, attention=attention_mech, max_len=vocab_size, generator = generator,dropout=dropout)\n","  # BEAM SEARCH\n","  beamSeasrch = BeamSearch(decoder, 3,3,line_embed,max_len)\n","  model = TSTModelAttention(dataset.max_seq_length, vocab_size, embed_size, hidden_size_z, hidden_size_y, line_embed, encoder, generator, decoder, classifier,beamSeasrch).to(device)\n","else:\n","  decoder = Decoder(embed_size, hidden_size, max_len=vocab_size, generator = generator, dropout=dropout)\n","  # BEAM SEARCH\n","  beamSeasrch = BeamSearch(decoder, 3,3,line_embed,max_len)\n","  model = TSTModel(dataset.max_seq_length, vocab_size, embed_size, hidden_size_z, hidden_size_y, line_embed, encoder, generator, decoder, classifier).to(device)\n","\n","optimizer_model = torch.optim.Adam(model.parameters(), lr=lr) \n","optimizer_discr = torch.optim.Adam(list(discriminator0.parameters()) + list(discriminator1.parameters()), lr=lr) \n","\n","rec_loss = nn.NLLLoss(reduction=\"mean\",ignore_index = PAD_INDEX)\n","class_loss = nn.BCELoss()\n","discr_loss = nn.BCELoss()\n","\n","epoch_losses = []\n","for epoch in range(epochs):\n","  epoch_loss = 0\n","  epoch_class_loss = 0\n","  epoch_rec_loss = 0\n","  epoch_adv_loss = 0\n","  epoch_loss_d = 0\n","  epoch_tokens = 0\n","  model.train()\n","  for i,(taylor_lines, taylor_len, drake_lines, drake_len) in enumerate(train_loader):\n","    lines = torch.cat((taylor_lines, drake_lines), 0).to(device)    \n","    line_lens = torch.cat((taylor_len, drake_len), 0).to(device)\n","    labels = torch.cat((torch.ones(size=(len(taylor_lines),), dtype=torch.int32),torch.zeros(size=(len(drake_lines),),dtype=torch.int32))).to(device)\n","    classifier_labels = torch.cat((labels,1-labels, labels))\n","    \n","    fake_labels = torch.cat((torch.zeros(size=(len(taylor_lines),), dtype=torch.int32),torch.ones(size=(len(taylor_lines),),dtype=torch.int32))).to(device)\n","    fake_labels = fake_labels\n","\n","    # Train discriminator\n","\n","    if discriminate:\n","      rec_orig, pred_class, decode_orig, decode_tsf, pred_fake0, pred_fake1 = model(lines, line_lens, labels)\n","      \n","      pred_fake0 = discriminator0(pred_fake0[0], pred_fake0[1])\n","      pred_fake1 = discriminator1(pred_fake1[0], pred_fake1[1])\n","\n","      loss_d0 = discr_loss(pred_fake0, fake_labels.to(torch.float))\n","      loss_d1 = discr_loss(pred_fake1, fake_labels.to(torch.float))\n","      loss_d = loss_d0 + loss_d1\n","\n","      optimizer_discr.zero_grad()\n","      loss_d.backward()\n","      optimizer_discr.step()\n","\n","    # Train model\n","\n","    rec_orig, pred_class, decode_orig, decode_tsf, pred_fake0, pred_fake1 = model(lines, line_lens, labels)\n","\n","    loss_rec = rec_loss(input=rec_orig[2].permute(0,2,1), target=lines[:, 1:])\n","\n","    loss = loss_rec\n","\n","    if attention:\n","      rec_treshold  = 1\n","    else:\n","      rec_treshold = 10\n","    \n","    if discriminate:\n","      pred_fake0 = discriminator0(pred_fake0[0], pred_fake0[1])\n","      pred_fake1 = discriminator1(pred_fake1[0], pred_fake1[1])\n","      loss_adv0 = class_loss(pred_fake0[len(drake_lines):], fake_labels[len(drake_lines):].to(torch.float))\n","      loss_adv1 = class_loss(pred_fake1[len(taylor_lines):], fake_labels[len(taylor_lines):].to(torch.float))\n","\n","      if loss_adv0 < 1.2 and loss_adv1 < 1.2 and loss_rec < rec_treshold:\n","      # Don't use adversarial training unless discriminator and reconstruction are both good\n","        loss -= (loss_adv0 + loss_adv1)\n","    \n","    if classify:\n","      loss_class = class_loss(pred_class, classifier_labels.to(torch.float))\n","      loss_class_generated = class_loss(pred_class[:-len(labels)], classifier_labels[:-len(labels)].to(torch.float))\n","      # loss_class = loss_class_generated\n","\n","      if loss_class_generated > 0.5 and loss_rec < rec_treshold:\n","      # If generated examples are too similar and reconstruction is good, only focus on achieving better style (i.e. classifier)\n","      # Else, focus on both\n","      # Note: play around with these, it probably affects performance\n","        loss = loss_class\n","      else:\n","        loss += loss_class\n","\n","    optimizer_model.zero_grad()\n","    loss.backward()\n","    optimizer_model.step()\n","    \n","    epoch_loss += loss.item()\n","    epoch_rec_loss += loss_rec.item() * line_lens.sum().item()\n","    epoch_tokens += line_lens.sum().item()\n","\n","    if discriminate:\n","      epoch_loss_d += loss_d.item()\n","      epoch_adv_loss += (loss_adv0.item() + loss_adv1.item())\n","      \n","    if classify:\n","      epoch_class_loss += loss_class.item()\n","\n","    if model.training and i % print_every == 0:\n","      print(\"Epoch Step: %d Loss: %f\" % (i, loss.item()))\n","      print(\"Epoch Step: %d Class Loss: %f\" % (i, loss_class.item()))\n","  \n","  epoch_losses.append(epoch_loss)\n","  print(\"Finished Training Epoch \", epoch)\n","  print(\"Training PPL\", np.exp(epoch_rec_loss / float(epoch_tokens)))\n","\n","  if discriminate:\n","    print(\"Adversarial Loss\", epoch_adv_loss)\n","    print(\"Discriminator Loss\", epoch_loss_d)\n","  \n","  if classify:\n","    print(\"Classification Loss\", epoch_class_loss)\n","\n","  val_loss = 0\n","  val_tokens = 0\n","  val_class_loss = 0\n","  correct_pred = 0\n","  correct_pred_all = 0\n","  correct_pred_drake = 0\n","  correct_pred_tay = 0\n","\n","  for i,(taylor_lines, taylor_len, drake_lines, drake_len) in enumerate(valid_loader):\n","    lines = torch.cat((taylor_lines, drake_lines), 0).to(device)    \n","    line_lens = torch.cat((taylor_len, drake_len), 0).to(device)\n","    labels = torch.cat((torch.ones(size=(len(taylor_lines),), dtype=torch.int32),torch.zeros(size=(len(drake_lines),),dtype=torch.int32))).to(device)\n","    classifier_labels = torch.cat((labels,1-labels, labels))\n","    \n","    fake_labels = torch.cat((torch.zeros(size=(len(taylor_lines),), dtype=torch.int32),torch.ones(size=(len(taylor_lines),),dtype=torch.int32))).to(device)\n","\n","    rec_orig, pred_class, decode_orig, decode_tsf, pred_fake0, pred_fake1 = model(lines, line_lens, labels)\n","    loss_rec = rec_loss(input=rec_orig[2].permute(0,2,1), target=lines[:, 1:])\n","    # loss_class = class_loss(pred_class, classifier_labels.to(torch.float))\n","\n","    val_loss += loss_rec.item() * line_lens.sum().item()\n","    val_tokens += line_lens.sum().item()\n","    # val_class_loss += loss_class.item()*classifier_labels.size(0)\n","\n","    if classify:\n","      correct_pred += torch.sum((pred_class[-len(lines):] >= 0.5) == classifier_labels[-len(lines):])\n","      correct_pred_all += torch.sum((pred_class >= 0.5) == classifier_labels)\n","\n","    if discriminate:\n","      pred_fake0 = discriminator0(pred_fake0[0], pred_fake0[1])\n","      pred_fake1 = discriminator1(pred_fake1[0], pred_fake1[1])\n","\n","      correct_pred_drake += torch.sum((pred_fake0 >= 0.5) == fake_labels) \n","      correct_pred_tay += torch.sum((pred_fake1 >= 0.5) == fake_labels)\n","  \n","  print(\"Valid PPL\", np.exp(val_loss / float(val_tokens)))\n","  if classify:\n","    print(\"Valid Classification Accuracy on True\", correct_pred / (2.*len(valid_dataset)))\n","    print(\"Valid Classification Accuracy on All\", correct_pred_all / (3.*2.*len(valid_dataset)))\n","\n","  if discriminate:\n","    print(\"Valid Classification Accuracy on Drake\", correct_pred_drake / (2.*len(valid_dataset)))\n","    print(\"Valid Classification Accuracy on Taylor\", correct_pred_tay / (2.*len(valid_dataset)))\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n"]},{"output_type":"stream","name":"stdout","text":["Epoch Step: 0 Loss: 10.127322\n","Epoch Step: 0 Class Loss: 0.693980\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-381ff8ac6b45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdiscriminate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m       \u001b[0mrec_orig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_orig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_tsf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_fake0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_fake1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m       \u001b[0mpred_fake0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_fake0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_fake0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-15-95d09c2de413>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, lines, line_lens, labels)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# Decode into original and transferred forms for classification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mdecode_orig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh0_orig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mdecode_tsf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh0_tsf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mhalf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-15-95d09c2de413>\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, encoder_hidden, h0, src_mask)\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mline_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSOS_INDEX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfirst_eos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-01df7ac8f552>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, encoder_hidden, encoder_finals, src_mask, max_len, hidden)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m       \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproj_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m       \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_gumbel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m       \u001b[0mlogits_vectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-177bcd1a5316>\u001b[0m in \u001b[0;36mforward_gumbel\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward_gumbel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgumbel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mlogprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogsoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgumbel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-177bcd1a5316>\u001b[0m in \u001b[0;36mgumbel\u001b[0;34m(self, logits, eps)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mgumbel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mU\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"iFrALY7-CBVq"},"source":["#### Save Model"]},{"cell_type":"code","metadata":{"id":"4cUviHkqGngZ"},"source":["from datetime import datetime\n","import pytz\n","import os\n","now = datetime.now()\n","now.astimezone(pytz.timezone('America/New_York'))\n","\n","model_dir = '/content/drive/Shareddrives/MIT NLP 8.864/model'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cb9sgSLsGTOQ"},"source":["def current_time():\n","  now = datetime.now()\n","  now = now.astimezone(pytz.timezone('America/New_York'))\n","  return now.strftime(\"%Y%m%d_%H%M%S\")\n","def lookup_words(x, vocab):\n","  return [vocab[i] for i in x]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tWIVaV2PFxB0"},"source":["##### Pre-trained Classifier"]},{"cell_type":"code","metadata":{"id":"rX3V-MDS8yrn"},"source":["if pre_train_classifier:\n","  classifier_path = os.path.join(model_dir,\n","                                 f'classifier/classifier_{current_time()}.pt')\n","  torch.save(classifier.state_dict(), classifier_path)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MhQ7JvVDIURI"},"source":["# # Reload model Example\n","# model = LSTMClassifier()\n","# model.load_state_dict(torch.load(classifier_path))\n","# model = model.to(device)\n","# model.eval()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CSiie6Y1F02-"},"source":["##### Style-Transfer Model"]},{"cell_type":"markdown","metadata":{"id":"-nyY5MVdIM62"},"source":["###### Set model path"]},{"cell_type":"code","metadata":{"id":"pDH74vWjEi-N"},"source":["model_name = 'model_'\n","if not attention and not classify and not discriminate:\n","  # Dana\n","  model_name += '1'\n","elif attention and not classify and not discriminate:\n","  # Dana\n","  model_name += '2'\n","elif not attention and classify and not discriminate:\n","  # Sirui\n","  model_name += '3'\n","elif attention and classify and not discriminate:\n","  # Sirui\n","  model_name += '4'\n","elif not attention and classify and discriminate:\n","  # Chenwei\n","  model_name += '5'\n","elif attention and classify and discriminate:\n","  # Chenwei\n","  model_name += '6'\n","elif not attention and not classify and discriminate:\n","  # Hammaad\n","  model_name += '7'\n","elif attention and not classify and discriminate:\n","  # Hammaad\n","  model_name += '8'\n","  \n","model_dir_current = os.path.join(model_dir,model_name)\n","model_dir_current = os.path.join(model_dir_current,\n","                                 f'{model_name}_{current_time()}')\n","if not os.path.isdir(model_dir_current):\n","  os.mkdir(model_dir_current)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FV4U3sAbKY-D"},"source":["###### Save model"]},{"cell_type":"code","metadata":{"id":"U6Pdc4LiL95Z"},"source":["model_init_args = {\"Embedding_num_embeddings\":vocab_size,\n","           'Embedding_embedding_dim':embed_size,\n","           'Encoder_input_size':embed_size, \n","           'Encoder_hidden_size':hidden_size,\n","           'GeneratorTransferredSampled_hidden_size':hidden_size, \n","           'GeneratorTransferredSampled_vocab_size':vocab_size, \n","           'GeneratorTransferredSampled_gamma':gamma,\n","           'LSTMClassifier_dimension':classifier.dimension,\n","           'LSTMDiscriminator_input_size':hidden_size, \n","           'LSTMDiscriminator_hidden_size':hidden_size}\n","if attention: \n","  model_init_args.update({\n","           'BahdanauAttention_hidden_size':hidden_size,\n","           'BahdanauAttention_key_size':hidden_size,\n","           'AttentionDecoder_input_size':embed_size,\n","            'AttentionDecoder_hidden_size':hidden_size, \n","            'AttentionDecoder_max_len':vocab_size,  \n","            'AttentionDecoder_dropout':dropout,\n","            'TSTModelAttention_max_len': dataset.max_seq_length, \n","            'TSTModelAttention_vocab_size':vocab_size, \n","            'TSTModelAttention_embed_size':embed_size, \n","            'TSTModelAttention_hidden_size_z':hidden_size_z, \n","            'TSTModelAttention_hidden_size_y':hidden_size_y})\n","else:\n","  model_init_args.update({\n","            'Decoder_':embed_size, \n","            'Decoder_':hidden_size, \n","            'Decoder_max_len':vocab_size, \n","            'Decoder_dropout':dropout,\n","            'TSTModel_max_len': dataset.max_seq_length, \n","            'TSTModel_vocab_size':vocab_size, \n","            'TSTModel_embed_size':embed_size, \n","            'TSTModel_hidden_size_z':hidden_size_z, \n","            'TSTModel_hidden_size_y':hidden_size_y})\n","           \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e1Y4aRftUxB3"},"source":["\n","model_path = os.path.join(model_dir_current,\n","                          f'{model_name}_{current_time()}.pt')\n","torch.save(model.state_dict(), model_path)\n","model_args_filename = model_path[:-3]+\"_params.txt\"\n","import json\n","with open(model_args_filename,'w') as f:\n","  json.dump(model_init_args, f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CbGOTvG-Lfin","executionInfo":{"status":"ok","timestamp":1638549825552,"user_tz":300,"elapsed":486,"user":{"displayName":"Sirui Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09569618597630381386"}},"outputId":"dbe059fc-0707-4e9c-9dfe-f34c22959876"},"source":["line_embed = nn.Embedding(vocab_size, embed_size)\n","encoder = Encoder(embed_size,hidden_size)\n","generator = GeneratorTransferredSampled(hidden_size,vocab_size, \n","                                        line_embed, gamma = gamma)\n","classifier = LSTMClassifier()\n","discriminator0 = LSTMDiscriminator(hidden_size, hidden_size).to(device)\n","discriminator1 = LSTMDiscriminator(hidden_size, hidden_size).to(device)\n","if attention:\n","  attention_mech = BahdanauAttention(hidden_size, key_size=hidden_size)\n","  decoder = AttentionDecoder(embed_size, hidden_size, attention=attention_mech, max_len=vocab_size, generator = generator,dropout=dropout)\n","  model = TSTModelAttention(dataset.max_seq_length, vocab_size, embed_size, hidden_size_z, hidden_size_y, line_embed, encoder, generator, decoder, classifier).to(device)\n","else:\n","  decoder = Decoder(embed_size, hidden_size, max_len=vocab_size, generator = generator, dropout=dropout)\n","  model = TSTModel(dataset.max_seq_length, vocab_size, embed_size, hidden_size_z, hidden_size_y, line_embed, encoder, generator, decoder, classifier).to(device)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lRgC2nXSB3EO","executionInfo":{"status":"ok","timestamp":1638549856432,"user_tz":300,"elapsed":227,"user":{"displayName":"Sirui Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09569618597630381386"}},"outputId":"4ff28b7c-7cab-4c9f-9747-436d5e1e5c85"},"source":["\n","model.load_state_dict(torch.load(model_path))\n","model = model.to(device)\n","model.eval()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TSTModelAttention(\n","  (encoder): Encoder(\n","    (rnn): GRU(100, 700, batch_first=True)\n","  )\n","  (generator): GeneratorTransferredSampled(\n","    (proj): Linear(in_features=700, out_features=12458, bias=True)\n","    (logsoftmax): LogSoftmax(dim=2)\n","    (softmax): Softmax(dim=2)\n","    (src_embed): Embedding(12458, 100)\n","  )\n","  (decoder): AttentionDecoder(\n","    (rnn): GRU(800, 700, batch_first=True, dropout=0.2)\n","    (generator): GeneratorTransferredSampled(\n","      (proj): Linear(in_features=700, out_features=12458, bias=True)\n","      (logsoftmax): LogSoftmax(dim=2)\n","      (softmax): Softmax(dim=2)\n","      (src_embed): Embedding(12458, 100)\n","    )\n","    (dropout_layer): Dropout(p=0.2, inplace=False)\n","    (rnn_to_pre): Linear(in_features=1500, out_features=700, bias=False)\n","    (attention): BahdanauAttention(\n","      (key_layer): Linear(in_features=700, out_features=700, bias=False)\n","      (query_layer): Linear(in_features=700, out_features=700, bias=False)\n","      (energy_layer): Linear(in_features=700, out_features=1, bias=False)\n","    )\n","  )\n","  (classifier): LSTMClassifier(\n","    (embedding): Linear(in_features=12458, out_features=300, bias=True)\n","    (lstm): LSTM(300, 128, batch_first=True, bidirectional=True)\n","    (drop): Dropout(p=0.5, inplace=False)\n","    (fc): Linear(in_features=256, out_features=1, bias=True)\n","  )\n","  (line_embed): Embedding(12458, 100)\n","  (y_embed_enc): Embedding(2, 200)\n","  (y_embed_gen): Embedding(2, 200)\n",")"]},"metadata":{},"execution_count":64}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9YMr-X3V9WXd","executionInfo":{"status":"ok","timestamp":1638547646945,"user_tz":300,"elapsed":7362,"user":{"displayName":"Sirui Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09569618597630381386"}},"outputId":"745d5ff8-bc78-4a06-8399-2aa148e86ec2"},"source":["correct = 0\n","for i,(taylor_lines, taylor_len, drake_lines, drake_len) in enumerate(valid_loader):\n","  lines = torch.cat((taylor_lines, drake_lines), 0).to(device)  \n","  classifier_lines = F.one_hot(lines[:,1:], len(vocab)).to(torch.float).to(device)\n","\n","  line_lens = torch.cat((taylor_len, drake_len), 0).to(device)\n","  labels = torch.cat((torch.ones(size=(len(taylor_lines),), dtype=torch.int32),torch.zeros(size=(len(drake_lines),),dtype=torch.int32))).to(device)\n","\n","  pred_class = model(classifier_lines, line_lens-1)\n","  correct += torch.sum((pred_class >= 0.5) == labels)\n","print(\"Pre-Valid Accuracy: \", correct / float(2*len(valid_dataset)))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Pre-Valid Accuracy:  tensor(0.8508, device='cuda:0')\n"]}]},{"cell_type":"markdown","metadata":{"id":"pJaumBnKnA7G"},"source":["#### Save Test Result"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":468},"id":"_4DZXwvxeTEX","executionInfo":{"status":"error","timestamp":1638560978486,"user_tz":300,"elapsed":38409,"user":{"displayName":"Sirui Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09569618597630381386"}},"outputId":"8051a546-3545-441a-e157-f656fa697897"},"source":["if os.path.exists(os.path.join(model_dir_current,\"raw.txt\")):\n","  os.remove(os.path.join(model_dir_current,\"raw.txt\"))\n","if os.path.exists(os.path.join(model_dir_current,\"orig.txt\")):\n","  os.remove(os.path.join(model_dir_current,\"orig.txt\"))\n","if os.path.exists(os.path.join(model_dir_current,\"tsf.txt\")):\n","  os.remove(os.path.join(model_dir_current,\"tsf.txt\"))\n","for i,(taylor_lines, taylor_len, drake_lines, drake_len) in enumerate(test_loader):\n","    lines = torch.cat((taylor_lines, drake_lines), 0).to(device)    \n","    line_lens = torch.cat((taylor_len, drake_len), 0).to(device)\n","    labels = torch.cat((torch.ones(size=(len(taylor_lines),), dtype=torch.int32),torch.zeros(size=(len(drake_lines),),dtype=torch.int32))).to(device)\n","    classifier_labels = torch.cat((labels,1-labels, labels))\n","    \n","    fake_labels = torch.cat((torch.zeros(size=(len(taylor_lines),), dtype=torch.int32),torch.ones(size=(len(taylor_lines),),dtype=torch.int32))).to(device)\n","\n","    rec_orig, pred_class, decode_orig, decode_tsf, _, _ = model(lines, line_lens, labels)\n","    for i in range(lines.shape[0]):\n","      with open(os.path.join(model_dir_current,\"raw.txt\"),'a') as f:\n","        f.write(str(lookup_words(lines[i],vocab))+\"\\n\")\n","      with open(os.path.join(model_dir_current,\"orig.txt\"),'a') as f:\n","        f.write(str(lookup_words(decode_orig[3][i],vocab))+\"\\n\")\n","      with open(os.path.join(model_dir_current,\"tsf.txt\"),'a') as f:\n","        f.write(str(lookup_words(decode_tsf[3][i],vocab))+\"\\n\")\n","      \n","    "],"execution_count":null,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-a79f43920fc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mfake_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtaylor_lines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtaylor_lines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mrec_orig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_orig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_tsf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir_current\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"raw.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-15-95d09c2de413>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, lines, line_lens, labels)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# Decode into original and transferred forms for classification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mdecode_orig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh0_orig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mdecode_tsf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh0_tsf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-15-95d09c2de413>\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, encoder_hidden, h0, src_mask)\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mline_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSOS_INDEX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfirst_eos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-01df7ac8f552>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, encoder_hidden, encoder_finals, src_mask, max_len, hidden)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m       \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproj_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m       \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_gumbel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-01df7ac8f552>\u001b[0m in \u001b[0;36mforward_step\u001b[0;34m(self, prev_embed, encoder_hidden, src_mask, proj_key, hidden)\u001b[0m\n\u001b[1;32m     45\u001b[0m     context, attn_probs = self.attention(\n\u001b[1;32m     46\u001b[0m         \u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproj_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproj_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         value=encoder_hidden, mask=src_mask)\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m# RNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-ef3cd8933c0a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, proj_key, value, mask)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# We first project the query (the decoder state).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# The projected keys (the encoder states) were already pre-computated.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# Calculate scores.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"5WANFTxxpPjW"},"source":["### Other"]},{"cell_type":"code","metadata":{"id":"Y6lRAikKW0EZ"},"source":["# Quick assessment\n","\n","\n","idx=5\n","print(lookup_words(lines[idx], vocab))\n","print(lookup_words(rec_orig[3][idx], vocab))\n","print(lookup_words(decode_orig[3][idx], vocab))\n","print(lookup_words(decode_tsf[3][idx], vocab))\n","\n","idx=8\n","print(lookup_words(lines[idx], vocab))\n","print(lookup_words(rec_orig[3][idx], vocab))\n","print(lookup_words(decode_orig[3][idx], vocab))\n","print(lookup_words(decode_tsf[3][idx], vocab))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":236},"id":"u0Y_sKJos5Eq","executionInfo":{"status":"error","timestamp":1638553175239,"user_tz":300,"elapsed":480,"user":{"displayName":"Sirui Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09569618597630381386"}},"outputId":"3fcdc2a0-562e-4838-84d9-f14cea5b6bac"},"source":["# Quick eval using classifier\n","for i,(taylor_lines, taylor_len, drake_lines, drake_len) in enumerate(valid_loader):\n","  lines = torch.cat((taylor_lines, drake_lines), 0).to(device)    \n","  line_lens = torch.cat((taylor_len, drake_len), 0).to(device)\n","  labels = torch.cat((torch.ones(size=(len(taylor_lines),), dtype=torch.int32),torch.zeros(size=(len(drake_lines),),dtype=torch.int32))).to(device)\n","  classifier_labels = torch.cat((labels,1-labels, labels))\n","  \n","  rec_orig, pred_class, decode_orig, decode_tsf, pred_fake0, pred_fake1 = model(lines, line_lens, labels)\n","  classifier_lines = torch.cat((decode_orig[2], decode_tsf[2], F.one_hot(lines[:,1:], vocab_size).to(torch.float)), 0)\n","\n","  rec_orig_len = first_eos(rec_orig[3]) + 1\n","  decode_orig_len = first_eos(decode_orig[3]) + 1\n","  decode_tsf_len = first_eos(decode_tsf[3]) + 1\n","  classifier_line_lens = torch.cat((decode_orig_len, decode_tsf_len, line_lens),0)\n","  \n","  pred_class = classifier(classifier_lines, classifier_line_lens-1)\n","  pred_class = pred_class.cpu().detach().numpy()\n","  classifier_labels = classifier_labels.cpu().detach().numpy()\n","\n","  pred_gen.append(pred_class[:-len(lines)])\n","  pred_real.append(pred_class[-len(lines):])\n","\n","  y_gen.append(classifier_labels[:-len(lines)])\n","  y_real.append(classifier_labels[-len(lines):])\n","\n","from sklearn.metrics import roc_auc_score\n","print(roc_auc_score(np.concatenate(y_real), np.concatenate(pred_real)))\n","print(roc_auc_score(np.concatenate(y_gen), np.concatenate(pred_gen)))"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-109-51e4ab70bab3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0mclassifier_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m   \u001b[0mpred_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_class\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m   \u001b[0mpred_real\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_class\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'pred_gen' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"vfpA5Nvd-oHa"},"source":["#### BeamSearch Validation (only for the first validation sample)"]},{"cell_type":"code","metadata":{"id":"qLroJgAe-qvi","colab":{"base_uri":"https://localhost:8080/","height":288},"executionInfo":{"status":"error","timestamp":1638557534528,"user_tz":300,"elapsed":336,"user":{"displayName":"Sirui Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09569618597630381386"}},"outputId":"486889d4-1061-4742-85eb-f47bc8e3f4cc"},"source":["for i,(taylor_lines, taylor_len, drake_lines, drake_len) in enumerate(valid_loader):\n","  lines = torch.cat((taylor_lines, drake_lines), 0).to(device)    \n","  line_lens = torch.cat((taylor_len, drake_len), 0).to(device)\n","  \n","  labels = torch.cat((torch.ones(size=(len(taylor_lines),), dtype=torch.int32),torch.zeros(size=(len(drake_lines),),dtype=torch.int32))).to(device)\n","  classifier_labels = torch.cat((labels,1-labels, labels))\n","\n","  lines = lines[0:1,:]\n","  line_lens = line_lens[0:1]\n","  labels = labels[0:1]\n","  decode_orig_beam, decode_tsf_beam = model.forward_beam(lines, line_lens, labels)\n","  \n","  print(\"3 Original Sentences decoded by beam search:\")\n","  print(lookup_words(decode_orig[-2][0], vocab))\n","  print(lookup_words(decode_orig[-2][1], vocab))\n","  print(lookup_words(decode_orig[-2][2], vocab))\n","  print(\"3 Transferred Sentences decoded by beam search:\")\n","  print(lookup_words(decode_tsf[-2][0], vocab))\n","  print(lookup_words(decode_tsf[-2][1], vocab))\n","  print(lookup_words(decode_tsf[-2][2], vocab))\n","  break"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-8b6b66061d65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtaylor_lines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtaylor_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrake_lines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrake_len\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtaylor_lines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrake_lines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mline_lens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtaylor_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrake_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtaylor_lines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrake_lines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'valid_loader' is not defined"]}]},{"cell_type":"code","metadata":{"id":"6qGpc1QpK6L9"},"source":["import operator"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MOhwNvcYYcjJ","executionInfo":{"status":"ok","timestamp":1638553323913,"user_tz":300,"elapsed":88,"user":{"displayName":"Sirui Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09569618597630381386"}},"outputId":"a4a6de82-a852-4eff-daed-ff4e1fde2806"},"source":["decode_tsf[-2][0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([6860, 6860, 6860,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n","           3, 8125,    3,    3], device='cuda:0')"]},"metadata":{},"execution_count":118}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D_psJlJAYeb0","executionInfo":{"status":"ok","timestamp":1638553248145,"user_tz":300,"elapsed":84,"user":{"displayName":"Sirui Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09569618597630381386"}},"outputId":"19c0cad3-808d-46c9-8a94-3ac12ba75ca6"},"source":["len(decode_tsf)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["5"]},"metadata":{},"execution_count":116}]},{"cell_type":"code","metadata":{"id":"FYh9kBTgYmCv"},"source":[""],"execution_count":null,"outputs":[]}]}