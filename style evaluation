{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"style evaluation","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"LXuaLouOo-WQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638711454802,"user_tz":300,"elapsed":18777,"user":{"displayName":"Katherine Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11324848797432579426"}},"outputId":"fe3b91bf-91f9-4e6d-e8e7-7a380ec7e9ff"},"source":["!pip install unidecode\n","import json\n","import re\n","from unidecode import unidecode\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","import nltk\n","nltk.download('punkt')\n","from collections import Counter\n","import pickle\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","from torch.utils import data\n","import operator\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","# assert device == \"cuda\"  \n","\n","seed = 42\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: unidecode in /usr/local/lib/python3.7/dist-packages (1.3.2)\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"JDi4Padcqh8T"},"source":["# # These IDs are reserved.\n","MAX_SENT_LENGTH = 15\n","MAX_SENT_LENGTH_PLUS_SOS_EOS = MAX_SENT_LENGTH + 2\n","PAD_INDEX = 0\n","UNK_INDEX = 1\n","SOS_INDEX = 2\n","EOS_INDEX = 3\n","RARE_WORD_TRESHOLD = 0\n","\n","class TSTDataset(data.Dataset):\n","    def __init__(self, taylor_sentences, drake_sentences, vocab, vocab_counts, sampling=1.):\n","        self.taylor_sentences = taylor_sentences[:int(len(taylor_sentences) * sampling)]\n","        self.drake_sentences = drake_sentences[:int(len(drake_sentences) * sampling)]\n","\n","        self.max_seq_length = MAX_SENT_LENGTH_PLUS_SOS_EOS\n","        self.vocab = vocab\n","        self.vocab_counts = vocab_counts\n","\n","        self.v2id = {v : i for i, v in enumerate(self.vocab)}\n","        self.id2v = {val : key for key, val in self.v2id.items()}\n","    \n","    def __len__(self):\n","        return min(len(self.taylor_sentences), len(self.drake_sentences))\n","    \n","    def __getitem__(self, index):\n","        taylor_sent = self.taylor_sentences[index]\n","        taylor_len = len(taylor_sent) + 2   # add <s> and </s> to each sentence\n","        taylor_id = []\n","        for w in taylor_sent:\n","            if w not in self.vocab:\n","                w = '<unk>'\n","            if vocab_counts[w] <= RARE_WORD_TRESHOLD:\n","                w = '<unk>'\n","            taylor_id.append(self.v2id[w])\n","\n","        taylor_id = ([SOS_INDEX] + taylor_id + [EOS_INDEX] + [PAD_INDEX] *\n","                  (self.max_seq_length - taylor_len))\n","\n","        drake_sent = self.drake_sentences[index]\n","        drake_len = len(drake_sent) + 2   # add <s> and </s> to each sentence\n","        drake_id = []\n","        for w in drake_sent:\n","            if w not in self.vocab:\n","                w = '<unk>'\n","            if vocab_counts[w] <= RARE_WORD_TRESHOLD:\n","                w = '<unk>'\n","            drake_id.append(self.v2id[w])\n","\n","        drake_id = ([SOS_INDEX] + drake_id + [EOS_INDEX] + [PAD_INDEX] *\n","                  (self.max_seq_length - drake_len))\n","\n","        return torch.tensor(taylor_id), taylor_len, torch.tensor(drake_id), drake_len\n","\n","train_dataset = torch.load('/content/drive/Shareddrives/MIT NLP 8.864/Data/train.pt')\n","valid_dataset = torch.load('/content/drive/Shareddrives/MIT NLP 8.864/Data/valid.pt')\n","test_dataset = torch.load('/content/drive/Shareddrives/MIT NLP 8.864/Data/test.pt')\n","vocab_file = open('/content/drive/Shareddrives/MIT NLP 8.864/Data/vocab.pkl', \"rb\")\n","vocab = pickle.load(vocab_file)\n","vocab_file.close()\n","\n","vocab_counts = Counter(vocab)\n","vocab_counts['<pad>'] = RARE_WORD_TRESHOLD + 1\n","vocab_counts['<unk>'] = RARE_WORD_TRESHOLD + 1\n","vocab_counts['<s>'] = RARE_WORD_TRESHOLD + 1\n","vocab_counts['</s>'] = RARE_WORD_TRESHOLD + 1\n","TAYLOR_STYLE=1 # for information only, don't change\n","DRAKE_STYLE=0  # for information only, don't change\n","train_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","valid_loader = data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ynBTHa5grUr7"},"source":["### Encoder"]},{"cell_type":"code","metadata":{"id":"FieXza5erYUH"},"source":["from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","\n","class Encoder(nn.Module):\n","  def __init__(self, input_size, hidden_size, dropout=0.):\n","    \"\"\"\n","    Inputs: \n","      - `input_size`: an int representing the RNN input size.\n","      - `hidden_size`: an int representing the RNN hidden size.\n","      - `dropout`: a float representing the dropout rate during training. Note\n","          that for 1-layer RNN this has no effect since dropout only applies to\n","          outputs of intermediate layers.\n","    \"\"\"\n","    super(Encoder, self).__init__()\n","    self.rnn = nn.GRU(input_size, hidden_size, num_layers=1, batch_first=True,\n","                      dropout=dropout, bidirectional=False)\n","\n","  def forward(self, inputs, lengths, init_state=None):\n","    \"\"\"\n","    Inputs:\n","      - `inputs`: a 3d-tensor of shape (batch_size, max_seq_length, embed_size)\n","          representing a batch of padded embedded word vectors of source\n","          sentences.\n","      - `lengths`: a 1d-tensor of shape (batch_size,) representing the sequence\n","          lengths of `inputs`.\n","\n","    Returns:\n","      - `outputs`: a 3d-tensor of shape\n","        (batch_size, max_seq_length, hidden_size).\n","      - `finals`: a 3d-tensor of shape (num_layers, batch_size, hidden_size).\n","      Hint: `outputs` and `finals` are both standard GRU outputs. Check:\n","      https://pytorch.org/docs/stable/nn.html#gru\n","    \"\"\"\n","    # Our variable-length inputs are padded to the same length for batching\n","    # Here we \"pack\" them for computational efficiency (see note below)\n","    packed = pack_padded_sequence(inputs, lengths.cpu(), batch_first=True,\n","                                  enforce_sorted=False)\n","    outputs, finals = self.rnn(packed, init_state)\n","    outputs, _ = pad_packed_sequence(outputs, batch_first=True,\n","                                     total_length=MAX_SENT_LENGTH_PLUS_SOS_EOS)\n","    return outputs, finals"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rKiJAkvZGMWl"},"source":["### Decoder"]},{"cell_type":"markdown","metadata":{"id":"q96M0-eW9OBl"},"source":["#### Generator"]},{"cell_type":"code","metadata":{"id":"E1Tdf6_283WY"},"source":["class GeneratorTransferredSampled(nn.Module):\n","  \"\"\"Define standard linear + softmax generation step.\"\"\"\n","  def __init__(self, hidden_size, vocab_size, src_embed, gamma=0.001):\n","    \"\"\"\n","    Inputs:\n","      - `src_embed`: a 2d-tensor of shape (vocab_size, embed_size )\n","    \"\"\"\n","    super(GeneratorTransferredSampled, self).__init__()\n","    self.proj = nn.Linear(hidden_size, vocab_size, bias=True)\n","    self.gamma = gamma\n","    self.logsoftmax = nn.LogSoftmax(dim = 2)\n","    self.softmax = nn.Softmax(dim = 2)\n","    self.src_embed = src_embed\n","\n","  def embedding(self,x):\n","    return torch.matmul(x,self.src_embed.weight)\n","    \n","  def gumbel_softmax(self,logits, eps=1e-20):\n","    U = torch.rand(logits.shape).to(device)\n","    G = -torch.log(-torch.log(U + eps) + eps).to(device)\n","    return self.logsoftmax((logits + G) / self.gamma)\n","\n","  def gumbel(self,logits, eps=1e-20):\n","    U = torch.rand(logits.shape).to(device)\n","    G = -torch.log(-torch.log(U + eps) + eps).to(device)\n","    return (logits + G) / self.gamma\n","\n","  def forward(self, x):\n","    logits = self.proj(x)\n","    logprob = self.logsoftmax(logits)\n","    prob = self.softmax(logits)\n","    output = self.embedding(prob)\n","    word  = logits.argmax(dim = 2, keepdim = False)\n","\n","    return output, logprob, word\n","\n","  def forward_gumbel(self, x):\n","    logits = self.proj(x)\n","    prob = self.softmax(self.gumbel(logits))\n","    logprob = self.logsoftmax(self.gumbel(logits))\n","    output = self.embedding(prob)\n","    word  = logits.argmax(dim = 2, keepdim = False)\n","\n","    return output, logprob, word"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HeRBfORu9P49"},"source":["#### Basic Decoder"]},{"cell_type":"code","metadata":{"id":"plJpAyPu86yw"},"source":["class Decoder(nn.Module):\n","  \"\"\"An RNN decoder + generator with GRU\"\"\"\n","\n","  def __init__(self, input_size, hidden_size, max_len,generator, num_layers = 1, dropout=0.):\n","    \"\"\"\n","      Inputs:\n","        - `input_size` , `hidden_size`, and `dropout` the same as in Encoder.\n","    \"\"\"\n","    super(Decoder, self).__init__()\n","    self.input_size = input_size\n","    self.hidden_size = hidden_size\n","    self.rnn = nn.GRU(input_size, hidden_size, num_layers=num_layers, batch_first=True,\n","                      dropout=dropout, bidirectional=False)\n","    self.generator = generator\n","    self.max_len = max_len\n","    self.dropout_layer = nn.Dropout(p=dropout)\n","    self.rnn_to_pre = nn.Linear(input_size + hidden_size,\n","                                hidden_size, bias=False)\n","\n","  def forward_step(self, prev_embed, hidden):\n","    \"\"\"Helper function for forward below:\n","       Perform a single decoder step (1 word).\n","\n","       Inputs:\n","      - `prev_embed`: a 3d-tensor of shape (batch_size, 1, embed_size = vocab_size)\n","          representing the padded embedded word vectors at this step in training\n","      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size) representing\n","          the current hidden state.\n","\n","      Returns:\n","      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size)\n","          representing the current decoder hidden state.\n","      - `output`: a 3d-tensor of shape (batch_size, max_len, vocab_size)\n","          representing the total generated outputs.\n","    \"\"\"\n","    pre_output, hidden = self.rnn(prev_embed, hidden)\n","    pre_output = torch.cat([prev_embed, pre_output], dim=2)\n","    pre_output = self.dropout_layer(pre_output)\n","    pre_output = self.rnn_to_pre(pre_output)\n","    pre_output = torch.tanh(pre_output)\n","\n","    return hidden, pre_output\n","\n","    ### Your code here!\n","    pre_output, hidden = self.rnn(prev_embed, hidden)\n","    pre_output = torch.cat([prev_embed, pre_output], dim=2)\n","    pre_output = self.dropout_layer(pre_output)\n","    pre_output = self.rnn_to_pre(pre_output)\n","    pre_output = self.pre_activation(pre_output)\n","    \n","  def forward_step_beam(self, prev_embed, encoder_hidden, \n","                   src_mask, proj_key, hidden):\n","    \"\"\"Beam Search only: Unroll the decoder one step at a time.\n","    Inputs:\n","      - `input`: a 3d-tensor of shape (batch_size, 1, embed_size)\n","          representing the padded embedded word vectors at this step in training\n","      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size) representing\n","          the current hidden state.\n","    Returns:\n","      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size)\n","          representing the current decoder hidden state.\n","      - `output`: a 3d-tensor of shape (batch_size, 1, vocab_size)\n","          representing the total generated outputs.    \n","      - `gumbel_logits`: a 3d-tensor of shape\n","          (batch_size, 1, trg_vocab_size) representing the mapped decoder\n","          outputs from gumbel softmax.\n","      - `output_word`: a 2d-tensor of shape\n","          (batch_size, 1) representing output sentence and\n","          the corresponding word index (can be used for embedding)    \n","      - `logits`: a 2d-tensor of shape\n","          (batch_size, 1, trg_vocab_size) representing the mapped decoder\n","          outputs from log softmax \n","          \"\"\"\n","    temp_hidden, pre_output = self.forward_step(prev_embed,encoder_hidden, \n","                   src_mask, proj_key, hidden)\n","    output, logits, output_word = self.generator.forward_gumbel(pre_output)\n","    return  temp_hidden, output, logits, output_word\n","\n","  def forward(self, input, encoder_finals,max_len, hidden=None):\n","    \"\"\"Unroll the decoder one step at a time.\n","\n","    Inputs:\n","      - `inputs`: a 3d-tensor of shape (batch_size, 1, embed_size)\n","          representing a batch of padded embedded word vectors of SOS . \n","          If size is (batch_size,max_len, embed_size), then it is teacher forcing.\n","      - `encoder_finals`: a 3d-tensor of shape\n","          (num_enc_layers, batch_size, hidden_size) representing the final\n","          encoder hidden states used to initialize the initial decoder hidden\n","          states.\n","      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size) representing\n","          the value to be used to initialize the initial decoder hidden states.\n","          If None, then use `encoder_finals`.\n","      - `max_len`: an int representing the maximum decoding length.\n","      - `style`: TAYLOR_STYLE or DRAKE_STYLE\n","\n","    Returns:\n","      - `hidden`: a 3d-tensor of shape\n","          (num_layers, batch_size, hidden_size) representing the final hidden\n","          state for each element in the batch.\n","      - `outputs`: a 3d-tensor of shape\n","          (batch_size, max_len, hidden_size) representing the raw decoder\n","          outputs (before mapping to a `trg_vocab_size`-dim vector).\n","      - `logits_vectors`: a 3d-tensor of shape\n","          (batch_size, max_len, trg_vocab_size) representing the mapped decoder\n","          outputs.\n","      - `words`: a 3d-tensor of shape\n","          (batch_size, max_len, 1) representing output sentence and\n","          the corresponding word index (can be used for embedding)  \n","    \"\"\"\n","\n","    # Initialize decoder hidden state.\n","    if hidden is None:\n","      hidden = self.init_hidden(encoder_finals)\n","    output_vectors = []\n","    logits_vectors = []\n","    words = []\n","    hidden_states = []\n","    hidden_states.append(hidden[-1][:,None,:])\n","    for i in range(max_len-1) :\n","      hidden, prev_output = self.forward_step(input,hidden)\n","      input, logits, output_word = self.generator.forward_gumbel(prev_output)\n","      # input, logits, output_word = self.generator(prev_output)\n","\n","      # input = torch.concat([input,torch.full(input.shape,style)], axis = -1)\n","      logits_vectors.append(logits)\n","      output_vectors.append(input)\n","      words.append(output_word)\n","      hidden_states.append(prev_output)\n","\n","    outputs = torch.cat(output_vectors, dim =1)\n","    logits_vectors = torch.cat(logits_vectors,dim = 1)\n","    words = torch.cat(words, axis = -1)\n","    hidden_states = torch.cat(hidden_states, axis = 1)\n","    return hidden, outputs , logits_vectors, words, hidden_states\n","\n","  def forward_teacher(self, input, encoder_finals, max_len=None, hidden=None):\n","    \"\"\"Unroll the decoder one step at a time.\n","\n","    Inputs:\n","      - `inputs`: a 3d-tensor of shape (batch_size,max_len, embed_size)\n","          representing a batch of padded embedded word vectors of original \n","          sentence and acts as  teacher forcing.\n","\n","    Returns:\n","      - `hidden`: a 3d-tensor of shape\n","          (num_layers, batch_size, hidden_size) representing the final hidden\n","          state for each element in the batch.\n","      - `outputs`: a 3d-tensor of shape\n","          (batch_size, max_len, hidden_size) representing the raw decoder\n","          outputs (before mapping to a `trg_vocab_size`-dim vector).\n","      - `logits_vectors`: a 3d-tensor of shape\n","          (batch_size, max_len, trg_vocab_size) representing the mapped decoder\n","          outputs each represents the probability? \n","      - `words`: a 3d-tensor of shape\n","          (batch_size, max_len, 1) representing output sentence and\n","          the corresponding word index (can be used for embedding)      \n","    \"\"\"\n","\n","    # Initialize decoder hidden state.\n","    if max_len is None:\n","      max_len = input.shape[1]\n","    if hidden is None:\n","      hidden = self.init_hidden(encoder_finals)\n","    output_vectors = []\n","    logits_vectors = []\n","    words = []\n","    hidden_states = []\n","    hidden_states.append(hidden[-1][:,None,:])\n","    for i in range(max_len):\n","      hidden, prev_output = self.forward_step(input[:,i:i+1,:],hidden)\n","      # output, logits, output_word = self.generator(prev_output)\n","      output, logits, output_word = self.generator.forward_gumbel(prev_output)\n","      \n","      logits_vectors.append(logits)\n","      output_vectors.append(output)\n","      words.append(output_word)\n","      hidden_states.append(prev_output)\n","\n","    outputs = torch.cat(output_vectors, dim =1)\n","    logits_vectors = torch.cat(logits_vectors,dim = 1)\n","    words = torch.cat(words, axis = -1)\n","    hidden_states = torch.cat(hidden_states, axis = 1)\n","    return hidden, outputs , logits_vectors, words, hidden_states\n","\n","  def init_hidden(self, encoder_finals):\n","    \"\"\"Use encoder final hidden state to initialize decoder's first hidden\n","       state.\n","\n","       Input: `encoder_finals` is same as in forward()\n","\n","       Returns: \n","         - `decoder_init_hiddens`: a 3d-tensor of shape \n","              (num_layers, batch_size, hidden_size) representing the initial\n","              hidden state of the decoder for each element in the batch \n","    \"\"\"\n","    decoder_init_hiddens = torch.tanh(encoder_finals)\n","    return decoder_init_hiddens"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iaAnYmyv9gbu"},"source":["#### Beam Search"]},{"cell_type":"code","metadata":{"id":"OA_J4dhd9ibD"},"source":["from queue import PriorityQueue\n","class BeamSearchNode:\n","  def __init__(self, hiddenstate, previousNode, cur_embed, wordId, \n","               logProb,  length ):\n","    self.h = hiddenstate\n","    self.prevNode = previousNode\n","    self.cur_embed = cur_embed\n","    self.wordid = wordId\n","    self.logp = logProb\n","    self.leng = length\n","    \n","  def __lt__(self,other):\n","    return self.logp < other.logp\n","\n","  def eval(self, alpha=1.0):\n","    return self.logp \n","    # Add here a function for shaping a reward\n","    # reward = 0\n","    # return self.logp / float(self.leng - 1 + 1e-6) + alpha * reward'\n","\n","class BeamSearch:\n","  def __init__(self,decoder, beam_width, topk, line_embed,max_len,\n","               max_iter=2000):\n","    \"\"\"Use Beam Search to generate a full sentence with the given decoder model\n","  Inputs:\n","      - `decoder`: decoder module with forward_step_beam function\n","      - `beam_width` : the length of the beam \n","      - `max_len`: an int representing the maximum decoding length.\n","      - `max_iter`: The maximum decoding iteration\n","    \"\"\"\n","    self.decoder = decoder\n","    self.beam_width = beam_width\n","    self.topk = topk\n","    self.line_embed = line_embed\n","    self.max_len = max_len\n","    self.max_iter = max_iter\n","\n","  def beam_decode(self, inputs, encoder_hidden, encoder_finals, src_mask, max_len,\n","                  hidden = None):\n","                  # inputs, encoder_finals,src_mask, proj_key, hidden):\n","    \"\"\"Use Beam Search to generate a full sentence with the given decoder model\n","    Inputs:\n","        - `inputs`: a 3d-tensor of shape (batch_size, 1, embed_size)\n","            representing a batch of padded embedded word vectors of SOS . \n","        - `encoder_finals`: a 3d-tensor of shape\n","            (num_enc_layers, batch_size, hidden_size) representing the final\n","            encoder hidden states used to initialize the initial decoder hidden\n","            states.\n","\n","    Returns:\n","        - `final_logp_batch`: a 2d-tensor of shape\n","            (batch_size, sentences_num) representing the probability of generating \n","            the sentence.\n","        - `final_hidden_batch`: a 4d-tensor of shape\n","            (sentences_num, num_layers, batch_size, hidden_size) representing \n","            the final hidden layer\n","        - `decoded_batch`: a 3d-tensor of shape\n","            (batch_size,sentences_num,  max_len) representing output sentence and\n","            the corresponding word index (can be used for embedding)  \n","    \n","    \"\"\"\n","    \n","    decoded_batch = []\n","    final_hidden_batch, final_logp_batch = [],[]\n","    # print(\"shape of encoder_finals:\",encoder_finals.shape)\n","    for i in range(inputs.shape[0]):\n","      if hidden is None:\n","        hidden = self.decoder.init_hidden(encoder_finals[:,i:i+1,:])\n","      decoder_input = inputs[i:i+1,:,:]\n","      # Number of sentence to generate\n","      endnodes = []\n","      number_required = self.topk\n","      proj_key = self.decoder.attention.key_layer(encoder_hidden[i:i+1,:,:])\n","\n","      # starting node -  hidden vector, previous node, cur_embed, word id , logp, length\n","      node = BeamSearchNode(self.decoder.init_hidden(encoder_finals[:,i:i+1,:]), \n","                            None, decoder_input, [[SOS_INDEX]], 0, 1)\n","      nodes = PriorityQueue()\n","\n","      nodes.put((-node.eval(), node))\n","      qsize = 1\n","\n","      while qsize<=self.max_iter:\n","        tocheck = min(nodes.qsize(), self.beam_width)\n","        new_nodes = PriorityQueue()\n","        while tocheck>0:\n","          score, n = nodes.get()\n","          decoder_input = n.cur_embed\n","          decoder_hidden = n.h\n","          if n.leng > self.max_len:\n","            endnodes.append((score, n))\n","            # if we reached maximum # of sentences required\n","            if len(endnodes) >= number_required:\n","                break\n","\n","          # decode for one step using decoder\n","          hidden, _, logsoftmax_logits, wordId = decoder.forward_step_beam(decoder_input, \n","                                                                           encoder_hidden[i:i+1,:,:],\n","                                                                          src_mask[i:i+1,:,:], \n","                                                                          proj_key, \n","                                                                          decoder_hidden)\n","          tocheck -= 1\n","          # PUT HERE REAL BEAM SEARCH OF TOP\n","          log_prob, indexes = torch.topk(logsoftmax_logits, self.beam_width)\n","          for new_k in range(self.beam_width):\n","            \n","            decoded_t = indexes[0][0][new_k].view(1, -1)\n","            log_p = log_prob[0][0][new_k]\n","            prev_embed = self.line_embed(decoded_t)\n","\n","            node = BeamSearchNode(decoder_hidden, n,prev_embed, decoded_t, \n","                                  n.logp + log_p,n.leng + 1)\n","            score = -node.eval()\n","            new_nodes.put((score, node))\n","          qsize += self.beam_width - 1\n","        nodes = new_nodes\n","\n","        if len(endnodes) >= number_required:\n","            break\n","        \n","\n","      # choose nbest paths, back trace them\n","      if len(endnodes) == 0:\n","          endnodes = [nodes.get() for _ in range(self.topk)]\n","\n","      utterances = []\n","      final_logps = []\n","      final_hiddens = []\n","      # final_gumbel_logits = []\n","      for score, n in sorted(endnodes, key=operator.itemgetter(0)):\n","          end_node = n\n","          utterance = []\n","          # gumbel_logits = []\n","          utterance.append(n.wordid[0][0])\n","          # gumbel_logits =gumbel_logits + [n.gumbel_logits]\n","          # back trace\n","          while n.prevNode != None:\n","              n = n.prevNode\n","              utterance.append(n.wordid[0][0])\n","              # if n.gumbel_logits is not None:\n","              #   gumbel_logits =gumbel_logits + [n.gumbel_logits]\n","\n","          utterance = torch.unsqueeze(torch.tensor(utterance[::-1][:self.max_len]), axis = 0)\n","          utterances.append(utterance)\n","          # gumbel_logits = torch.cat(gumbel_logits, dim = 1)\n","          final_logp = end_node.logp \n","          final_hidden = end_node.h\n","          final_logps.append(final_logp)\n","          final_hiddens.append(torch.unsqueeze(torch.tensor(final_hidden), axis = 0))\n","          # final_gumbel_logits.append(gumbel_logits)\n","        \n","      utterances = torch.cat(utterances, axis = 0 )\n","      decoded_batch.append(torch.unsqueeze(utterances, axis = 0))\n","      final_logp_batch.append(torch.unsqueeze(torch.tensor(final_logps), axis = 0))\n","      final_hiddens = torch.cat(final_hiddens, axis = 0)\n","      final_hidden_batch.append(final_hiddens)\n","      # final_gumbel_logits = torch.cat(final_gumbel_logits, axis = 0)\n","      # final_gumbel_logits_batch.append(torch.unsqueeze(final_gumbel_logits, axis = 0))\n","\n","    # decoded_batch size = (batch, topk, sentence_len, 1)\n","    final_logp_batch = torch.cat(final_logp_batch, axis = 0)\n","    final_hidden_batch = torch.cat(final_hidden_batch, axis = 2)\n","    # final_gumbel_logits_batch = torch.cat(final_gumbel_logits_batch, axis = 0)\n","    decoded_batch = torch.cat(decoded_batch, axis = 0)\n","    print(\"final_logp_batch.shape:\",final_logp_batch.shape)\n","    print(\"final_hidden_batch.shape:\",final_hidden_batch.shape)\n","    # print(\"final_gumbel_logits_batch.shape:\",final_gumbel_logits_batch.shape)\n","    print(\"decoded_batch.shape:\",decoded_batch.shape)\n","    print(\"max_len:\", self.max_len)\n","    print(\"topk:\", self.topk)\n","    print(\"batch size:\", inputs.shape[0])\n","    return final_hidden_batch, final_logp_batch,  decoded_batch\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DAoA5kq8Al2r"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"NX25tl4G5vWa"},"source":["### Attention Decoder"]},{"cell_type":"code","metadata":{"id":"VrWt0I5N6ijP"},"source":["class BahdanauAttention(nn.Module):\n","    \"\"\"Implements Bahdanau (MLP) attention\"\"\"\n","    \n","    def __init__(self, hidden_size, key_size=None, query_size=None):\n","        super(BahdanauAttention, self).__init__()\n","        \n","        # We assume a bi-directional encoder so key_size is 2*hidden_size\n","        key_size = 2 * hidden_size if key_size is None else key_size\n","        query_size = hidden_size if query_size is None else query_size\n","\n","        self.key_layer = nn.Linear(key_size, hidden_size, bias=False)\n","        self.query_layer = nn.Linear(query_size, hidden_size, bias=False)\n","        self.energy_layer = nn.Linear(hidden_size, 1, bias=False)\n","        \n","        # to store attention scores\n","        self.alphas = None\n","        \n","    def forward(self, query=None, proj_key=None, value=None, mask=None):\n","        assert mask is not None, \"mask is required\"\n","\n","        # We first project the query (the decoder state).\n","        # The projected keys (the encoder states) were already pre-computated.\n","        query = self.query_layer(query)\n","        \n","        # Calculate scores.\n","        scores = self.energy_layer(torch.tanh(query + proj_key))\n","        scores = scores.squeeze(2).unsqueeze(1)\n","        \n","        # Mask out invalid positions.\n","        # The mask marks valid positions so we invert it using `mask & 0`.\n","        scores.data.masked_fill_(mask == 0, -float('inf'))\n","        \n","        # Turn scores to probabilities.\n","        alphas = F.softmax(scores, dim=-1)\n","        self.alphas = alphas        \n","        \n","        # The context vector is the weighted sum of the values.\n","        context = torch.bmm(alphas, value)\n","        \n","        # context shape: [B, 1, 2D], alphas shape: [B, 1, M]\n","        return context, alphas"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KKIgeUK75xSm"},"source":["class AttentionDecoder(nn.Module):\n","  \"\"\"An RNN decoder + generator with GRU\"\"\"\n","\n","  def __init__(self, input_size, hidden_size, attention, max_len,generator, num_layers = 1, dropout=0.):\n","    \"\"\"\n","      Inputs:\n","        - `input_size` , `hidden_size`, and `dropout` the same as in Encoder.\n","    \"\"\"\n","    super(AttentionDecoder, self).__init__()\n","    self.input_size = input_size\n","    self.hidden_size = hidden_size\n","    # self.rnn = nn.GRU(input_size, hidden_size, num_layers=num_layers, batch_first=True,\n","    #                   dropout=dropout, bidirectional=False)\n","    self.rnn = nn.GRU(input_size + hidden_size, hidden_size, num_layers,\n","                          batch_first=True, dropout=dropout)\n","    \n","    self.generator = generator\n","    self.max_len = max_len\n","    self.dropout_layer = nn.Dropout(p=dropout)\n","    # self.rnn_to_pre = nn.Linear(input_size + hidden_size,\n","    #                             hidden_size, bias=False)\n","    self.rnn_to_pre = nn.Linear(hidden_size + hidden_size + input_size,\n","                                hidden_size, bias=False)\n","    self.attention = attention\n","\n","  def forward_step(self, prev_embed, encoder_hidden, src_mask, proj_key, hidden):\n","    \"\"\"Helper function for forward below:\n","       Perform a single decoder step (1 word).\n","\n","       Inputs:\n","      - `prev_embed`: a 3d-tensor of shape (batch_size, 1, embed_size = vocab_size)\n","          representing the padded embedded word vectors at this step in training\n","      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size) representing\n","          the current hidden state.\n","\n","      Returns:\n","      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size)\n","          representing the current decoder hidden state.\n","      - `output`: a 3d-tensor of shape (batch_size, max_len, vocab_size)\n","          representing the total generated outputs.\n","    \"\"\"\n","\n","    # compute context vector using attention mechanism\n","    query = hidden[-1].unsqueeze(1)  # [#layers, B, D] -> [B, 1, D]\n","    context, attn_probs = self.attention(\n","        query=query, proj_key=proj_key,\n","        value=encoder_hidden, mask=src_mask)\n","    \n","    # RNN\n","    rnn_input = torch.cat([prev_embed, context], dim=2)\n","\n","    output, hidden = self.rnn(rnn_input, hidden)\n","    \n","    pre_output = torch.cat([prev_embed, output, context], dim=2)\n","    pre_output = self.dropout_layer(pre_output)\n","    pre_output = self.rnn_to_pre(pre_output)\n","    \n","    return hidden, pre_output\n","\n","  def forward_step_beam(self, prev_embed, encoder_hidden, \n","                   src_mask, proj_key, hidden):\n","    \"\"\"Beam Search only: Unroll the decoder one step at a time.\"\"\"\n","    temp_hidden, pre_output = self.forward_step(prev_embed,encoder_hidden, \n","                   src_mask, proj_key, hidden)\n","    output, logits, output_word = self.generator.forward_gumbel(pre_output)\n","    return  temp_hidden, output, logits, output_word\n","\n","  def forward(self, input, encoder_hidden, encoder_finals, src_mask, max_len, hidden=None):\n","    \"\"\"Unroll the decoder one step at a time.\n","\n","    Inputs:\n","      - `inputs`: a 3d-tensor of shape (batch_size, 1, embed_size)\n","          representing a batch of padded embedded word vectors of SOS . \n","          If size is (batch_size,max_len, embed_size), then it is teacher forcing.\n","      - `encoder_finals`: a 3d-tensor of shape\n","          (num_enc_layers, batch_size, hidden_size) representing the final\n","          encoder hidden states used to initialize the initial decoder hidden\n","          states.\n","      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size) representing\n","          the value to be used to initialize the initial decoder hidden states.\n","          If None, then use `encoder_finals`.\n","      - `max_len`: an int representing the maximum decoding length.\n","      - `style`: TAYLOR_STYLE or DRAKE_STYLE\n","\n","    Returns:\n","      - `hidden`: a 3d-tensor of shape\n","          (num_layers, batch_size, hidden_size) representing the final hidden\n","          state for each element in the batch.\n","      - `outputs`: a 3d-tensor of shape\n","          (batch_size, max_len, hidden_size) representing the raw decoder\n","          outputs (before mapping to a `trg_vocab_size`-dim vector).\n","      - `logits_vectors`: a 3d-tensor of shape\n","          (batch_size, max_len, trg_vocab_size) representing the mapped decoder\n","          outputs.\n","      - `words`: a 3d-tensor of shape\n","          (batch_size, max_len, 1) representing output sentence and\n","          the corresponding word index (can be used for embedding)  \n","    \"\"\"\n","\n","    # Initialize decoder hidden state.\n","    if hidden is None:\n","      hidden = self.init_hidden(encoder_finals)\n","\n","    proj_key = self.attention.key_layer(encoder_hidden)\n","\n","    output_vectors = []\n","    logits_vectors = []\n","    words = []\n","    hidden_states = []\n","    hidden_states.append(hidden[-1][:,None,:])\n","    \n","    for i in range(max_len-1) :\n","      \n","      hidden, prev_output = self.forward_step(input,encoder_hidden, src_mask, proj_key, hidden)\n","      input, logits, output_word = self.generator.forward_gumbel(prev_output)\n","\n","      logits_vectors.append(logits)\n","      output_vectors.append(input)\n","      words.append(output_word)\n","      hidden_states.append(prev_output)\n","\n","    outputs = torch.cat(output_vectors, dim =1)\n","    logits_vectors = torch.cat(logits_vectors,dim = 1)\n","    words = torch.cat(words, axis = -1)\n","    hidden_states = torch.cat(hidden_states, axis = 1)\n","\n","    return hidden, outputs , logits_vectors, words, hidden_states\n","  \n","  def forward_teacher(self, input, encoder_hidden, encoder_finals, src_mask, max_len=None, hidden=None):\n","    \"\"\"Unroll the decoder one step at a time.\n","\n","    Inputs:\n","      - `inputs`: a 3d-tensor of shape (batch_size,max_len, embed_size)\n","          representing a batch of padded embedded word vectors of original \n","          sentence and acts as  teacher forcing.\n","\n","    Returns:\n","      - `hidden`: a 3d-tensor of shape\n","          (num_layers, batch_size, hidden_size) representing the final hidden\n","          state for each element in the batch.\n","      - `outputs`: a 3d-tensor of shape\n","          (batch_size, max_len, hidden_size) representing the raw decoder\n","          outputs (before mapping to a `trg_vocab_size`-dim vector).\n","      - `logits_vectors`: a 3d-tensor of shape\n","          (batch_size, max_len, trg_vocab_size) representing the mapped decoder\n","          outputs each represents the probability? \n","      - `words`: a 3d-tensor of shape\n","          (batch_size, max_len, 1) representing output sentence and\n","          the corresponding word index (can be used for embedding)      \n","    \"\"\"\n","\n","    # Initialize decoder hidden state.\n","    if max_len is None:\n","      max_len = input.shape[1]\n","    if hidden is None:\n","      hidden = self.init_hidden(encoder_finals)\n","    \n","    proj_key = self.attention.key_layer(encoder_hidden)\n","\n","    output_vectors = []\n","    logits_vectors = []\n","    words = []\n","    hidden_states = []\n","    hidden_states.append(hidden[-1][:,None,:])\n","\n","    for i in range(max_len):\n","      hidden, prev_output = self.forward_step(input[:,i:i+1,:], encoder_hidden, src_mask, proj_key, hidden)\n","      \n","      output, logits, output_word = self.generator(prev_output)\n","      logits_vectors.append(logits)\n","      output_vectors.append(output)\n","      words.append(output_word)\n","      hidden_states.append(prev_output)\n","\n","    outputs = torch.cat(output_vectors, dim =1)\n","    logits_vectors = torch.cat(logits_vectors,dim = 1)\n","    words = torch.cat(words, axis = -1)\n","    hidden_states = torch.cat(hidden_states, axis = 1)\n","    return hidden, outputs , logits_vectors, words, hidden_states\n","\n","  def init_hidden(self, encoder_finals):\n","    \"\"\"Use encoder final hidden state to initialize decoder's first hidden\n","       state.\n","\n","       Input: `encoder_finals` is same as in forward()\n","\n","       Returns: \n","         - `decoder_init_hiddens`: a 3d-tensor of shape \n","              (num_layers, batch_size, hidden_size) representing the initial\n","              hidden state of the decoder for each element in the batch \n","    \"\"\"\n","    decoder_init_hiddens = torch.tanh(encoder_finals)\n","    return decoder_init_hiddens"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wg_FbKl-T9xd"},"source":["### Classifier"]},{"cell_type":"code","metadata":{"id":"SeDjRElB65UC"},"source":["class LSTMDiscriminator(nn.Module):\n","  def __init__(self, input_size, hidden_size, LSTMlayers=1, dropout = 0.5):\n","    super(LSTMDiscriminator, self).__init__()\n","\n","    self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=LSTMlayers, \n","                        batch_first=True, bidirectional=True)\n","    self.drop = nn.Dropout(p=dropout)\n","    self.fc = nn.Linear(2*hidden_size, 1)\n","    self.hidden_size = hidden_size\n","\n","  def forward(self, text_emb, text_len):\n","    text_len[text_len==0] += 1\n","\n","    packed_input = pack_padded_sequence(text_emb, text_len.cpu(), batch_first=True, enforce_sorted=False)\n","    packed_output, _ = self.lstm(packed_input)\n","    output, _ = pad_packed_sequence(packed_output, batch_first=True)\n","\n","    out_forward = output[range(len(output)), text_len - 1, :self.hidden_size]\n","    out_reverse = output[:, 0, self.hidden_size:]\n","    out_reduced = torch.cat((out_forward, out_reverse), 1)\n","    text_fea = self.drop(out_reduced)\n","\n","    text_fea = self.fc(text_fea)\n","    text_fea = torch.squeeze(text_fea, 1)\n","    text_out = torch.sigmoid(text_fea)\n","\n","    return text_out\n","    \n","class LSTMClassifier(nn.Module):\n","\n","    def __init__(self, dimension=128):\n","        super(LSTMClassifier, self).__init__()\n","\n","        self.embedding = nn.Linear(len(vocab), 300)\n","        self.dimension = dimension\n","        self.lstm = nn.LSTM(input_size=300,\n","                            hidden_size=dimension,\n","                            num_layers=1,\n","                            batch_first=True,\n","                            bidirectional=True)\n","        self.drop = nn.Dropout(p=0.5)\n","\n","        self.fc = nn.Linear(2*dimension, 1)\n","\n","    def forward(self, text, text_len):\n","\n","        text_emb = self.embedding(text)\n","        text_len[text_len==0] += 1\n","\n","        packed_input = pack_padded_sequence(text_emb, text_len.cpu(), batch_first=True, enforce_sorted=False)\n","        packed_output, _ = self.lstm(packed_input)\n","        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n","\n","        out_forward = output[range(len(output)), text_len - 1, :self.dimension]\n","        out_reverse = output[:, 0, self.dimension:]\n","        out_reduced = torch.cat((out_forward, out_reverse), 1)\n","        text_fea = self.drop(out_reduced)\n","\n","        text_fea = self.fc(text_fea)\n","        text_fea = torch.squeeze(text_fea, 1)\n","        text_out = torch.sigmoid(text_fea)\n","\n","        return text_out"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CkBJ1ws7A2OF"},"source":["# Evaluation"]},{"cell_type":"markdown","metadata":{"id":"9XEGiaaa93DR"},"source":["#### TSTModel"]},{"cell_type":"code","metadata":{"id":"yjs1mzrQEyef"},"source":["class TSTModel(nn.Module):\n","  def __init__(self, max_len, vocab_size, embed_size, hidden_size_z, hidden_size_y, line_embed, encoder, generator, decoder, classifier):\n","    super(TSTModel, self).__init__()\n","\n","    self.hidden_size = hidden_size_y + hidden_size_z\n","\n","    self.encoder = encoder\n","    self.generator = generator\n","    self.decoder = decoder\n","    self.classifier = classifier\n","\n","    self.line_embed = line_embed\n","    self.y_embed_enc = nn.Embedding(2,hidden_size_y)\n","    self.y_embed_gen = nn.Embedding(2,hidden_size_y)\n","\n","    self.max_len = max_len\n","    self.vocab_size = vocab_size\n","    self.embed_size = embed_size\n","    self.hidden_size_z = hidden_size_z\n","    self.hidden_size_y = hidden_size_y\n","\n","  def forward(self, lines, line_lens, labels):\n","\n","    encoder_hidden, encoded_lines = self.encode(lines, line_lens, labels)\n","    z = encoded_lines[-1][:,self.hidden_size_y:]\n","\n","    h0_orig = torch.cat((self.y_embed_gen(labels),z), 1)[None,:]\n","    h0_tsf = torch.cat((self.y_embed_gen(1-labels),z), 1)[None,:]\n","\n","    # Decode back into original form for reconstruction\n","    rec_orig = self.reconstruct(h0_orig, lines[:, :-1])\n","\n","    # Decode into original and transferred forms for classification\n"," \n","    decode_orig = self.decode(h0_orig)\n","    decode_tsf = self.decode(h0_tsf)\n","    \n","    half = int(lines.size(0) / 2)\n","\n","    discrim1_input = torch.cat((rec_orig[4][:half], decode_tsf[4][half:]))\n","    discrim0_input = torch.cat((rec_orig[4][half:], decode_tsf[4][:half]))\n","\n","    classifier_lines = torch.cat((decode_orig[2], decode_tsf[2], F.one_hot(lines[:,1:], self.vocab_size).to(torch.float)), 0)\n","    \n","    rec_orig_len = first_eos(rec_orig[3]) + 1\n","    decode_orig_len = first_eos(decode_orig[3]) + 1\n","    decode_tsf_len = first_eos(decode_tsf[3]) + 1\n","\n","    classifier_line_lens = torch.cat((decode_orig_len, decode_tsf_len, line_lens),0)\n","    # classifier_line_lens = torch.cat((line_lens, line_lens, line_lens),0)\n","    discrim0_lens = torch.cat((rec_orig_len[half:], decode_tsf_len[:half]))\n","    discrim1_lens = torch.cat((rec_orig_len[:half], decode_tsf_len[half:]))\n","\n","    pred_class = self.classifier(classifier_lines, classifier_line_lens-1)\n","\n","    # return rec_orig, decode_orig\n","    return rec_orig, pred_class, decode_orig, decode_tsf, (discrim0_input, discrim0_lens), (discrim1_input, discrim1_lens)\n","\n","  def encode(self, lines, line_lens, labels):\n","    init_state = torch.cat((self.y_embed_enc(labels), torch.zeros((len(lines),self.hidden_size_z), device=device)), 1)[None,:].to(device)\n","    return self.encoder(self.line_embed(lines), line_lens, init_state)\n","\n","  def reconstruct(self, h0, lines):\n","    original = self.line_embed(lines)\n","    return self.decoder.forward_teacher(original,h0)\n","\n","  def decode(self, h0):\n","    target = self.line_embed(torch.tensor([SOS_INDEX]).repeat(h0.size()[1],1).to(device))\n","    return self.decoder.forward(target,h0,self.max_len)\n","\n","def first_eos(x):\n","  eos_pos = (x == EOS_INDEX)\n","  found, indices = ((eos_pos.cumsum(1) == 1) & eos_pos).max(1)\n","  indices = indices + (~found*x.size(1))\n","  return indices"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C1-4ipH195Oe"},"source":["#### TSTModelAttention"]},{"cell_type":"code","metadata":{"id":"5dN3kpsaZpFX"},"source":["class TSTModelAttention(nn.Module):\n","  def __init__(self, max_len, vocab_size, embed_size, hidden_size_z, \n","               hidden_size_y, line_embed, encoder, generator, decoder, \n","               classifier,beamSeasrch):\n","    super(TSTModelAttention, self).__init__()\n","\n","    self.hidden_size = hidden_size_y + hidden_size_z\n","\n","    self.encoder = encoder\n","    self.generator = generator\n","    self.decoder = decoder\n","    self.classifier = classifier\n","\n","    self.beamSeasrch = beamSeasrch\n","\n","    self.line_embed = line_embed\n","    self.y_embed_enc = nn.Embedding(2,hidden_size_y)\n","    self.y_embed_gen = nn.Embedding(2,hidden_size_y)\n","\n","    self.max_len = max_len\n","    self.vocab_size = vocab_size\n","    self.embed_size = embed_size\n","    self.hidden_size_z = hidden_size_z\n","    self.hidden_size_y = hidden_size_y\n","\n","  def forward(self, lines, line_lens, labels):\n","\n","    src_mask = (lines != PAD_INDEX).unsqueeze(-2)\n","    encoder_hidden, encoder_finals = self.encode(lines, line_lens, labels)\n","\n","    z = encoder_finals[-1][:,self.hidden_size_y:]\n","\n","    h0_orig = torch.cat((self.y_embed_gen(labels),z), 1)[None,:]\n","    h0_tsf = torch.cat((self.y_embed_gen(1-labels),z), 1)[None,:]\n","\n","    # Decode back into original form for reconstruction\n","    rec_orig = self.reconstruct(encoder_hidden, h0_orig, lines[:, :-1], src_mask)\n","\n","    # Decode into original and transferred forms for classification\n","    decode_orig = self.decode(encoder_hidden, h0_orig, src_mask)\n","    decode_tsf = self.decode(encoder_hidden, h0_tsf, src_mask)\n","    \n","    half = int(lines.size(0) / 2)\n","\n","    discrim1_input = torch.cat((rec_orig[4][:half], decode_tsf[4][half:]))\n","    discrim0_input = torch.cat((rec_orig[4][half:], decode_tsf[4][:half]))\n","\n","    classifier_lines = torch.cat((decode_orig[2], decode_tsf[2], F.one_hot(lines[:,1:], self.vocab_size).to(torch.float)), 0)\n","\n","    rec_orig_len = first_eos(rec_orig[3]) + 1\n","    decode_orig_len = first_eos(decode_orig[3]) + 1\n","    decode_tsf_len = first_eos(decode_tsf[3]) + 1\n","\n","    classifier_line_lens = torch.cat((decode_orig_len, decode_tsf_len, line_lens),0)\n","    # classifier_line_lens = torch.cat((line_lens, line_lens, line_lens),0)\n","    discrim0_lens = torch.cat((rec_orig_len[half:], decode_tsf_len[:half]))\n","    discrim1_lens = torch.cat((rec_orig_len[:half], decode_tsf_len[half:]))\n","\n","    pred_class = self.classifier(classifier_lines, classifier_line_lens-1)\n","    \n","    # return rec_orig, pred_class, decode_orig, decode_tsf, (discrim0_input, line_lens), (discrim1_input, line_lens)\n","    return rec_orig, pred_class, decode_orig, decode_tsf, (discrim0_input, discrim0_lens), (discrim1_input, discrim1_lens)\n","\n","  def forward_beam(self,lines, line_lens, labels):\n","    src_mask = (lines != PAD_INDEX).unsqueeze(-2)\n","    encoder_hidden, encoder_finals = self.encode(lines, line_lens, labels)\n","    z = encoder_finals[-1][:,self.hidden_size_y:]\n","\n","    h0_orig = torch.cat((self.y_embed_gen(labels),z), 1)[None,:]\n","    h0_tsf = torch.cat((self.y_embed_gen(1-labels),z), 1)[None,:]\n","\n","    decode_orig = self.decode_beam(encoder_hidden, h0_orig, src_mask)\n","    decode_tsf = self.decode_beam(encoder_hidden, h0_tsf, src_mask)\n","\n","    # half = int(lines.size(0) / 2)\n","\n","    # discrim1_input = torch.cat((rec_orig[4][:half], decode_tsf[4][half:]))\n","    # discrim0_input = torch.cat((rec_orig[4][half:], decode_tsf[4][:half]))\n","\n","    # classifier_lines = torch.cat((decode_orig[2], decode_tsf[2], F.one_hot(lines[:,1:], self.vocab_size).to(torch.float)), 0)\n","    # classifier_line_lens = torch.cat((line_lens, line_lens, line_lens),0)\n","    # pred_class = self.classifier(classifier_lines, classifier_line_lens-1)\n","\n","    return decode_orig, decode_tsf #rec_orig, ,pred_class,  (discrim0_input, line_lens), (discrim1_input, line_lens)\n","\n","  \n","  def decode_beam(self,encoder_hidden,h0,src_mask):\n","    target = self.line_embed(torch.tensor([SOS_INDEX]).repeat(h0.size()[1],1).to(device))\n","    return self.beamSeasrch.beam_decode(target, encoder_hidden,h0,src_mask, self.max_len)\n","\n","  def encode(self, lines, line_lens, labels):\n","    init_state = torch.cat((self.y_embed_enc(labels), torch.zeros((len(lines),self.hidden_size_z), device=device)), 1)[None,:].to(device)\n","    return self.encoder(self.line_embed(lines), line_lens, init_state)\n","\n","  def reconstruct(self, encoder_hidden, h0, lines, src_mask):\n","    original = self.line_embed(lines)\n","    return self.decoder.forward_teacher(original,encoder_hidden, h0, src_mask)\n","\n","  def decode(self, encoder_hidden, h0, src_mask):\n","    target = self.line_embed(torch.tensor([SOS_INDEX]).repeat(h0.size()[1],1).to(device))\n","    return self.decoder.forward(target, encoder_hidden, h0, src_mask, self.max_len)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"693yti0MBGvx"},"source":["## Evaluate Style"]},{"cell_type":"code","metadata":{"id":"uzg21oyq08RM"},"source":["attention = False\n","classify = True\n","discriminate = False\n","\n","pre_train_classifier = False\n","\n","epochs = 11\n","class_epochs = 2\n","lr = 1e-3\n","batch_size = 32\n","print_every = 100\n","\n","max_len = MAX_SENT_LENGTH_PLUS_SOS_EOS\n","vocab_size = len(vocab)\n","embed_size = 100\n","hidden_size_z = 500\n","hidden_size_y = 200\n","hidden_size = hidden_size_z + hidden_size_y\n","dropout = 0.2\n","gamma = 0.1\n","\n","TAYLOR_STYLE=1 # for information only, don't change\n","DRAKE_STYLE=0  # for information only, don't change\n","train_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","valid_loader = data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n","\n","line_embed = nn.Embedding(vocab_size, embed_size)\n","encoder = Encoder(embed_size,hidden_size)\n","generator = GeneratorTransferredSampled(hidden_size,vocab_size, line_embed, gamma = gamma)\n","classifier = LSTMClassifier()\n","discriminator0 = LSTMDiscriminator(hidden_size, hidden_size).to(device)\n","discriminator1 = LSTMDiscriminator(hidden_size, hidden_size).to(device)\n","\n","\n","if attention:\n","  attention_mech = BahdanauAttention(hidden_size, key_size=hidden_size)\n","  decoder = AttentionDecoder(embed_size, hidden_size, attention=attention_mech, max_len=vocab_size, generator = generator,dropout=dropout)\n","  # BEAM SEARCH\n","  beamSeasrch = BeamSearch(decoder, 3,3,line_embed,max_len)\n","  model = TSTModelAttention(max_len, vocab_size, embed_size, hidden_size_z, hidden_size_y, line_embed, encoder, generator, decoder, classifier).to(device)\n","else:\n","  decoder = Decoder(embed_size, hidden_size, max_len=vocab_size, generator = generator, dropout=dropout)\n","  # BEAM SEARCH\n","  beamSeasrch = BeamSearch(decoder, 3,3,line_embed,max_len)\n","  model = TSTModel(max_len, vocab_size, embed_size, hidden_size_z, hidden_size_y, line_embed, encoder, generator, decoder, classifier).to(device)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NJUWXFm-7Uee"},"source":["def lookup_indices(x, vocab):\n","  # print(x)\n","  return [vocab.index(i) for i in x]\n","  \n","def first_eos(x):\n","  eos_pos = (x == EOS_INDEX)\n","  found, indices = ((eos_pos.cumsum(1) == 1) & eos_pos).max(1)\n","  indices = indices + (~found*x.size(1))\n","  return indices"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n8u4Tgxb8ENv","executionInfo":{"status":"ok","timestamp":1638713701313,"user_tz":300,"elapsed":113,"user":{"displayName":"Katherine Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11324848797432579426"}},"outputId":"c0984ab6-028a-4f88-a94e-0fbf24e081e6"},"source":["model.eval()\n","classifier_for_eval.eval()\n","\n","pred_gen=[]\n","pred_real=[]\n","y_gen=[]\n","y_real=[]\n","for i,(taylor_lines, taylor_len, drake_lines, drake_len) in enumerate(test_loader):\n","  lines = torch.cat((taylor_lines, drake_lines), 0).to(device)    \n","  line_lens = torch.cat((taylor_len, drake_len), 0).to(device)\n","  labels = torch.cat((torch.ones(size=(len(taylor_lines),), dtype=torch.int32),torch.zeros(size=(len(drake_lines),),dtype=torch.int32))).to(device)\n","  classifier_labels = torch.cat((labels,1-labels, labels))\n","  \n","  rec_orig, pred_class, decode_orig, decode_tsf, pred_fake0, pred_fake1 = model(lines, line_lens, labels)\n","  classifier_lines = torch.cat((decode_orig[2], decode_tsf[2], F.one_hot(lines[:,1:], vocab_size).to(torch.float)), 0)\n","\n","  rec_orig_len = first_eos(rec_orig[3]) + 1\n","  decode_orig_len = first_eos(decode_orig[3]) + 1\n","  decode_tsf_len = first_eos(decode_tsf[3]) + 1\n","  classifier_line_lens = torch.cat((decode_orig_len, decode_tsf_len, line_lens),0)\n","  \n","  pred_class = classifier_for_eval(classifier_lines, classifier_line_lens-1)\n","  pred_class = pred_class.cpu().detach().numpy()\n","  classifier_labels = classifier_labels.cpu().detach().numpy()\n","\n","  pred_gen.append(pred_class[:-len(lines)])\n","  pred_real.append(pred_class[-len(lines):])\n","\n","  y_gen.append(classifier_labels[:-len(lines)])\n","  y_real.append(classifier_labels[-len(lines):])\n","\n","from sklearn.metrics import roc_auc_score\n","print(roc_auc_score(np.concatenate(y_real), np.concatenate(pred_real)))\n","print(roc_auc_score(np.concatenate(y_gen), np.concatenate(pred_gen)))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["17"]},"metadata":{},"execution_count":46}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":201},"id":"-p7Unm5l7aQZ","executionInfo":{"status":"error","timestamp":1638713594576,"user_tz":300,"elapsed":163,"user":{"displayName":"Katherine Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11324848797432579426"}},"outputId":"c8830cfd-3161-4d80-d769-cdd145918b25"},"source":["# rec_orig_len = first_eos(rec_orig[3]) + 1\n","decode_orig_len = first_eos(torch.tensor([lookup_indices(origis[0][:-1],vocab)])) + 1\n","decode_tsf_len = first_eos(torch.tensor([lookup_indices(tsf[0][:-1],vocab)])) + 1\n","classifier_line_lens = torch.cat((decode_orig_len, decode_tsf_len, line_lens),0)\n","pred_class = classifier(classifier_lines, classifier_line_lens-1)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-43-583de7a9b4ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdecode_tsf_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_eos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlookup_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtsf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mclassifier_line_lens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecode_orig_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_tsf_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline_lens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'line_lens' is not defined"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":236},"id":"tj9qFCYw0OCC","executionInfo":{"status":"error","timestamp":1638714296597,"user_tz":300,"elapsed":455,"user":{"displayName":"Katherine Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11324848797432579426"}},"outputId":"8da48b33-a12d-4dd9-8bd3-8506a2de9317"},"source":["for i,(taylor_lines, taylor_len, drake_lines, drake_len) in enumerate(test_loader):\n","\n","      lines = torch.cat((taylor_lines, drake_lines), 0) \n","      line_lens = torch.cat((taylor_len, drake_len), 0).to(device)\n","      labels = torch.cat((torch.ones(size=(len(taylor_lines),), dtype=torch.int32),torch.zeros(size=(len(drake_lines),),dtype=torch.int32))).to(device)\n","      classifier_labels = torch.cat((labels,1-labels, labels))\n","\n","      current_orig = origis[i*batch_size:(i+1)*batch_size]\n","      current_tsf = tsf[i*batch_size:(i+1)*batch_size]\n","      d_orig = torch.tensor([lookup_indices(o[:-1],vocab) for o in current_orig])\n","      d_tsf = torch.tensor([lookup_indices(o[:-1],vocab) for o in current_tsf])\n","      classifier_lines = torch.cat((d_orig, d_tsf, F.one_hot(lines[:,1:], len(vocab)).to(torch.float)), 0)\n","      classifier_lines = torch.cat((decode_orig[2], decode_tsf[2], F.one_hot(lines[:,1:], self.vocab_size).to(torch.float)), 0)\n","      \n","      pred_class = self.classifier(classifier_lines, classifier_line_lens-1)\n","\n","      correct_pred += torch.sum((pred_class[-len(lines):] >= 0.5) == classifier_labels[-len(lines):])\n","      correct_pred_all += torch.sum((pred_class >= 0.5) == classifier_labels)\n","\n","      print(\"Valid Classification Accuracy on True\", correct_pred / (2.*len(test_dataset)))\n","      print(\"Valid Classification Accuracy on All\", correct_pred_all / (3.*2.*len(test_dataset)))\n","\n","      print(lines)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-48-a08582d5e21f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0md_orig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlookup_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcurrent_orig\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m       \u001b[0md_tsf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlookup_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcurrent_tsf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m       \u001b[0mclassifier_lines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_orig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_tsf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m       \u001b[0mpred_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier_lines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier_line_lens\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 2 and 3"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PZg4GBUA_3Nc","executionInfo":{"status":"ok","timestamp":1638714538636,"user_tz":300,"elapsed":129,"user":{"displayName":"Katherine Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11324848797432579426"}},"outputId":"ef630014-7b29-41e8-b559-2fa83f1c756d"},"source":["current_orig"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['it',\n","  \"'s\",\n","  'all',\n","  'in',\n","  'me',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  ''],\n"," ['and',\n","  'she',\n","  'got',\n","  'no',\n","  'cause',\n","  'i',\n","  'i',\n","  'always',\n","  'about',\n","  'to',\n","  'believe',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  ''],\n"," ['and',\n","  'all',\n","  'i',\n","  \"'m\",\n","  'so',\n","  'when',\n","  'when',\n","  'everyone',\n","  'if',\n","  'cold',\n","  'when',\n","  'you',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  ''],\n"," ['something',\n","  'you',\n","  'got',\n","  'to',\n","  'little',\n","  'little',\n","  'her',\n","  '</s>',\n","  '</s>',\n","  'me',\n","  'it',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  ''],\n"," ['i',\n","  'wish',\n","  'i',\n","  'could',\n","  'keep',\n","  'you',\n","  'up',\n","  'in',\n","  'you',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  ''],\n"," ['and',\n","  'he',\n","  'smiles',\n","  'politely',\n","  'come',\n","  'with',\n","  'now',\n","  'you',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  ''],\n"," ['you',\n","  'got',\n","  'ta',\n","  'blue',\n","  'have',\n","  'follow',\n","  'you',\n","  'be',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  ''],\n"," ['i',\n","  ',',\n","  'i',\n","  ',',\n","  'i',\n","  'never',\n","  'chase',\n","  ',',\n","  ',',\n","  'i',\n","  'shake',\n","  'off',\n","  '</s>',\n","  '</s>',\n","  'it',\n","  '</s>',\n","  ''],\n"," ['lifeline',\n","  'and',\n","  'struck',\n","  'cars',\n","  'a',\n","  'crime',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  'it',\n","  '</s>',\n","  ''],\n"," ['and',\n","  ',',\n","  'now',\n","  'we',\n","  'come',\n","  ',',\n","  'big',\n","  'nights',\n","  '!',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  ''],\n"," ['i',\n","  'had',\n","  'a',\n","  'bad',\n","  'rush',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  ''],\n"," ['i',\n","  \"'ve\",\n","  'superman',\n","  'fly',\n","  'away',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  ''],\n"," ['so',\n","  'oh-oh',\n","  ',',\n","  'oh-oh',\n","  ',',\n","  'oh-oh',\n","  ',',\n","  'oh-oh',\n","  ',',\n","  'oh-oh',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  ''],\n"," ['ah',\n","  'your',\n","  'room',\n","  'room',\n","  ',',\n","  'all',\n","  'alone',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  ''],\n"," ['and',\n","  'i',\n","  'turn',\n","  'the',\n","  'waves',\n","  'on',\n","  'on',\n","  'me',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  'me',\n","  ''],\n"," ['i',\n","  'did',\n","  \"n't\",\n","  'know',\n","  'watch',\n","  'more',\n","  'as',\n","  'if',\n","  'they',\n","  'hide',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '?',\n","  ''],\n"," ['that',\n","  'way',\n","  'i',\n","  'think',\n","  'care',\n","  ',',\n","  'but',\n","  'you',\n","  'all',\n","  'my',\n","  'my',\n","  'family',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  ''],\n"," [\"'cause\",\n","  'august',\n","  '-',\n","  'you',\n","  \"'re\",\n","  'gone',\n","  \"'\",\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  ''],\n"," ['and',\n","  'it',\n","  'was',\n","  'so',\n","  'nice',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  ''],\n"," ['my',\n","  'reputation',\n","  'are',\n","  'never',\n","  \"'\",\n","  'the',\n","  'party',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  ''],\n"," ['i',\n","  'do',\n","  \"n't\",\n","  'wan',\n","  'na',\n","  'hide',\n","  'you',\n","  'now',\n","  'i',\n","  'do',\n","  'now',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  ''],\n"," ['when',\n","  'when',\n","  'anyone',\n","  'does',\n","  'with',\n","  'me',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  ''],\n"," ['and',\n","  'i',\n","  'wish',\n","  'i',\n","  'could',\n","  'give',\n","  'to',\n","  'to',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  ''],\n"," ['and',\n","  'bring',\n","  'on',\n","  'on',\n","  'the',\n","  'pretenders',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  ',',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  ''],\n"," ['you',\n","  \"'re\",\n","  'leaving',\n","  'on',\n","  'your',\n","  'makeup',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  ''],\n"," ['taylor',\n","  'swift',\n","  '-',\n","  '``',\n","  '``',\n","  'maria',\n","  \"''\",\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  ''],\n"," ['growing',\n","  'up',\n","  'on',\n","  'sam',\n","  'hill',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  ',',\n","  'it',\n","  'seems',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  ''],\n"," ['i',\n","  \"'ll\",\n","  'never',\n","  'have',\n","  'you',\n","  'for',\n","  'myself',\n","  'again',\n","  'granted',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  ''],\n"," ['just',\n","  'look',\n","  'at',\n","  'you',\n","  'look',\n","  'at',\n","  'me',\n","  'me',\n","  '</s>',\n","  'me',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  ''],\n"," ['you',\n","  ',',\n","  ',',\n","  'the',\n","  'on',\n","  'the',\n","  'road',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  ''],\n"," ['say',\n","  'you',\n","  \"'re\",\n","  'sorry',\n","  'of',\n","  'us',\n","  ',',\n","  'that',\n","  'white',\n","  'makeup',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  ''],\n"," ['june',\n","  '8th',\n","  ',',\n","  '2015',\n","  ':',\n","  'charlotte',\n","  ',',\n","  'north',\n","  'carolina',\n","  ',',\n","  'usa',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '']]"]},"metadata":{},"execution_count":52}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-6rnH8iY1IjX","executionInfo":{"status":"ok","timestamp":1638714488472,"user_tz":300,"elapsed":123,"user":{"displayName":"Katherine Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11324848797432579426"}},"outputId":"0ec6749a-ec59-4c6b-fe8c-0f8e6400398f"},"source":["F.one_hot(lines[:,1:],  len(vocab)).to(torch.float).shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([64, 16, 12458])"]},"metadata":{},"execution_count":51}]},{"cell_type":"code","metadata":{"id":"ugrYX61MyqND"},"source":["import os\n","import json\n","\n","model_dir = '/content/drive/Shareddrives/MIT NLP 8.864/model'\n","\n","# TODO: Change model_name\n","model_name = 'model_3/model_3_20211204_183338'\n","\n","model_name = os.path.join(model_dir,model_name)\n","origis_path = os.path.join(model_name,'orig.txt')\n","tsf_path = os.path.join(model_name,'tsf.txt')\n","raw_path = os.path.join(model_name,'raw.txt')\n","origis = []\n","tsf = []\n","raw = []\n","with open(origis_path,'r') as f:\n","  for l in f.readlines():\n","    origis.append(eval(l)+[''])\n","with open(tsf_path,'r') as f:\n","  for l in f.readlines():\n","    tsf.append(eval(l)+[''])\n","with open(raw_path,'r') as f:\n","  for l in f.readlines():\n","    raw.append(eval(l))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kiRkhnMDBz5g"},"source":["model.eval()\n","classifier_for_eval.eval()\n","\n","pred_gen=[]\n","pred_real=[]\n","y_gen=[]\n","y_real=[]\n","for i,(taylor_lines, taylor_len, drake_lines, drake_len) in enumerate(test_loader):\n","  lines = torch.cat((taylor_lines, drake_lines), 0).to(device)    \n","  line_lens = torch.cat((taylor_len, drake_len), 0).to(device)\n","  labels = torch.cat((torch.ones(size=(len(taylor_lines),), dtype=torch.int32),torch.zeros(size=(len(drake_lines),),dtype=torch.int32))).to(device)\n","  classifier_labels = torch.cat((labels,1-labels, labels))\n","  \n","  rec_orig, pred_class, decode_orig, decode_tsf, pred_fake0, pred_fake1 = model(lines, line_lens, labels)\n","  classifier_lines = torch.cat((decode_orig[2], decode_tsf[2], F.one_hot(lines[:,1:], vocab_size).to(torch.float)), 0)\n","\n","  rec_orig_len = first_eos(rec_orig[3]) + 1\n","  decode_orig_len = first_eos(decode_orig[3]) + 1\n","  decode_tsf_len = first_eos(decode_tsf[3]) + 1\n","  classifier_line_lens = torch.cat((decode_orig_len, decode_tsf_len, line_lens),0)\n","  \n","  pred_class = classifier_for_eval(classifier_lines, classifier_line_lens-1)\n","  pred_class = pred_class.cpu().detach().numpy()\n","  classifier_labels = classifier_labels.cpu().detach().numpy()\n","\n","  pred_gen.append(pred_class[:-len(lines)])\n","  pred_real.append(pred_class[-len(lines):])\n","\n","  y_gen.append(classifier_labels[:-len(lines)])\n","  y_real.append(classifier_labels[-len(lines):])\n","\n","from sklearn.metrics import roc_auc_score\n","print(roc_auc_score(np.concatenate(y_real), np.concatenate(pred_real)))\n","print(roc_auc_score(np.concatenate(y_gen), np.concatenate(pred_gen)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fQxQYvEpwSYg"},"source":["class LSTMClassifier(nn.Module):\n","\n","    def __init__(self, dimension=128):\n","        super(LSTMClassifier, self).__init__()\n","\n","        self.embedding = nn.Linear(len(vocab), 300)\n","        self.dimension = dimension\n","        self.lstm = nn.LSTM(input_size=300,\n","                            hidden_size=dimension,\n","                            num_layers=1,\n","                            batch_first=True,\n","                            bidirectional=True)\n","        self.drop = nn.Dropout(p=0.5)\n","\n","        self.fc = nn.Linear(2*dimension, 1)\n","\n","    def forward(self, text, text_len):\n","\n","        text_emb = self.embedding(text)\n","        text_len[text_len==0] += 1\n","\n","        packed_input = pack_padded_sequence(text_emb, text_len.cpu(), batch_first=True, enforce_sorted=False)\n","        packed_output, _ = self.lstm(packed_input)\n","        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n","\n","        out_forward = output[range(len(output)), text_len - 1, :self.dimension]\n","        out_reverse = output[:, 0, self.dimension:]\n","        out_reduced = torch.cat((out_forward, out_reverse), 1)\n","        text_fea = self.drop(out_reduced)\n","\n","        text_fea = self.fc(text_fea)\n","        text_fea = torch.squeeze(text_fea, 1)\n","        text_out = torch.sigmoid(text_fea)\n","\n","        return text_out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uD6wYilqvkHB"},"source":["model_saved_time = '' # To Change\n","model_dir = '/content/drive/Shareddrives/MIT NLP 8.864/model'\n","classifier_path = os.path.join(model_dir,\n","                                 f'classifier/classifier_{model_saved_time}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9WgUYPIhvW_4"},"source":["# Reload model Example\n","model = LSTMClassifier()\n","model.load_state_dict(torch.load(classifier_path))\n","model = model.to(device)\n","model.eval()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2wb8ULa-y8Ez"},"source":["classifier_lines = torch.cat((decode_orig[2], decode_tsf[2], F.one_hot(lines[:,1:], self.vocab_size).to(torch.float)), 0)\n","pred_class = self.classifier(classifier_lines, classifier_line_lens-1)"],"execution_count":null,"outputs":[]}]}