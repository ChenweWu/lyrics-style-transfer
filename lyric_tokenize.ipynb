{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"lyric_tokenize.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a3z8V6MKkllF","executionInfo":{"status":"ok","timestamp":1636126339738,"user_tz":240,"elapsed":3324,"user":{"displayName":"esc","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GguJV8bjc8KXweJbVFlXaek04XFMDA6xySQJrubPg=s64","userId":"10970071143705357180"}},"outputId":"a85b011d-55b1-4812-a30d-62f9c5d46ce1"},"source":["!pip install unidecode"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: unidecode in /usr/local/lib/python3.7/dist-packages (1.3.2)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AdPamBGAjCV8","executionInfo":{"status":"ok","timestamp":1636126341077,"user_tz":240,"elapsed":649,"user":{"displayName":"esc","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GguJV8bjc8KXweJbVFlXaek04XFMDA6xySQJrubPg=s64","userId":"10970071143705357180"}},"outputId":"460780dd-34d0-435c-c33c-ea98a1f32328"},"source":["import json\n","import re\n","from unidecode import unidecode\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","import nltk\n","nltk.download('punkt')\n","from collections import Counter"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k8kZ4Arokgqt","executionInfo":{"status":"ok","timestamp":1636126341080,"user_tz":240,"elapsed":16,"user":{"displayName":"esc","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GguJV8bjc8KXweJbVFlXaek04XFMDA6xySQJrubPg=s64","userId":"10970071143705357180"}},"outputId":"4d7da7b2-7736-43ac-f06d-b84470b4c385"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"id":"DYSUIT2PjCWC","executionInfo":{"status":"ok","timestamp":1636126375268,"user_tz":240,"elapsed":31171,"user":{"displayName":"esc","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GguJV8bjc8KXweJbVFlXaek04XFMDA6xySQJrubPg=s64","userId":"10970071143705357180"}}},"source":["f = open('/content/drive/Shareddrives/MIT NLP 8.864/Data/drake.json')\n","drake = json.load(f)\n","f.close()\n","\n","f = open('/content/drive/Shareddrives/MIT NLP 8.864/Data/tswift.json')\n","taylor = json.load(f)\n","f.close()\n","\n","drake = [drake['songs'][i]['lyrics'] for i in range(len(drake['songs']))]\n","taylor = [taylor['songs'][i]['lyrics'] for i in range(len(taylor['songs']))]\n","\n","taylor_lyrics = [re.sub('\\u2005', ' ', re.sub(r'[\\(\\[].*?[\\)\\]]', '', taylor[i])).split('\\n') for i in range(len(taylor))]\n","taylor_lyrics = [[unidecode(i) for i in taylor_lyrics[j]] for j in range(len(taylor_lyrics))]\n","taylor_lyrics = [[re.sub('\\d+EmbedShare URLCopyEmbedCopy', '', i) for i in taylor_lyrics[j]] for j in range(len(taylor_lyrics))]\n","taylor_lyrics = [[re.sub('\\d+.EmbedShare URLCopyEmbedCopy', '', i) for i in taylor_lyrics[j]] for j in range(len(taylor_lyrics))]\n","taylor_lyrics = [[re.sub('EmbedShare URLCopyEmbedCopy', '', i) for i in taylor_lyrics[j]] for j in range(len(taylor_lyrics))]\n","taylor_lyrics = [[i for i in taylor_lyrics[j] if i != ''] for j in range(len(taylor_lyrics))]\n","\n","drake_lyrics = [re.sub('\\u2005', ' ', re.sub(r'[\\(\\[].*?[\\)\\]]', '', drake[i])).split('\\n') for i in range(len(drake))]\n","drake_lyrics = [[unidecode(i) for i in drake_lyrics[j]] for j in range(len(drake_lyrics))]\n","drake_lyrics = [[re.sub('\\d+EmbedShare URLCopyEmbedCopy', '', i) for i in drake_lyrics[j]] for j in range(len(drake_lyrics))]\n","drake_lyrics = [[re.sub('\\d+.EmbedShare URLCopyEmbedCopy', '', i) for i in drake_lyrics[j]] for j in range(len(drake_lyrics))]\n","drake_lyrics = [[re.sub('EmbedShare URLCopyEmbedCopy', '', i) for i in drake_lyrics[j]] for j in range(len(drake_lyrics))]\n","drake_lyrics = [[i for i in drake_lyrics[j] if i != ''] for j in range(len(drake_lyrics))]\n","\n","taylor_lyrics = [[line1 + ', ' + line2 for line1,line2 in zip(song[0::2], song[1::2])] for song in taylor_lyrics]\n","taylor_lyrics = [[line1 + ', ' + line2 for line1,line2 in zip(song[0::2], song[1::2])] for song in taylor_lyrics]\n","\n","drake_tokenized = [[word_tokenize(drake_lyrics[i][j]) for j in range(len(drake_lyrics[i]))] for i in range(len(drake_lyrics))]\n","taylor_tokenized = [[word_tokenize(taylor_lyrics[i][j]) for j in range(len(taylor_lyrics[i]))] for i in range(len(taylor_lyrics))]\n","\n","drake_tokenized = [[[word.lower() for word in line] for line in song] for song in drake_tokenized]\n","taylor_tokenized = [[[word.lower() for word in line] for line in song] for song in taylor_tokenized]\n","\n","drake_length = sum([[len(sent) for sent in song] for song in drake_tokenized], [])\n","taylor_length = sum([[len(sent) for sent in song] for song in taylor_tokenized], [])\n","\n","drake_lyrics = sum([[sent for sent in song if (len(sent) >= 10 and len (sent) <= 30)] for song in drake_tokenized], [])\n","taylor_lyrics = sum([[sent for sent in song if (len(sent) >= 10 and len (sent) <= 30)] for song in taylor_tokenized], [])\n","\n","taylor_vocab = sum(taylor_lyrics,[])\n","drake_vocab = sum(drake_lyrics,[])\n","\n","def unique(list1):\n","     \n","    # insert the list to the set\n","    list_set = set(list1)\n","    # convert the set to the list\n","    unique_list = (list(list_set))\n","    return unique_list\n","\n","vocab = taylor_vocab + drake_vocab\n","vocab_counts = Counter(vocab)\n","vocab = unique(vocab)\n","vocab = ['<pad>','<unk>','<s>', '</s>'] + vocab\n","\n","from torch.utils import data\n","import torch\n","\n","# These IDs are reserved.\n","MAX_SENT_LENGTH = 30\n","MAX_SENT_LENGTH_PLUS_SOS_EOS = 32\n","PAD_INDEX = 0\n","UNK_INDEX = 1\n","SOS_INDEX = 2\n","EOS_INDEX = 3\n","RARE_WORD_TRESHOLD = 0\n","\n","vocab_counts['<pad>'] = RARE_WORD_TRESHOLD + 1\n","vocab_counts['<unk>'] = RARE_WORD_TRESHOLD + 1\n","vocab_counts['<s>'] = RARE_WORD_TRESHOLD + 1\n","vocab_counts['</s>'] = RARE_WORD_TRESHOLD + 1\n","\n","class TSTDataset(data.Dataset):\n","    def __init__(self, taylor_sentences, drake_sentences, vocab, vocab_counts, sampling=1.):\n","        self.taylor_sentences = taylor_sentences[:int(len(taylor_sentences) * sampling)]\n","        self.drake_sentences = drake_sentences[:int(len(drake_sentences) * sampling)]\n","\n","        self.max_seq_length = MAX_SENT_LENGTH_PLUS_SOS_EOS\n","        self.vocab = vocab\n","        self.vocab_counts = vocab_counts\n","\n","        self.v2id = {v : i for i, v in enumerate(self.vocab)}\n","        self.id2v = {val : key for key, val in self.v2id.items()}\n","    \n","    def __len__(self):\n","        return min(len(self.taylor_sentences), len(self.drake_sentences))\n","    \n","    def __getitem__(self, index):\n","        taylor_sent = self.taylor_sentences[index]\n","        taylor_len = len(taylor_sent) + 2   # add <s> and </s> to each sentence\n","        taylor_id = []\n","        for w in taylor_sent:\n","            if w not in self.vocab:\n","                w = '<unk>'\n","            if vocab_counts[w] <= RARE_WORD_TRESHOLD:\n","                w = '<unk>'\n","            taylor_id.append(self.v2id[w])\n","\n","        taylor_id = ([SOS_INDEX] + taylor_id + [EOS_INDEX] + [PAD_INDEX] *\n","                  (self.max_seq_length - taylor_len))\n","\n","        drake_sent = self.drake_sentences[index]\n","        drake_len = len(drake_sent) + 2   # add <s> and </s> to each sentence\n","        drake_id = []\n","        for w in drake_sent:\n","            if w not in self.vocab:\n","                w = '<unk>'\n","            if vocab_counts[w] <= RARE_WORD_TRESHOLD:\n","                w = '<unk>'\n","            drake_id.append(self.v2id[w])\n","\n","        drake_id = ([SOS_INDEX] + drake_id + [EOS_INDEX] + [PAD_INDEX] *\n","                  (self.max_seq_length - drake_len))\n","\n","        return torch.tensor(taylor_id), taylor_len, torch.tensor(drake_id), drake_len\n","\n","dataset = TSTDataset(taylor_lyrics, drake_lyrics, vocab, vocab_counts)\n","data_loader = data.DataLoader(dataset, batch_size=2, shuffle=True)"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ysnDBkXbl-73","executionInfo":{"status":"ok","timestamp":1636126375586,"user_tz":240,"elapsed":403,"user":{"displayName":"esc","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GguJV8bjc8KXweJbVFlXaek04XFMDA6xySQJrubPg=s64","userId":"10970071143705357180"}},"outputId":"9af22bc8-d7ca-4153-f992-ff601ffa2efa"},"source":["for a,b,c,d in data_loader:\n","    print(a)\n","    print([' '.join([dataset.id2v[word] for word in line]) for line in a.tolist()])\n","    print(c)\n","    print([' '.join([dataset.id2v[word] for word in line]) for line in c.tolist()])\n","    break"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[   2, 6786,  833, 6132,  781, 2143, 7624, 2143, 6786, 6732,  781, 2143,\n","         6786, 6075, 6985,  808, 7185, 4841,  781,    3,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0],\n","        [   2, 2215, 2143, 2215, 2143, 4988, 8342, 4839, 6001, 8020, 2143,  721,\n","         6075, 2594, 1971, 2059, 3137, 7393, 2143, 3036, 3397, 8642, 2594,  808,\n","         5580,    3,    0,    0,    0,    0,    0,    0]])\n","[\"<s> `` sparks fly '' , 2011 , `` superman '' , `` if this was a movie '' </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\", '<s> oh , oh , i got tired of waiting , wondering if you were ever coming around , my faith in you was fading </s> <pad> <pad> <pad> <pad> <pad> <pad>']\n","tensor([[   2, 7785, 2594, 4164, 6743, 5341, 3544, 2594, 3739, 2900, 2143, 4988,\n","         8342, 2594,    3,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0],\n","        [   2, 2594, 5483, 4234, 6109,  528, 8483,  187,  528, 2871,   32, 6391,\n","         2457,    3,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0]])\n","[\"<s> when you 're stressed out and you need something , i got you </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\", \"<s> you do n't have to prove shit to no one except yourself </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\"]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wp4xACl1venl","executionInfo":{"status":"ok","timestamp":1636126593713,"user_tz":240,"elapsed":252,"user":{"displayName":"esc","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GguJV8bjc8KXweJbVFlXaek04XFMDA6xySQJrubPg=s64","userId":"10970071143705357180"}},"outputId":"ff2075b4-04e4-42c2-bd1d-09070aedc971"},"source":[""],"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[   2, 4054, 6109, 3392, 3179, 7185, 3198, 2143, 4054, 6109, 3392, 3179,\n","         7185, 7666, 2143, 3179, 7185,  763, 7666, 2143, 7185, 8793, 6001, 4739,\n","         8281,    3,    0,    0,    0,    0,    0,    0],\n","        [   2, 5483, 4234, 2594, 4149, 6575, 3876, 1684, 2425, 2143,  528, 1744,\n","         4082, 2420, 3909, 2285, 2143, 2474, 1635, 2143, 7785, 4988, 8490, 2594,\n","          638, 6852, 2143, 4988, 7230, 8763,    3,    0]])\n","tensor([26, 31])\n","tensor([[   2, 8842, 8850, 2143, 8842, 7530, 2143, 6075, 2594, 4164, 8411, 8329,\n","         8218,    3,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0],\n","        [   2, 1823, 4560, 1118, 3955, 7185, 6914, 2143, 1422, 4234,  187, 1879,\n","            3,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0]])\n","tensor([14, 13])\n"]}]}]}