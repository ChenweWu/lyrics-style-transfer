{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Evaluation.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"1Ld890NnrrFj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637333386951,"user_tz":300,"elapsed":2022,"user":{"displayName":"esc","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GguJV8bjc8KXweJbVFlXaek04XFMDA6xySQJrubPg=s64","userId":"10970071143705357180"}},"outputId":"f0f94642-e533-4622-c432-b7414dc6f0f1"},"source":["import nltk\n","nltk.download('punkt')\n","from nltk import word_tokenize\n","from nltk.translate.bleu_score import sentence_bleu\n","import numpy as np"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]}]},{"cell_type":"markdown","metadata":{"id":"ZZRIdbGwYF3w"},"source":["# Pretrained Models"]},{"cell_type":"markdown","metadata":{"id":"4qYj2C4wXlX7"},"source":["## Embedding - GloVe  "]},{"cell_type":"code","metadata":{"id":"F2FIA5d0i8o9"},"source":["!wget http://nlp.stanford.edu/data/glove.6B.zip\n","!unzip glove*.zip\n","\n","print('Indexing word vectors.')\n","\n","embeddings_index = {}\n","f = open('glove.6B.100d.txt', encoding='utf-8')\n","for line in f:\n","    values = line.split()\n","    word = values[0]\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = coefs\n","f.close()\n","\n","print('Found %s word vectors.' % len(embeddings_index))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":572},"id":"blohxXT3Xku1","executionInfo":{"status":"error","timestamp":1637333690983,"user_tz":300,"elapsed":185677,"user":{"displayName":"esc","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GguJV8bjc8KXweJbVFlXaek04XFMDA6xySQJrubPg=s64","userId":"10970071143705357180"}},"outputId":"3a7831a7-b1dc-49b7-f5eb-cf839f1d914a"},"source":["!wget http://nlp.stanford.edu/data/glove.6B.zip\n","!unzip -q glove.6B.zip\n","\n","path_to_glove_file = os.path.join(\n","    os.path.expanduser(\"~\"), \".keras/datasets/glove.6B.100d.txt\"\n",")\n","\n","embeddings_index = {}\n","with open(path_to_glove_file) as f:\n","    for line in f:\n","        word, coefs = line.split(maxsplit=1)\n","        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n","        embeddings_index[word] = coefs\n","\n","print(\"Found %s word vectors.\" % len(embeddings_index))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2021-11-19 14:51:48--  http://nlp.stanford.edu/data/glove.6B.zip\n","Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n","Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n","--2021-11-19 14:51:48--  https://nlp.stanford.edu/data/glove.6B.zip\n","Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n","--2021-11-19 14:51:48--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n","Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n","Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 862182613 (822M) [application/zip]\n","Saving to: ‘glove.6B.zip’\n","\n","glove.6B.zip        100%[===================>] 822.24M  5.23MB/s    in 2m 40s  \n","\n","2021-11-19 14:54:28 (5.13 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n","\n"]},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-07cc16e8a22c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unzip -q glove.6B.zip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m path_to_glove_file = os.path.join(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"~\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".keras/datasets/glove.6B.100d.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m )\n","\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"g1SA7-F6axks"},"source":["## Embedding - Universal Sentence Encoder \n"]},{"cell_type":"code","metadata":{"id":"baReL87Sa0kr"},"source":["#@title Load the Universal Sentence Encoder's TF Hub module\n","from absl import logging\n","\n","import tensorflow as tf\n","\n","import tensorflow_hub as hub\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","import pandas as pd\n","import re\n","import seaborn as sns\n","\n","module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/4\", \"https://tfhub.dev/google/universal-sentence-encoder-large/5\"]\n","model = hub.load(module_url)\n","print (\"module %s loaded\" % module_url)\n","def universe_embed(input):\n","  return model(input)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qjlpwe2kYC8R"},"source":["# Compute Metrics"]},{"cell_type":"markdown","metadata":{"id":"U-okbIwprwqn"},"source":["**BLEU SCORES**: self-bleu and geometric mean-bleu\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"yZztGU-brYTr"},"source":["import nltk\n","nltk.download('punkt')\n","from nltk.translate.bleu_score import sentence_bleu\n","\n","\n","## self bleu \n","def bleuScore(data): \n","  # list of list of tokenized strings\n","  origs = []\n","  gens = []\n","\n","  for batch in range(len(data): \n","  # for batch in range(2*len(data): \n","    #to do: (call original line) from data and append each to origs\n","    #to do: (call generated line) from data and append each to gens \n","    \n","    score1 = corpus_bleu(origs, gens, weights=(0,0,0,0))\n","    score2 = corpus_bleu(origs, gens, weights=(0.5, 0.5, 0, 0))\n","    score3 = corpus_bleu(origs, gens, weights=(0.33, 0.33, 0.33, 0))\n","    score4 = corpus_bleu(origs, gens, weights=(0.25, 0.25, 0.25, 0.25))\n","    \n","  bleu_score = np.cumsum((score1*score2*score3*score4)**0.25)\n","  bleu_score = bleu_score / len(data)\n","  # bleu_score = bleu_score / (2*len(data))\n","\n","  return bleu_score*100\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w8M9he7or5uK"},"source":["**BERT SCORE:** between source and transferred embeddings"]},{"cell_type":"code","metadata":{"id":"NmkPLMaTHFiW"},"source":["pip install bert-score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YE2oTvKwrdTT"},"source":["## bert score\n","\n","from bert_score import score, BERTScorer\n","\n","# def bertScore(originals, generated): #text sentences\n","#   scorer = BERTScorer(lang=\"en\", rescale_with_baseline=True)\n","#   P, R, F1 = scorer.score(generated, originals)\n","\n","# return F1\n","\n","def bertScore(data): #embeddings\n","  origs = []\n","  gens= []\n","  for batch in range(len(data): \n","  # for batch in range(2*len(data): \n","    #to do: (call original line) from data\n","    origs.append()\n","    #to do: (call generated line) from data\n","    gens.append()\n","  _, _, F1 = score(gens, origs, lang=\"en\", rescale_with_baseline=True)\n","  \n","  bertScore = np.cumsum(list(F1.numpy()))/len(list(F1.numpy()))*100\n","\n","  return bertScore \n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dG9pNOb8r-iE"},"source":["**Cosine Similarity**: between source and transferred embeddings\n","Embedding Options:\n","1. tf-idf (may be not a good choice because we don't actually have files but only sentences?)\n","2. GloVe\n","3. our trained encoder\n","4. Universal Sentence Encoder (https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/semantic_similarity_with_tf_hub_universal_encoder.ipynb)"]},{"cell_type":"code","metadata":{"id":"fHIrHXh6sB3j"},"source":["def cosine(data):\n","  similarities = []\n","  for batch in range(len(data): \n","  # for batch in range(2*len(data): \n","    #to do: original = (call each original line) from data ; souce embedding\n","    #to do: generated = (call each generated line) from data ; transferred embedding\n","    similarities.append(torch.cosine_similarity(original, generated).item())\n","\n","  avg_cosine_similarity = np.sum(similarities)/len(data)\n","  # avg_cosine_similarity = np.sum(similarities)/(2*len(data))\n","\n","  return avg_cosine_similarity"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jKa970cbX7M2"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8RXZ470grtLN"},"source":["\n","## **Fluency:**\n","general perplexity, data perplexity, and total perplexity\n","\n","Relevant article:\n","https://www.scribendi.ai/comparing-bert-and-gpt-2-as-language-models-to-score-the-grammatical-correctness-of-a-sentence/"]},{"cell_type":"code","metadata":{"id":"yVib1WYpVWc3"},"source":["def id2word(self,indexes):\n","  \"\"\"Convert a list of word indexes to string. Prepare for perplexity.\n","  Input: [int]\n","  Return: [str]\"\"\"\n","  result = []\n","  for i in indexes:\n","    result.append(self.id2v[i])\n","  return result"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7afl-zutpDV2"},"source":["### Self model"]},{"cell_type":"code","metadata":{"id":"D0lgBMeHrPxB"},"source":["def perplexity(probs):\n","  \"\"\"probs: (batch, max_len, vocab_size) the probability of each word in each \n","  position of the sentence. \n","    It is also called logits_vectors as the output of decoder\"\"\"\n","  ppl = [0]*probs.shape[0]\n","  sentence_len = probs.shape[1]\n","  probs = torch.max(probs, dim = -1)\n","  for i in range(probs.shape[0]): \n","    ppl[i] = torch.exp(torch.mean(torch.log(probs[i,:])))\n","  return torch.mean(ppl)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AcysxYn4feXO"},"source":["### Bert - Perplexity\n","Colab link: https://colab.research.google.com/github/huggingface/notebooks/blob/master/transformers_doc/quicktour.ipynb\n","Documentation link: https://huggingface.co/transformers/model_doc/bert.html#bertformaskedlm"]},{"cell_type":"code","metadata":{"id":"eMI-GbkigIYJ"},"source":["! pip install transformers datasets\n","# import torch\n","from transformers import BertTokenizer,BertForMaskedLM\n","# Load pre-trained model (weights)\n","with torch.no_grad():\n","    model = BertForMaskedLM.from_pretrained('bert-large-cased')\n","    model.eval()\n","    # Load pre-trained model tokenizer (vocabulary)\n","    tokenizer = BertTokenizer.from_pretrained('bert-large-cased')\n","def bert_ppl(sentence):\n","    tokenize_input = tokenizer.tokenize(sentence)\n","    tokenize_input = [\"[CLS]\"]+tokenize_input+[\"[SEP]\"]\n","    tensor_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])\n","    with torch.no_grad():\n","        loss=model(tensor_input, masked_lm_labels=tensor_input)[0]\n","    return torch.exp(loss.detach().numpy())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fHBzm18Phemi"},"source":["## Pretrained GPT & GPT2 - Perplexity\n","pretrained GPT : https://huggingface.co/transformers/model_doc/gpt.html#openaigptlmheadmodel\n","\n","GPT2 perplexity with fixed length instruction: https://huggingface.co/transformers/perplexity.html"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":368},"id":"QqEkV1npiL6R","executionInfo":{"status":"error","timestamp":1637077779878,"user_tz":300,"elapsed":224,"user":{"displayName":"Katherine Hu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11324848797432579426"}},"outputId":"0f2a6609-4bb4-4a3b-f750-b327803b494d"},"source":["from transformers import GPT2LMHeadModel, GPT2TokenizerFast, OpenAIGPTTokenizer, OpenAIGPTLMHeadModel\n","# GPT2\n","device = 'cuda'\n","model_id = 'gpt2-large'\n","GPT2model = GPT2LMHeadModel.from_pretrained(model_id).to(device)\n","GPT2tokenizer = GPT2TokenizerFast.from_pretrained(model_id)\n","# GPT\n","GPTmodel = OpenAIGPTLMHeadModel.from_pretrained('openai-gpt')\n","GPTtokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt')\n","\n","\n","def gpt_ppl(transfered_words, gram_size = 1, \n","            tokenizer= GPT2tokenizer, model = GPT2model):\n","  \"\"\"\n","  Calculate the mean perplexity score of sentences returned \n","  by the style transfer network. \n","  input:\n","  - `transfered_sents`: a 2d-tensor of shape\n","      (batch_size, max_len) representing output sentence and\n","      the corresponding words \n","  - `gram_size`: the sliding window size to calculate perplexity score. \n","    this is same as the stride in the instruction.\n","  - `tokenizer`: can be either GPTtokenizer or GPT2tokenizer\n","      option 1 (GPT2): GPT2TokenizerFast.from_pretrained('gpt2-large')\n","      option 2 (GPT) : OpenAIGPTTokenizer.from_pretrained('openai-gpt')\n","  - `model`: which GPT model to generate probability\n","      option 1 (GPT2): GPT2LMHeadModel.from_pretrained('gpt2-large').to('cuda')\n","      option 2 (GPT) : OpenAIGPTLMHeadModel.from_pretrained('openai-gpt')\n","  return:\n","    - `ppls`: a list of perplexity scores for all sentences\n","  \"\"\"\n","  gpt_max_length = model.config.n_positions \n","  ppls = []\n","  for sent_i in range(transfered_words.shape[0]):\n","\n","    encodings = GPT2tokenizer(' '.join(transfered_words[sent_i]), return_tensors='pt')\n","    nlls = []\n","    for i in tqdm(range(0, encodings.input_ids.size(1), gram_size)):\n","      begin_loc = max(i + gram_size - gpt_max_length, 0)\n","      end_loc = min(i + gram_size, encodings.input_ids.size(1))\n","      trg_len = end_loc - i    # may be different from gram_size on last loop\n","      input_ids = encodings.input_ids[:,begin_loc:end_loc].to(device)\n","      target_ids = input_ids.clone()\n","      target_ids[:,:-trg_len] = -100\n","\n","      with torch.no_grad():\n","          outputs = model(input_ids, labels=target_ids)\n","          neg_log_likelihood = outputs[0] * trg_len\n","\n","      nlls.append(neg_log_likelihood)\n","\n","    ppl = torch.exp(torch.stack(nlls).sum() / end_loc)\n","    ppls.append(ppl)\n","  return ppls"],"execution_count":null,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-cfb4a70d31b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGPT2LMHeadModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGPT2TokenizerFast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cuda'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'gpt2-large'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mGPT2model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT2LMHeadModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mGPT2tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT2TokenizerFast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","metadata":{"id":"zACzor24oI10"},"source":["def tppl(gpt1_ppls, gpt2_ppls):\n","  return np.sqrt(torch.mean(gpt1_ppls)*torch.mean(gpt2_ppls))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X5EXaiNxUy19"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h6mQ_GnIUzPY"},"source":["# Get Eval Score"]},{"cell_type":"code","metadata":{"id":"m6A-8zGwU3S2"},"source":["import os\n","import json\n","\n","model_dir = '/content/drive/Shareddrives/MIT NLP 8.864/model'\n","\n","# TODO: Change model_name\n","#model_name = \n","\n","model_name = os.path.join(model_dir,model_name)\n","origis_path = os.path.join(model_name,'orig.txt')\n","tsf_path = os.path.join(model_name,'tsf.txt')\n","raw_path = os.path.join(model_name,'raw.txt')\n","origis = []\n","tsf = []\n","raw = []\n","with open(origis_path,'r') as f:\n","  for l in f.readlines():\n","    origis.append(eval(l)+[''])\n","with open(tsf_path,'r') as f:\n","  for l in f.readlines():\n","    tsf.append(eval(l)+[''])\n","with open(raw_path,'r') as f:\n","  for l in f.readlines():\n","    raw.append(eval(l))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QArPk-DgU_tq"},"source":["# scorer = BERTScorer(lang=\"en\", rescale_with_baseline=True)\n","\n","def generate_batch_eval(origis,word_output):#,vocab):\n","  result = []\n","  not_wanted = ['<s>','<pad>','</s>'] \n","  \n","  def clean_words(x):\n","    test_list = [t if t not in not_wanted else '' for t in x]\n","    test_example = ' '.join(test_list) \n","    return test_list, test_example\n","\n","  # def lookup_words(x, vocab):\n","  #   return [vocab[i] for i in x]\n","\n","\n","  batch_size = 0\n","  if type(word_output)==list:\n","    batch_size = len(word_output)\n","  else:\n","    batch_size = word_output.shape[0]\n","  cosine_universe,cosine_sent_trans,cosine_mpnets = [],[],[]\n","  cosine_mpnets ,cosine_distils = [],[]\n","  bert_ppls , gpt_ppls = [],[]\n","  bertScore_result , bleuScore_result = [] ,[]\n","  output_lists = []\n","  raw_lists = []\n","  for idx in range(batch_size):\n","    # Clean Words\n","    # test_example = lookup_words(word_output[idx], vocab)\n","    test_list, test_example = clean_words(word_output[idx])\n","    # ori = lookup_words(origis[idx], vocab)\n","    ori_list, ori = clean_words(origis[idx])\n","    raw_lists.append(ori)\n","    output_lists.append(test_example)\n","\n","    # GPT_PPL\n","    gpt_ppls +=gpt_ppl(test_example)\n","    # print(\"GPT_PPL finished\")\n","\n","    # bert_ppl\n","    # bert_ppls.append(bert_ppl(test_example))\n","\n","    # BleuScore\n","    bleuScore_result.append(bleuScore(ori,test_example))\n","  print(\"BleuScore finished\")\n","\n","    \n","\n","  # cosine similarity - universal \n","  cosine_universe=universal_cosine(raw_lists,output_lists)\n","  print(\"cosine_universe finished\")\n","\n","  # cosine similarity - sentence_transformer_cosine\n","  cosine_sent_trans=sentence_transformer_cosine(raw_lists, output_lists)\n","  print(\"sentence_transformer_cosine finished\")\n","\n","  # cosine similarity - mpnet_cosine\n","  cosine_mpnets=mpnet_cosine(raw_lists,output_lists)\n","  print(\"cosine_mpnets finished\")\n","\n","  # cosine similarity - distilBert\n","  cosine_distils=distilRB_cosine(raw_lists, output_lists)\n","  print(\"cosine_distils finished\")\n","\n","  # BertScore\n","  bertScore_result = bertScore(raw_lists, output_lists)\n","  print(\"bertScore finished\")\n","\n","  \n","  all_evals = {\"cosine_universal_sentence\":cosine_universe,\n","               \"cosine_sent_transformer\":cosine_sent_trans,\n","               \"cosine_mpnets\":cosine_mpnets,\n","               \"cosine_distils\":cosine_distils,\n","               \"ppl_gpt\":gpt_ppls,\n","               \"ppl_bert\":bert_ppls,\n","               \"bertScore_result\":bertScore_result,\n","               \"bleuScore_result\":bleuScore_result\n","               }\n","  return all_evals"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zP66Gaa6VBZz"},"source":["result_orig = generate_batch_eval(raw,origis)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OpmNuHbbVDE6"},"source":["result_tsf = generate_batch_eval(raw,tsf)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QNnSxyqyVIg4"},"source":["cosine_universal_sentence = torch.mean(torch.concat([result_orig['cosine_universal_sentence'],\n","              result_tsf['cosine_universal_sentence']]))\n","cosine_sent_transformer = torch.mean(torch.concat([result_orig['cosine_sent_transformer'],\n","              result_tsf['cosine_sent_transformer']]))\n","cosine_mpnets = torch.mean(torch.concat([result_orig['cosine_mpnets'],\n","              result_tsf['cosine_mpnets']]))\n","cosine_distils = torch.mean(torch.concat([result_orig['cosine_distils'],\n","              result_tsf['cosine_distils']]))\n","ppl_gpt = torch.mean(torch.tensor(gpt_ppl('test_example')))\n","# ppl_bert = np.mean(result_['ppl_bert']+result_['ppl_bert'])\n","bertScore_result = np.mean(result_orig['bertScore_result']+result_tsf['bertScore_result'])\n","bleuScore_result = np.mean(result_orig['bleuScore_result']+result_tsf['bleuScore_result'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K5aAwvcTVLDY"},"source":["print(\"cosine_universal_sentence:\",cosine_universal_sentence)\n","print(\"cosine_sent_transformer:\",cosine_sent_transformer)\n","print(\"cosine_mpnets:\",cosine_mpnets)\n","print(\"cosine_distils:\",cosine_distils)\n","print(\"ppl_gpt:\",ppl_gpt)\n","print(\"bertScore_result:\",bertScore_result)\n","print(\"bleuScore_result:\",bleuScore_result)"],"execution_count":null,"outputs":[]}]}